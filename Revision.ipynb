{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/david/anaconda3/envs/rit/lib/python3.9/site-packages/torch/cuda/__init__.py:138: UserWarning: CUDA initialization: The NVIDIA driver on your system is too old (found version 11060). Please update your GPU driver by downloading and installing a new version from the URL: http://www.nvidia.com/Download/index.aspx Alternatively, go to: https://pytorch.org to install a PyTorch version that has been compiled with your version of the CUDA driver. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import utils as ut # esta librería tiene funciones para poder obtener un procesamiento del <T,H>\n",
    "import ConcepNet_ as cn # esta librería tiene funciones para poder obtener un procesamiento del <T,H>\n",
    "#import utilidades as utils # esta librería tiene funciones para poder obtener un procesamiento del <T,H>\n",
    "import processTxt as ptxt\n",
    "import spacy\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Obtener NER para un proceso más rápido"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_md\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Côte d'Ivoire ORG\n",
      "West Africa GPE\n",
      "two CARDINAL\n",
      "Gbagbo PERSON\n",
      "September 2002 DATE\n",
      "Muslim NORP\n",
      "Christian NORP\n"
     ]
    }
   ],
   "source": [
    "doc = nlp(\"Côte d'Ivoire, once a haven of stability in West Africa and the world's top cocoa producer, has been split in two since a failed coup against Gbagbo in September 2002, pitting rebels from the Muslim-dominated north against the Christian-populated south.\")\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(ent.text, ent.label_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#The middle eastern woman wearing the pink headscarf is walking beside a woman in a purple headscarf.\n",
    "#Only one woman is wearing a head scarf."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [],
   "source": [
    "#usar representaciones david para obtener el verbo y vincular con las palabras 1 y 2 que le continuan\n",
    "#validar que existan en concetpnet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['a', 'soccer', 'team', 'be', 'warm', 'up', 'before', 'a', 'match', '.']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "lemmas_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['DET', 'NOUN', 'NOUN', 'AUX', 'VERB', 'ADP', 'ADP', 'DET', 'NOUN', 'PUNCT']"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pos_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "warm_up\n",
      "warm_before\n"
     ]
    }
   ],
   "source": [
    "for k in range(len(pos_t)):\n",
    "    if \"VERB\" ==pos_t[k]:\n",
    "        print(lemmas_t[k]+\"_\"+lemmas_t[k+1])\n",
    "        print(lemmas_t[k]+\"_\"+lemmas_t[k+2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Concepnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'related_to': {'current',\n",
       "  'date',\n",
       "  'day',\n",
       "  'era',\n",
       "  'hesternal',\n",
       "  'hesternally',\n",
       "  'hodiernal',\n",
       "  'hodiernally',\n",
       "  'last_night',\n",
       "  'nowadays',\n",
       "  'nudiustertian',\n",
       "  'tomorrow_night',\n",
       "  'tonight',\n",
       "  'yesterday'},\n",
       " 'form_of': set(),\n",
       " 'is_a': {'christmas',\n",
       "  'day',\n",
       "  'mothers_day',\n",
       "  'newspaper',\n",
       "  'present',\n",
       "  'television_show'},\n",
       " 'part_of': set(),\n",
       " 'has_a': set(),\n",
       " 'used_for': set(),\n",
       " 'capable_of': set(),\n",
       " 'at_location': {'manhattan', 'new_york', 'new_york_city'},\n",
       " 'entails': set(),\n",
       " 'causes': set(),\n",
       " 'has_subevent': set(),\n",
       " 'has_first_subevent': set(),\n",
       " 'has_last_subevent': set(),\n",
       " 'has_prerequisite': set(),\n",
       " 'has_property': set(),\n",
       " 'motivated_by_goal': set(),\n",
       " 'desires': set(),\n",
       " 'synonym': {'nowadays', 'these_days', 'today'},\n",
       " 'antonym': set(),\n",
       " 'distinct_from': {'tomorrow', 'yesterday'},\n",
       " 'derived_from': {'day'},\n",
       " 'defined_as': {'day_right_now'},\n",
       " 'manner_of': set(),\n",
       " 'located_near': set(),\n",
       " 'has_context': {'meteorology', 'us'},\n",
       " 'similar_to': set(),\n",
       " 'etymologically_related_to': {'day', 'to'},\n",
       " 'causes_desire': set(),\n",
       " 'made_of': set(),\n",
       " 'receives_action': set(),\n",
       " 'created_by': set()}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cn.df_diccionario_generales[\"today\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'related_to': {'answer', 'ball', 'baseball', 'easy', 'question', 'rounders'},\n",
       " 'form_of': set(),\n",
       " 'is_a': {'ball', 'baseball', 'flower', 'game', 'sport', 'team_sport'},\n",
       " 'part_of': set(),\n",
       " 'has_a': set(),\n",
       " 'used_for': set(),\n",
       " 'capable_of': set(),\n",
       " 'at_location': set(),\n",
       " 'entails': set(),\n",
       " 'causes': set(),\n",
       " 'has_subevent': set(),\n",
       " 'has_first_subevent': set(),\n",
       " 'has_last_subevent': set(),\n",
       " 'has_prerequisite': set(),\n",
       " 'has_property': set(),\n",
       " 'motivated_by_goal': set(),\n",
       " 'desires': set(),\n",
       " 'synonym': {'diamond_ball',\n",
       "  'indoor_baseball',\n",
       "  'kitten_ball',\n",
       "  'mush_ball',\n",
       "  'pumpkin_ball',\n",
       "  'softball',\n",
       "  'town_ball'},\n",
       " 'antonym': set(),\n",
       " 'distinct_from': set(),\n",
       " 'derived_from': {'ball', 'soft'},\n",
       " 'defined_as': set(),\n",
       " 'manner_of': set(),\n",
       " 'located_near': set(),\n",
       " 'has_context': set(),\n",
       " 'similar_to': set(),\n",
       " 'etymologically_related_to': set(),\n",
       " 'causes_desire': set(),\n",
       " 'made_of': set(),\n",
       " 'receives_action': set(),\n",
       " 'created_by': set()}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cn.df_diccionario_generales[\"softball\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "cn.relaciones(\"smiling\",\"enjoys\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "derecha\n",
      "cry - smile | antonym /a/[/r/antonym/,/c/en/cry/,/c/en/smile/]\n",
      "derecha\n",
      "frown - smile | antonym /a/[/r/antonym/,/c/en/frown/,/c/en/smile/]\n",
      "derecha\n",
      "frown - smile | antonym /a/[/r/antonym/,/c/en/frown/v/,/c/en/smile/]\n",
      "derecha\n",
      "scowl - smile | antonym /a/[/r/antonym/,/c/en/scowl/,/c/en/smile/]\n",
      "derecha\n",
      "sob - smile | antonym /a/[/r/antonym/,/c/en/sob/,/c/en/smile/]\n",
      "derecha\n",
      "human - smile | capable_of /a/[/r/capable_of/,/c/en/human/,/c/en/smile/]\n",
      "derecha\n",
      "infant_humans - smile | capable_of /a/[/r/capable_of/,/c/en/infant_humans/,/c/en/smile/]\n",
      "derecha\n",
      "buying_shirt - smile | causes /a/[/r/causes/,/c/en/buying_shirt/,/c/en/smile/]\n",
      "derecha\n",
      "enjoying_day - smile | causes /a/[/r/causes/,/c/en/enjoying_day/,/c/en/smile/]\n",
      "derecha\n",
      "going_to_performance - smile | causes /a/[/r/causes/,/c/en/going_to_performance/,/c/en/smile/]\n",
      "derecha\n",
      "hiking - smile | causes /a/[/r/causes/,/c/en/hiking/,/c/en/smile/]\n",
      "derecha\n",
      "humour - smile | causes /a/[/r/causes/,/c/en/humour/,/c/en/smile/]\n",
      "derecha\n",
      "listening_to_music - smile | causes /a/[/r/causes/,/c/en/listening_to_music/,/c/en/smile/]\n",
      "derecha\n",
      "meeting_people - smile | causes /a/[/r/causes/,/c/en/meeting_people/,/c/en/smile/]\n",
      "derecha\n",
      "remembering - smile | causes /a/[/r/causes/,/c/en/remembering/,/c/en/smile/]\n",
      "derecha\n",
      "tasting_sweet - smile | causes /a/[/r/causes/,/c/en/tasting_sweet/,/c/en/smile/]\n",
      "derecha\n",
      "thanking - smile | causes /a/[/r/causes/,/c/en/thanking/,/c/en/smile/]\n",
      "derecha\n",
      "besmile - smile | derived_from /a/[/r/derived_from/,/c/en/besmile/,/c/en/smile/]\n",
      "derecha\n",
      "nonsmile - smile | derived_from /a/[/r/derived_from/,/c/en/nonsmile/,/c/en/smile/]\n",
      "derecha\n",
      "outsmile - smile | derived_from /a/[/r/derived_from/,/c/en/outsmile/,/c/en/smile/]\n",
      "derecha\n",
      "semi_smile - smile | derived_from /a/[/r/derived_from/,/c/en/semi_smile/,/c/en/smile/]\n",
      "derecha\n",
      "smile_smirk - smile | derived_from /a/[/r/derived_from/,/c/en/smile_smirk/,/c/en/smile/]\n",
      "derecha\n",
      "smileful - smile | derived_from /a/[/r/derived_from/,/c/en/smileful/,/c/en/smile/]\n",
      "derecha\n",
      "smileless - smile | derived_from /a/[/r/derived_from/,/c/en/smileless/,/c/en/smile/]\n",
      "derecha\n",
      "smilelike - smile | derived_from /a/[/r/derived_from/,/c/en/smilelike/,/c/en/smile/]\n",
      "derecha\n",
      "smiler - smile | derived_from /a/[/r/derived_from/,/c/en/smiler/,/c/en/smile/]\n",
      "derecha\n",
      "smilest - smile | derived_from /a/[/r/derived_from/,/c/en/smilest/,/c/en/smile/]\n",
      "derecha\n",
      "smilet - smile | derived_from /a/[/r/derived_from/,/c/en/smilet/,/c/en/smile/]\n",
      "derecha\n",
      "smileth - smile | derived_from /a/[/r/derived_from/,/c/en/smileth/,/c/en/smile/]\n",
      "derecha\n",
      "smilie - smile | derived_from /a/[/r/derived_from/,/c/en/smilie/,/c/en/smile/]\n",
      "derecha\n",
      "smilish - smile | derived_from /a/[/r/derived_from/,/c/en/smilish/,/c/en/smile/]\n",
      "derecha\n",
      "smize - smile | derived_from /a/[/r/derived_from/,/c/en/smize/,/c/en/smile/]\n",
      "derecha\n",
      "smil'd - smile | form_of /a/[/r/form_of/,/c/en/smil'd/v/,/c/en/smile/]\n",
      "derecha\n",
      "smiled - smile | form_of /a/[/r/form_of/,/c/en/smiled/v/,/c/en/smile/]\n",
      "derecha\n",
      "smiledst - smile | form_of /a/[/r/form_of/,/c/en/smiledst/v/,/c/en/smile/]\n",
      "derecha\n",
      "smiles - smile | form_of /a/[/r/form_of/,/c/en/smiles/n/,/c/en/smile/]\n",
      "derecha\n",
      "smiles - smile | form_of /a/[/r/form_of/,/c/en/smiles/v/,/c/en/smile/]\n",
      "derecha\n",
      "smilest - smile | form_of /a/[/r/form_of/,/c/en/smilest/v/,/c/en/smile/]\n",
      "derecha\n",
      "smileth - smile | form_of /a/[/r/form_of/,/c/en/smileth/v/,/c/en/smile/]\n",
      "derecha\n",
      "smiling - smile | form_of /a/[/r/form_of/,/c/en/smiling/v/,/c/en/smile/]\n",
      "derecha\n",
      "celebrate - smile | has_first_subevent /a/[/r/has_first_subevent/,/c/en/celebrate/,/c/en/smile/]\n",
      "derecha\n",
      "chat_with_friends - smile | has_first_subevent /a/[/r/has_first_subevent/,/c/en/chat_with_friends/,/c/en/smile/]\n",
      "derecha\n",
      "copulate - smile | has_first_subevent /a/[/r/has_first_subevent/,/c/en/copulate/,/c/en/smile/]\n",
      "derecha\n",
      "enjoy_company_of_friends - smile | has_first_subevent /a/[/r/has_first_subevent/,/c/en/enjoy_company_of_friends/,/c/en/smile/]\n",
      "derecha\n",
      "enjoy_day - smile | has_first_subevent /a/[/r/has_first_subevent/,/c/en/enjoy_day/,/c/en/smile/]\n",
      "derecha\n",
      "express_how_funny - smile | has_first_subevent /a/[/r/has_first_subevent/,/c/en/express_how_funny/,/c/en/smile/]\n",
      "derecha\n",
      "flirt - smile | has_first_subevent /a/[/r/has_first_subevent/,/c/en/flirt/,/c/en/smile/]\n",
      "derecha\n",
      "get_good_grade - smile | has_first_subevent /a/[/r/has_first_subevent/,/c/en/get_good_grade/,/c/en/smile/]\n",
      "derecha\n",
      "get_money_from - smile | has_first_subevent /a/[/r/has_first_subevent/,/c/en/get_money_from/,/c/en/smile/]\n",
      "derecha\n",
      "happy - smile | has_first_subevent /a/[/r/has_first_subevent/,/c/en/happy/,/c/en/smile/]\n",
      "derecha\n",
      "have_food - smile | has_first_subevent /a/[/r/has_first_subevent/,/c/en/have_food/,/c/en/smile/]\n",
      "derecha\n",
      "have_fun - smile | has_first_subevent /a/[/r/has_first_subevent/,/c/en/have_fun/,/c/en/smile/]\n",
      "derecha\n",
      "hear_news - smile | has_first_subevent /a/[/r/has_first_subevent/,/c/en/hear_news/,/c/en/smile/]\n",
      "derecha\n",
      "hear_singing - smile | has_first_subevent /a/[/r/has_first_subevent/,/c/en/hear_singing/,/c/en/smile/]\n",
      "derecha\n",
      "lie - smile | has_first_subevent /a/[/r/has_first_subevent/,/c/en/lie/,/c/en/smile/]\n",
      "derecha\n",
      "look_better - smile | has_first_subevent /a/[/r/has_first_subevent/,/c/en/look_better/,/c/en/smile/]\n",
      "derecha\n",
      "meet_friend - smile | has_first_subevent /a/[/r/has_first_subevent/,/c/en/meet_friend/,/c/en/smile/]\n",
      "derecha\n",
      "pass_university_exams - smile | has_first_subevent /a/[/r/has_first_subevent/,/c/en/pass_university_exams/,/c/en/smile/]\n",
      "derecha\n",
      "please_parents - smile | has_first_subevent /a/[/r/has_first_subevent/,/c/en/please_parents/,/c/en/smile/]\n",
      "derecha\n",
      "reach_tentative_agreement - smile | has_first_subevent /a/[/r/has_first_subevent/,/c/en/reach_tentative_agreement/,/c/en/smile/]\n",
      "derecha\n",
      "see_favorite_show - smile | has_first_subevent /a/[/r/has_first_subevent/,/c/en/see_favorite_show/,/c/en/smile/]\n",
      "derecha\n",
      "see_idea_become_reality - smile | has_first_subevent /a/[/r/has_first_subevent/,/c/en/see_idea_become_reality/,/c/en/smile/]\n",
      "derecha\n",
      "sell_new_book - smile | has_first_subevent /a/[/r/has_first_subevent/,/c/en/sell_new_book/,/c/en/smile/]\n",
      "derecha\n",
      "serve_customers - smile | has_first_subevent /a/[/r/has_first_subevent/,/c/en/serve_customers/,/c/en/smile/]\n",
      "derecha\n",
      "socialize - smile | has_first_subevent /a/[/r/has_first_subevent/,/c/en/socialize/,/c/en/smile/]\n",
      "derecha\n",
      "stay_healthy - smile | has_first_subevent /a/[/r/has_first_subevent/,/c/en/stay_healthy/,/c/en/smile/]\n",
      "derecha\n",
      "taste_sweet - smile | has_first_subevent /a/[/r/has_first_subevent/,/c/en/taste_sweet/,/c/en/smile/]\n",
      "derecha\n",
      "thank - smile | has_first_subevent /a/[/r/has_first_subevent/,/c/en/thank/,/c/en/smile/]\n",
      "derecha\n",
      "win_baseball_game - smile | has_first_subevent /a/[/r/has_first_subevent/,/c/en/win_baseball_game/,/c/en/smile/]\n",
      "derecha\n",
      "agree_with - smile | has_last_subevent /a/[/r/has_last_subevent/,/c/en/agree_with/,/c/en/smile/]\n",
      "derecha\n",
      "buy_hamburger - smile | has_last_subevent /a/[/r/has_last_subevent/,/c/en/buy_hamburger/,/c/en/smile/]\n",
      "derecha\n",
      "clean_house - smile | has_last_subevent /a/[/r/has_last_subevent/,/c/en/clean_house/,/c/en/smile/]\n",
      "derecha\n",
      "copulate - smile | has_last_subevent /a/[/r/has_last_subevent/,/c/en/copulate/,/c/en/smile/]\n",
      "derecha\n",
      "entertain - smile | has_last_subevent /a/[/r/has_last_subevent/,/c/en/entertain/,/c/en/smile/]\n",
      "derecha\n",
      "flirt - smile | has_last_subevent /a/[/r/has_last_subevent/,/c/en/flirt/,/c/en/smile/]\n",
      "derecha\n",
      "give_assistance - smile | has_last_subevent /a/[/r/has_last_subevent/,/c/en/give_assistance/,/c/en/smile/]\n",
      "derecha\n",
      "kiss - smile | has_last_subevent /a/[/r/has_last_subevent/,/c/en/kiss/,/c/en/smile/]\n",
      "derecha\n",
      "listen_to_radio - smile | has_last_subevent /a/[/r/has_last_subevent/,/c/en/listen_to_radio/,/c/en/smile/]\n",
      "derecha\n",
      "look_better - smile | has_last_subevent /a/[/r/has_last_subevent/,/c/en/look_better/,/c/en/smile/]\n",
      "derecha\n",
      "read_letter - smile | has_last_subevent /a/[/r/has_last_subevent/,/c/en/read_letter/,/c/en/smile/]\n",
      "derecha\n",
      "satisfy_appetites_with_minimum_effort - smile | has_last_subevent /a/[/r/has_last_subevent/,/c/en/satisfy_appetites_with_minimum_effort/,/c/en/smile/]\n",
      "derecha\n",
      "see_favorite_show - smile | has_last_subevent /a/[/r/has_last_subevent/,/c/en/see_favorite_show/,/c/en/smile/]\n",
      "derecha\n",
      "agree_with - smile | has_prerequisite /a/[/r/has_prerequisite/,/c/en/agree_with/,/c/en/smile/]\n",
      "derecha\n",
      "enjoy_day - smile | has_prerequisite /a/[/r/has_prerequisite/,/c/en/enjoy_day/,/c/en/smile/]\n",
      "derecha\n",
      "entertain - smile | has_prerequisite /a/[/r/has_prerequisite/,/c/en/entertain/,/c/en/smile/]\n",
      "derecha\n",
      "have_fun - smile | has_prerequisite /a/[/r/has_prerequisite/,/c/en/have_fun/,/c/en/smile/]\n",
      "derecha\n",
      "make_friends - smile | has_prerequisite /a/[/r/has_prerequisite/,/c/en/make_friends/,/c/en/smile/]\n",
      "derecha\n",
      "serve_customers - smile | has_prerequisite /a/[/r/has_prerequisite/,/c/en/serve_customers/,/c/en/smile/]\n",
      "derecha\n",
      "work_box_office - smile | has_prerequisite /a/[/r/has_prerequisite/,/c/en/work_box_office/,/c/en/smile/]\n",
      "derecha\n",
      "working_box_office - smile | has_prerequisite /a/[/r/has_prerequisite/,/c/en/working_box_office/,/c/en/smile/]\n",
      "derecha\n",
      "attending_classical_concert - smile | has_subevent /a/[/r/has_subevent/,/c/en/attending_classical_concert/,/c/en/smile/]\n",
      "derecha\n",
      "conquer_nation - smile | has_subevent /a/[/r/has_subevent/,/c/en/conquer_nation/,/c/en/smile/]\n",
      "derecha\n",
      "dreaming - smile | has_subevent /a/[/r/has_subevent/,/c/en/dreaming/,/c/en/smile/]\n",
      "derecha\n",
      "eating_ice_cream - smile | has_subevent /a/[/r/has_subevent/,/c/en/eating_ice_cream/,/c/en/smile/]\n",
      "derecha\n",
      "enjoy_company_of_friends - smile | has_subevent /a/[/r/has_subevent/,/c/en/enjoy_company_of_friends/,/c/en/smile/]\n",
      "derecha\n",
      "enjoy_day - smile | has_subevent /a/[/r/has_subevent/,/c/en/enjoy_day/,/c/en/smile/]\n",
      "derecha\n",
      "enjoying_day - smile | has_subevent /a/[/r/has_subevent/,/c/en/enjoying_day/,/c/en/smile/]\n",
      "derecha\n",
      "enjoying_film - smile | has_subevent /a/[/r/has_subevent/,/c/en/enjoying_film/,/c/en/smile/]\n",
      "derecha\n",
      "expressing_information - smile | has_subevent /a/[/r/has_subevent/,/c/en/expressing_information/,/c/en/smile/]\n",
      "derecha\n",
      "flirt - smile | has_subevent /a/[/r/has_subevent/,/c/en/flirt/,/c/en/smile/]\n",
      "derecha\n",
      "flirting - smile | has_subevent /a/[/r/has_subevent/,/c/en/flirting/,/c/en/smile/]\n",
      "derecha\n",
      "get_good_grade - smile | has_subevent /a/[/r/has_subevent/,/c/en/get_good_grade/,/c/en/smile/]\n",
      "derecha\n",
      "getting - smile | has_subevent /a/[/r/has_subevent/,/c/en/getting/,/c/en/smile/]\n",
      "derecha\n",
      "getting_good_grade - smile | has_subevent /a/[/r/has_subevent/,/c/en/getting_good_grade/,/c/en/smile/]\n",
      "derecha\n",
      "getting_physical_activity - smile | has_subevent /a/[/r/has_subevent/,/c/en/getting_physical_activity/,/c/en/smile/]\n",
      "derecha\n",
      "happy - smile | has_subevent /a/[/r/has_subevent/,/c/en/happy/,/c/en/smile/]\n",
      "derecha\n",
      "have_fun - smile | has_subevent /a/[/r/has_subevent/,/c/en/have_fun/,/c/en/smile/]\n",
      "derecha\n",
      "having_fun - smile | has_subevent /a/[/r/has_subevent/,/c/en/having_fun/,/c/en/smile/]\n",
      "derecha\n",
      "having_good_time - smile | has_subevent /a/[/r/has_subevent/,/c/en/having_good_time/,/c/en/smile/]\n",
      "derecha\n",
      "hearing_crowd_go_mad - smile | has_subevent /a/[/r/has_subevent/,/c/en/hearing_crowd_go_mad/,/c/en/smile/]\n",
      "derecha\n",
      "helping - smile | has_subevent /a/[/r/has_subevent/,/c/en/helping/,/c/en/smile/]\n",
      "derecha\n",
      "howling_with_laughter - smile | has_subevent /a/[/r/has_subevent/,/c/en/howling_with_laughter/,/c/en/smile/]\n",
      "derecha\n",
      "improving_image - smile | has_subevent /a/[/r/has_subevent/,/c/en/improving_image/,/c/en/smile/]\n",
      "derecha\n",
      "killing_people - smile | has_subevent /a/[/r/has_subevent/,/c/en/killing_people/,/c/en/smile/]\n",
      "derecha\n",
      "kiss - smile | has_subevent /a/[/r/has_subevent/,/c/en/kiss/,/c/en/smile/]\n",
      "derecha\n",
      "laugh - smile | has_subevent /a/[/r/has_subevent/,/c/en/laugh/,/c/en/smile/]\n",
      "derecha\n",
      "like - smile | has_subevent /a/[/r/has_subevent/,/c/en/like/,/c/en/smile/]\n",
      "derecha\n",
      "looking_better - smile | has_subevent /a/[/r/has_subevent/,/c/en/looking_better/,/c/en/smile/]\n",
      "derecha\n",
      "making_friends - smile | has_subevent /a/[/r/has_subevent/,/c/en/making_friends/,/c/en/smile/]\n",
      "derecha\n",
      "meet_girls - smile | has_subevent /a/[/r/has_subevent/,/c/en/meet_girls/,/c/en/smile/]\n",
      "derecha\n",
      "meet_people - smile | has_subevent /a/[/r/has_subevent/,/c/en/meet_people/,/c/en/smile/]\n",
      "derecha\n",
      "meeting_friend - smile | has_subevent /a/[/r/has_subevent/,/c/en/meeting_friend/,/c/en/smile/]\n",
      "derecha\n",
      "meeting_people - smile | has_subevent /a/[/r/has_subevent/,/c/en/meeting_people/,/c/en/smile/]\n",
      "derecha\n",
      "open_gift - smile | has_subevent /a/[/r/has_subevent/,/c/en/open_gift/,/c/en/smile/]\n",
      "derecha\n",
      "opening_gift - smile | has_subevent /a/[/r/has_subevent/,/c/en/opening_gift/,/c/en/smile/]\n",
      "derecha\n",
      "people_happy - smile | has_subevent /a/[/r/has_subevent/,/c/en/people_happy/,/c/en/smile/]\n",
      "derecha\n",
      "perform - smile | has_subevent /a/[/r/has_subevent/,/c/en/perform/,/c/en/smile/]\n",
      "derecha\n",
      "performing - smile | has_subevent /a/[/r/has_subevent/,/c/en/performing/,/c/en/smile/]\n",
      "derecha\n",
      "pretending - smile | has_subevent /a/[/r/has_subevent/,/c/en/pretending/,/c/en/smile/]\n",
      "derecha\n",
      "reading_letter - smile | has_subevent /a/[/r/has_subevent/,/c/en/reading_letter/,/c/en/smile/]\n",
      "derecha\n",
      "receiving_degree - smile | has_subevent /a/[/r/has_subevent/,/c/en/receiving_degree/,/c/en/smile/]\n",
      "derecha\n",
      "remembering - smile | has_subevent /a/[/r/has_subevent/,/c/en/remembering/,/c/en/smile/]\n",
      "derecha\n",
      "see_favorite_show - smile | has_subevent /a/[/r/has_subevent/,/c/en/see_favorite_show/,/c/en/smile/]\n",
      "derecha\n",
      "sell_new_book - smile | has_subevent /a/[/r/has_subevent/,/c/en/sell_new_book/,/c/en/smile/]\n",
      "derecha\n",
      "serve_customers - smile | has_subevent /a/[/r/has_subevent/,/c/en/serve_customers/,/c/en/smile/]\n",
      "derecha\n",
      "serving_customers - smile | has_subevent /a/[/r/has_subevent/,/c/en/serving_customers/,/c/en/smile/]\n",
      "derecha\n",
      "socialize - smile | has_subevent /a/[/r/has_subevent/,/c/en/socialize/,/c/en/smile/]\n",
      "derecha\n",
      "surprising - smile | has_subevent /a/[/r/has_subevent/,/c/en/surprising/,/c/en/smile/]\n",
      "derecha\n",
      "talking_to - smile | has_subevent /a/[/r/has_subevent/,/c/en/talking_to/,/c/en/smile/]\n",
      "derecha\n",
      "taste_sweet - smile | has_subevent /a/[/r/has_subevent/,/c/en/taste_sweet/,/c/en/smile/]\n",
      "derecha\n",
      "tasting_sweet - smile | has_subevent /a/[/r/has_subevent/,/c/en/tasting_sweet/,/c/en/smile/]\n",
      "derecha\n",
      "telling_many_people_about - smile | has_subevent /a/[/r/has_subevent/,/c/en/telling_many_people_about/,/c/en/smile/]\n",
      "derecha\n",
      "thank - smile | has_subevent /a/[/r/has_subevent/,/c/en/thank/,/c/en/smile/]\n",
      "derecha\n",
      "thanking - smile | has_subevent /a/[/r/has_subevent/,/c/en/thanking/,/c/en/smile/]\n",
      "derecha\n",
      "walk_in_rain - smile | has_subevent /a/[/r/has_subevent/,/c/en/walk_in_rain/,/c/en/smile/]\n",
      "derecha\n",
      "win_baseball_game - smile | has_subevent /a/[/r/has_subevent/,/c/en/win_baseball_game/,/c/en/smile/]\n",
      "derecha\n",
      "winning_baseball_game - smile | has_subevent /a/[/r/has_subevent/,/c/en/winning_baseball_game/,/c/en/smile/]\n",
      "derecha\n",
      "grin - smile | is_a /a/[/r/is_a/,/c/en/grin/,/c/en/smile/]\n",
      "derecha\n",
      "hear_music - smile | motivated_by_goal /a/[/r/motivated_by_goal/,/c/en/hear_music/,/c/en/smile/]\n",
      "derecha\n",
      "beam - smile | related_to /a/[/r/related_to/,/c/en/beam/v/,/c/en/smile/]\n",
      "derecha\n",
      "besmile - smile | related_to /a/[/r/related_to/,/c/en/besmile/v/,/c/en/smile/]\n",
      "derecha\n",
      "clown - smile | related_to /a/[/r/related_to/,/c/en/clown/,/c/en/smile/]\n",
      "derecha\n",
      "crack_smile - smile | related_to /a/[/r/related_to/,/c/en/crack_smile/v/,/c/en/smile/]\n",
      "derecha\n",
      "duchenne_smile - smile | related_to /a/[/r/related_to/,/c/en/duchenne_smile/n/,/c/en/smile/]\n",
      "derecha\n",
      "expression - smile | related_to /a/[/r/related_to/,/c/en/expression/,/c/en/smile/]\n",
      "derecha\n",
      "face - smile | related_to /a/[/r/related_to/,/c/en/face/,/c/en/smile/]\n",
      "derecha\n",
      "fish_eating_grin - smile | related_to /a/[/r/related_to/,/c/en/fish_eating_grin/n/,/c/en/smile/]\n",
      "derecha\n",
      "frown - smile | related_to /a/[/r/related_to/,/c/en/frown/,/c/en/smile/]\n",
      "derecha\n",
      "grin - smile | related_to /a/[/r/related_to/,/c/en/grin/,/c/en/smile/]\n",
      "derecha\n",
      "grin - smile | related_to /a/[/r/related_to/,/c/en/grin/n/wikt/en_1/,/c/en/smile/]\n",
      "derecha\n",
      "grin_like_cheshire_cat - smile | related_to /a/[/r/related_to/,/c/en/grin_like_cheshire_cat/v/,/c/en/smile/]\n",
      "derecha\n",
      "grip_and_grin - smile | related_to /a/[/r/related_to/,/c/en/grip_and_grin/n/,/c/en/smile/]\n",
      "derecha\n",
      "half_smile - smile | related_to /a/[/r/related_to/,/c/en/half_smile/n/,/c/en/smile/]\n",
      "derecha\n",
      "happy - smile | related_to /a/[/r/related_to/,/c/en/happy/,/c/en/smile/]\n",
      "derecha\n",
      "laugh - smile | related_to /a/[/r/related_to/,/c/en/laugh/,/c/en/smile/]\n",
      "derecha\n",
      "lip - smile | related_to /a/[/r/related_to/,/c/en/lip/,/c/en/smile/]\n",
      "derecha\n",
      "mouth - smile | related_to /a/[/r/related_to/,/c/en/mouth/,/c/en/smile/]\n",
      "derecha\n",
      "nonsmile - smile | related_to /a/[/r/related_to/,/c/en/nonsmile/n/,/c/en/smile/]\n",
      "derecha\n",
      "nonsmiling - smile | related_to /a/[/r/related_to/,/c/en/nonsmiling/a/,/c/en/smile/]\n",
      "derecha\n",
      "outsmile - smile | related_to /a/[/r/related_to/,/c/en/outsmile/v/,/c/en/smile/]\n",
      "derecha\n",
      "pickin_and_grinnin - smile | related_to /a/[/r/related_to/,/c/en/pickin_and_grinnin/n/,/c/en/smile/]\n",
      "derecha\n",
      "say_cheese - smile | related_to /a/[/r/related_to/,/c/en/say_cheese/v/,/c/en/smile/]\n",
      "derecha\n",
      "shit_eating_grin - smile | related_to /a/[/r/related_to/,/c/en/shit_eating_grin/n/,/c/en/smile/]\n",
      "derecha\n",
      "simper - smile | related_to /a/[/r/related_to/,/c/en/simper/v/,/c/en/smile/]\n",
      "derecha\n",
      "smile_from_ear_to_ear - smile | related_to /a/[/r/related_to/,/c/en/smile_from_ear_to_ear/n/,/c/en/smile/]\n",
      "derecha\n",
      "smile_smirk - smile | related_to /a/[/r/related_to/,/c/en/smile_smirk/v/,/c/en/smile/]\n",
      "derecha\n",
      "smiled - smile | related_to /a/[/r/related_to/,/c/en/smiled/v/,/c/en/smile/]\n",
      "derecha\n",
      "smileful - smile | related_to /a/[/r/related_to/,/c/en/smileful/a/,/c/en/smile/]\n",
      "derecha\n",
      "smileless - smile | related_to /a/[/r/related_to/,/c/en/smileless/a/,/c/en/smile/]\n",
      "derecha\n",
      "smilelike - smile | related_to /a/[/r/related_to/,/c/en/smilelike/a/,/c/en/smile/]\n",
      "derecha\n",
      "smiler - smile | related_to /a/[/r/related_to/,/c/en/smiler/n/,/c/en/smile/]\n",
      "derecha\n",
      "smiles - smile | related_to /a/[/r/related_to/,/c/en/smiles/n/,/c/en/smile/]\n",
      "derecha\n",
      "smiles - smile | related_to /a/[/r/related_to/,/c/en/smiles/v/,/c/en/smile/]\n",
      "derecha\n",
      "smilest - smile | related_to /a/[/r/related_to/,/c/en/smilest/v/,/c/en/smile/]\n",
      "derecha\n",
      "smilet - smile | related_to /a/[/r/related_to/,/c/en/smilet/n/,/c/en/smile/]\n",
      "derecha\n",
      "smileth - smile | related_to /a/[/r/related_to/,/c/en/smileth/v/,/c/en/smile/]\n",
      "derecha\n",
      "smiley_face - smile | related_to /a/[/r/related_to/,/c/en/smiley_face/n/,/c/en/smile/]\n",
      "derecha\n",
      "smiling - smile | related_to /a/[/r/related_to/,/c/en/smiling/n/,/c/en/smile/]\n",
      "derecha\n",
      "smiling - smile | related_to /a/[/r/related_to/,/c/en/smiling/v/,/c/en/smile/]\n",
      "derecha\n",
      "smilingly - smile | related_to /a/[/r/related_to/,/c/en/smilingly/r/,/c/en/smile/]\n",
      "derecha\n",
      "smilingness - smile | related_to /a/[/r/related_to/,/c/en/smilingness/n/,/c/en/smile/]\n",
      "derecha\n",
      "smilish - smile | related_to /a/[/r/related_to/,/c/en/smilish/a/,/c/en/smile/]\n",
      "derecha\n",
      "smirk - smile | related_to /a/[/r/related_to/,/c/en/smirk/n/,/c/en/smile/]\n",
      "derecha\n",
      "smirk - smile | related_to /a/[/r/related_to/,/c/en/smirk/v/,/c/en/smile/]\n",
      "derecha\n",
      "smize - smile | related_to /a/[/r/related_to/,/c/en/smize/v/,/c/en/smile/]\n",
      "derecha\n",
      "straighten_one's_face - smile | related_to /a/[/r/related_to/,/c/en/straighten_one's_face/v/,/c/en/smile/]\n",
      "derecha\n",
      "unsmilingness - smile | related_to /a/[/r/related_to/,/c/en/unsmilingness/n/,/c/en/smile/]\n",
      "derecha\n",
      "beam - smile | synonym /a/[/r/synonym/,/c/en/beam/v/,/c/en/smile/]\n",
      "derecha\n",
      "archaic_smile - smile | derived_from /a/[/r/derived_from/,/c/en/archaic_smile/,/c/en/smile/n/]\n",
      "derecha\n",
      "besmile - smile | derived_from /a/[/r/derived_from/,/c/en/besmile/,/c/en/smile/n/]\n",
      "derecha\n",
      "chelsea_smile - smile | derived_from /a/[/r/derived_from/,/c/en/chelsea_smile/,/c/en/smile/n/]\n",
      "derecha\n",
      "glasgow_smile - smile | derived_from /a/[/r/derived_from/,/c/en/glasgow_smile/,/c/en/smile/n/]\n",
      "derecha\n",
      "smileless - smile | derived_from /a/[/r/derived_from/,/c/en/smileless/,/c/en/smile/n/]\n",
      "derecha\n",
      "smilet - smile | derived_from /a/[/r/derived_from/,/c/en/smilet/,/c/en/smile/n/]\n",
      "derecha\n",
      "smiley - smile | derived_from /a/[/r/derived_from/,/c/en/smiley/,/c/en/smile/n/]\n",
      "derecha\n",
      "vertical_smile - smile | derived_from /a/[/r/derived_from/,/c/en/vertical_smile/,/c/en/smile/n/]\n",
      "derecha\n",
      "smiles - smile | form_of /a/[/r/form_of/,/c/en/smiles/,/c/en/smile/n/]\n",
      "derecha\n",
      "smiler - smile | derived_from /a/[/r/derived_from/,/c/en/smiler/,/c/en/smile/v/]\n",
      "derecha\n",
      "smiled - smile | form_of /a/[/r/form_of/,/c/en/smiled/,/c/en/smile/v/]\n",
      "derecha\n",
      "smiles - smile | form_of /a/[/r/form_of/,/c/en/smiles/,/c/en/smile/v/]\n",
      "derecha\n",
      "smiling - smile | form_of /a/[/r/form_of/,/c/en/smiling/,/c/en/smile/v/]\n",
      "derecha\n",
      "simper - smile | is_a /a/[/r/is_a/,/c/en/simper/n/wn/communication/,/c/en/smile/n/wn/communication/]\n",
      "derecha\n",
      "smirk - smile | is_a /a/[/r/is_a/,/c/en/smirk/n/wn/communication/,/c/en/smile/n/wn/communication/]\n",
      "derecha\n",
      "grin - smile | synonym /a/[/r/synonym/,/c/en/grin/n/wn/communication/,/c/en/smile/n/wn/communication/]\n",
      "derecha\n",
      "grinning - smile | synonym /a/[/r/synonym/,/c/en/grinning/n/wn/communication/,/c/en/smile/n/wn/communication/]\n",
      "derecha\n",
      "smiling - smile | synonym /a/[/r/synonym/,/c/en/smiling/n/wn/communication/,/c/en/smile/n/wn/communication/]\n",
      "derecha\n",
      "beam - smile | manner_of /a/[/r/manner_of/,/c/en/beam/v/wn/body/,/c/en/smile/v/wn/body/]\n",
      "derecha\n",
      "dimple - smile | manner_of /a/[/r/manner_of/,/c/en/dimple/v/wn/body/,/c/en/smile/v/wn/body/]\n",
      "derecha\n",
      "grin - smile | manner_of /a/[/r/manner_of/,/c/en/grin/v/wn/body/,/c/en/smile/v/wn/body/]\n",
      "derecha\n",
      "smirk - smile | manner_of /a/[/r/manner_of/,/c/en/smirk/v/wn/body/,/c/en/smile/v/wn/body/]\n",
      "derecha\n",
      "sneer - smile | manner_of /a/[/r/manner_of/,/c/en/sneer/v/wn/body/,/c/en/smile/v/wn/body/]\n",
      "izquierda\n",
      "smile - cry | antonym /a/[/r/antonym/,/c/en/smile/,/c/en/cry/]\n",
      "izquierda\n",
      "smile - frown | antonym /a/[/r/antonym/,/c/en/smile/,/c/en/frown/]\n",
      "izquierda\n",
      "smile - rainbow | antonym /a/[/r/antonym/,/c/en/smile/,/c/en/rainbow/]\n",
      "izquierda\n",
      "smile - lift_spirits | capable_of /a/[/r/capable_of/,/c/en/smile/,/c/en/lift_spirits/]\n",
      "izquierda\n",
      "smile - lifting_spirits_all_day | capable_of /a/[/r/capable_of/,/c/en/smile/,/c/en/lifting_spirits_all_day/]\n",
      "izquierda\n",
      "smile - light_up_room | capable_of /a/[/r/capable_of/,/c/en/smile/,/c/en/light_up_room/]\n",
      "izquierda\n",
      "smile - please_another | capable_of /a/[/r/capable_of/,/c/en/smile/,/c/en/please_another/]\n",
      "izquierda\n",
      "smile - warm_heart | capable_of /a/[/r/capable_of/,/c/en/smile/,/c/en/warm_heart/]\n",
      "izquierda\n",
      "smile - warm_old_man's_heart | capable_of /a/[/r/capable_of/,/c/en/smile/,/c/en/warm_old_man's_heart/]\n",
      "izquierda\n",
      "smile - frown | distinct_from /a/[/r/distinct_from/,/c/en/smile/,/c/en/frown/]\n",
      "izquierda\n",
      "smile - laugh | distinct_from /a/[/r/distinct_from/,/c/en/smile/,/c/en/laugh/]\n",
      "izquierda\n",
      "smile - make_others_happy_too | has_subevent /a/[/r/has_subevent/,/c/en/smile/,/c/en/make_others_happy_too/]\n",
      "izquierda\n",
      "smile - work_facial_muscles | has_subevent /a/[/r/has_subevent/,/c/en/smile/,/c/en/work_facial_muscles/]\n",
      "izquierda\n",
      "smile - facial_expression | is_a /a/[/r/is_a/,/c/en/smile/,/c/en/facial_expression/]\n",
      "izquierda\n",
      "smile - sign_of_friendliness | is_a /a/[/r/is_a/,/c/en/smile/,/c/en/sign_of_friendliness/]\n",
      "izquierda\n",
      "smile - atmosphere_friendly | motivated_by_goal /a/[/r/motivated_by_goal/,/c/en/smile/,/c/en/atmosphere_friendly/]\n",
      "izquierda\n",
      "smile - good_had_just_happened | motivated_by_goal /a/[/r/motivated_by_goal/,/c/en/smile/,/c/en/good_had_just_happened/]\n",
      "izquierda\n",
      "smile - happy | motivated_by_goal /a/[/r/motivated_by_goal/,/c/en/smile/,/c/en/happy/]\n",
      "izquierda\n",
      "smile - heather_told_joje | motivated_by_goal /a/[/r/motivated_by_goal/,/c/en/smile/,/c/en/heather_told_joje/]\n",
      "izquierda\n",
      "smile - action | related_to /a/[/r/related_to/,/c/en/smile/,/c/en/action/]\n",
      "izquierda\n",
      "smile - backwards | related_to /a/[/r/related_to/,/c/en/smile/,/c/en/backwards/]\n",
      "izquierda\n",
      "smile - backwards_frown | related_to /a/[/r/related_to/,/c/en/smile/,/c/en/backwards_frown/]\n",
      "izquierda\n",
      "smile - be_happy | related_to /a/[/r/related_to/,/c/en/smile/,/c/en/be_happy/]\n",
      "izquierda\n",
      "smile - being | related_to /a/[/r/related_to/,/c/en/smile/,/c/en/being/]\n",
      "izquierda\n",
      "smile - being_happy | related_to /a/[/r/related_to/,/c/en/smile/,/c/en/being_happy/]\n",
      "izquierda\n",
      "smile - big | related_to /a/[/r/related_to/,/c/en/smile/,/c/en/big/]\n",
      "izquierda\n",
      "smile - big_grin | related_to /a/[/r/related_to/,/c/en/smile/,/c/en/big_grin/]\n",
      "izquierda\n",
      "smile - cheese | related_to /a/[/r/related_to/,/c/en/smile/,/c/en/cheese/]\n",
      "izquierda\n",
      "smile - contagious | related_to /a/[/r/related_to/,/c/en/smile/,/c/en/contagious/]\n",
      "izquierda\n",
      "smile - crescent | related_to /a/[/r/related_to/,/c/en/smile/,/c/en/crescent/]\n",
      "izquierda\n",
      "smile - curly | related_to /a/[/r/related_to/,/c/en/smile/,/c/en/curly/]\n",
      "izquierda\n",
      "smile - curly_line | related_to /a/[/r/related_to/,/c/en/smile/,/c/en/curly_line/]\n",
      "izquierda\n",
      "smile - curve | related_to /a/[/r/related_to/,/c/en/smile/,/c/en/curve/]\n",
      "izquierda\n",
      "smile - curved | related_to /a/[/r/related_to/,/c/en/smile/,/c/en/curved/]\n",
      "izquierda\n",
      "smile - curved_lips | related_to /a/[/r/related_to/,/c/en/smile/,/c/en/curved_lips/]\n",
      "izquierda\n",
      "smile - dimples | related_to /a/[/r/related_to/,/c/en/smile/,/c/en/dimples/]\n",
      "izquierda\n",
      "smile - emotion | related_to /a/[/r/related_to/,/c/en/smile/,/c/en/emotion/]\n",
      "izquierda\n",
      "smile - emotional | related_to /a/[/r/related_to/,/c/en/smile/,/c/en/emotional/]\n",
      "izquierda\n",
      "smile - emotional_response | related_to /a/[/r/related_to/,/c/en/smile/,/c/en/emotional_response/]\n",
      "izquierda\n",
      "smile - express | related_to /a/[/r/related_to/,/c/en/smile/,/c/en/express/]\n",
      "izquierda\n",
      "smile - express_happiness | related_to /a/[/r/related_to/,/c/en/smile/,/c/en/express_happiness/]\n",
      "izquierda\n",
      "smile - expression | related_to /a/[/r/related_to/,/c/en/smile/,/c/en/expression/]\n",
      "izquierda\n",
      "smile - expressions | related_to /a/[/r/related_to/,/c/en/smile/,/c/en/expressions/]\n",
      "izquierda\n",
      "smile - face | related_to /a/[/r/related_to/,/c/en/smile/,/c/en/face/]\n",
      "izquierda\n",
      "smile - face_action | related_to /a/[/r/related_to/,/c/en/smile/,/c/en/face_action/]\n",
      "izquierda\n",
      "smile - face_expression | related_to /a/[/r/related_to/,/c/en/smile/,/c/en/face_expression/]\n",
      "izquierda\n",
      "smile - face_gesture | related_to /a/[/r/related_to/,/c/en/smile/,/c/en/face_gesture/]\n",
      "izquierda\n",
      "smile - face_grin | related_to /a/[/r/related_to/,/c/en/smile/,/c/en/face_grin/]\n",
      "izquierda\n",
      "smile - facial | related_to /a/[/r/related_to/,/c/en/smile/,/c/en/facial/]\n",
      "izquierda\n",
      "smile - facial_expression | related_to /a/[/r/related_to/,/c/en/smile/,/c/en/facial_expression/]\n",
      "izquierda\n",
      "smile - facial_friendliness | related_to /a/[/r/related_to/,/c/en/smile/,/c/en/facial_friendliness/]\n",
      "izquierda\n",
      "smile - facial_gesture | related_to /a/[/r/related_to/,/c/en/smile/,/c/en/facial_gesture/]\n",
      "izquierda\n",
      "smile - facial_happiness | related_to /a/[/r/related_to/,/c/en/smile/,/c/en/facial_happiness/]\n",
      "izquierda\n",
      "smile - facial_motion | related_to /a/[/r/related_to/,/c/en/smile/,/c/en/facial_motion/]\n",
      "izquierda\n",
      "smile - facial_movement | related_to /a/[/r/related_to/,/c/en/smile/,/c/en/facial_movement/]\n",
      "izquierda\n",
      "smile - feeling | related_to /a/[/r/related_to/,/c/en/smile/,/c/en/feeling/]\n",
      "izquierda\n",
      "smile - feeling_action | related_to /a/[/r/related_to/,/c/en/smile/,/c/en/feeling_action/]\n",
      "izquierda\n",
      "smile - for_happiness | related_to /a/[/r/related_to/,/c/en/smile/,/c/en/for_happiness/]\n",
      "izquierda\n",
      "smile - for_pictures | related_to /a/[/r/related_to/,/c/en/smile/,/c/en/for_pictures/]\n",
      "izquierda\n",
      "smile - friendliness | related_to /a/[/r/related_to/,/c/en/smile/,/c/en/friendliness/]\n",
      "izquierda\n",
      "smile - friendly | related_to /a/[/r/related_to/,/c/en/smile/,/c/en/friendly/]\n",
      "izquierda\n",
      "smile - frown | related_to /a/[/r/related_to/,/c/en/smile/,/c/en/frown/]\n",
      "izquierda\n",
      "smile - gesture | related_to /a/[/r/related_to/,/c/en/smile/,/c/en/gesture/]\n",
      "izquierda\n",
      "smile - grin | related_to /a/[/r/related_to/,/c/en/smile/,/c/en/grin/]\n",
      "izquierda\n",
      "smile - grin_expression | related_to /a/[/r/related_to/,/c/en/smile/,/c/en/grin_expression/]\n",
      "izquierda\n",
      "smile - grinning | related_to /a/[/r/related_to/,/c/en/smile/,/c/en/grinning/]\n",
      "izquierda\n",
      "smile - happiness | related_to /a/[/r/related_to/,/c/en/smile/,/c/en/happiness/]\n",
      "izquierda\n",
      "smile - happiness_face | related_to /a/[/r/related_to/,/c/en/smile/,/c/en/happiness_face/]\n",
      "izquierda\n",
      "smile - happiness_sign | related_to /a/[/r/related_to/,/c/en/smile/,/c/en/happiness_sign/]\n",
      "izquierda\n",
      "smile - happy | related_to /a/[/r/related_to/,/c/en/smile/,/c/en/happy/]\n",
      "izquierda\n",
      "smile - happy_action | related_to /a/[/r/related_to/,/c/en/smile/,/c/en/happy_action/]\n",
      "izquierda\n",
      "smile - happy_expression | related_to /a/[/r/related_to/,/c/en/smile/,/c/en/happy_expression/]\n",
      "izquierda\n",
      "smile - happy_face | related_to /a/[/r/related_to/,/c/en/smile/,/c/en/happy_face/]\n",
      "izquierda\n",
      "smile - happy_grin | related_to /a/[/r/related_to/,/c/en/smile/,/c/en/happy_grin/]\n",
      "izquierda\n",
      "smile - happy_mouth | related_to /a/[/r/related_to/,/c/en/smile/,/c/en/happy_mouth/]\n",
      "izquierda\n",
      "smile - happy_movement | related_to /a/[/r/related_to/,/c/en/smile/,/c/en/happy_movement/]\n",
      "izquierda\n",
      "smile - happy_thing | related_to /a/[/r/related_to/,/c/en/smile/,/c/en/happy_thing/]\n",
      "izquierda\n",
      "smile - inverted | related_to /a/[/r/related_to/,/c/en/smile/,/c/en/inverted/]\n",
      "izquierda\n",
      "smile - inverted_frown | related_to /a/[/r/related_to/,/c/en/smile/,/c/en/inverted_frown/]\n",
      "izquierda\n",
      "smile - inverted_rainbow | related_to /a/[/r/related_to/,/c/en/smile/,/c/en/inverted_rainbow/]\n",
      "izquierda\n",
      "smile - laugh | related_to /a/[/r/related_to/,/c/en/smile/,/c/en/laugh/]\n",
      "izquierda\n",
      "smile - laughing | related_to /a/[/r/related_to/,/c/en/smile/,/c/en/laughing/]\n",
      "izquierda\n",
      "smile - laughing_silently | related_to /a/[/r/related_to/,/c/en/smile/,/c/en/laughing_silently/]\n",
      "izquierda\n",
      "smile - laughter | related_to /a/[/r/related_to/,/c/en/smile/,/c/en/laughter/]\n",
      "izquierda\n",
      "smile - like | related_to /a/[/r/related_to/,/c/en/smile/,/c/en/like/]\n",
      "izquierda\n",
      "smile - like_grin | related_to /a/[/r/related_to/,/c/en/smile/,/c/en/like_grin/]\n",
      "izquierda\n",
      "smile - like_laughing | related_to /a/[/r/related_to/,/c/en/smile/,/c/en/like_laughing/]\n",
      "izquierda\n",
      "smile - line | related_to /a/[/r/related_to/,/c/en/smile/,/c/en/line/]\n",
      "izquierda\n",
      "smile - lip | related_to /a/[/r/related_to/,/c/en/smile/,/c/en/lip/]\n",
      "izquierda\n",
      "smile - lip_action | related_to /a/[/r/related_to/,/c/en/smile/,/c/en/lip_action/]\n",
      "izquierda\n",
      "smile - lip_movement | related_to /a/[/r/related_to/,/c/en/smile/,/c/en/lip_movement/]\n",
      "izquierda\n",
      "smile - lips | related_to /a/[/r/related_to/,/c/en/smile/,/c/en/lips/]\n",
      "izquierda\n",
      "smile - lips_spread | related_to /a/[/r/related_to/,/c/en/smile/,/c/en/lips_spread/]\n",
      "izquierda\n",
      "smile - lips_teeth | related_to /a/[/r/related_to/,/c/en/smile/,/c/en/lips_teeth/]\n",
      "izquierda\n",
      "smile - look | related_to /a/[/r/related_to/,/c/en/smile/,/c/en/look/]\n",
      "izquierda\n",
      "smile - look_happy | related_to /a/[/r/related_to/,/c/en/smile/,/c/en/look_happy/]\n",
      "izquierda\n",
      "smile - miles | related_to /a/[/r/related_to/,/c/en/smile/,/c/en/miles/]\n",
      "izquierda\n",
      "smile - motion | related_to /a/[/r/related_to/,/c/en/smile/,/c/en/motion/]\n",
      "izquierda\n",
      "smile - mouth | related_to /a/[/r/related_to/,/c/en/smile/,/c/en/mouth/]\n",
      "izquierda\n",
      "smile - mouth_action | related_to /a/[/r/related_to/,/c/en/smile/,/c/en/mouth_action/]\n",
      "izquierda\n",
      "smile - mouth_curved | related_to /a/[/r/related_to/,/c/en/smile/,/c/en/mouth_curved/]\n",
      "izquierda\n",
      "smile - mouth_expression | related_to /a/[/r/related_to/,/c/en/smile/,/c/en/mouth_expression/]\n",
      "izquierda\n",
      "smile - mouth_gesture | related_to /a/[/r/related_to/,/c/en/smile/,/c/en/mouth_gesture/]\n",
      "izquierda\n",
      "smile - mouth_happy | related_to /a/[/r/related_to/,/c/en/smile/,/c/en/mouth_happy/]\n",
      "izquierda\n",
      "smile - mouth_movement | related_to /a/[/r/related_to/,/c/en/smile/,/c/en/mouth_movement/]\n",
      "izquierda\n",
      "smile - mouth_position | related_to /a/[/r/related_to/,/c/en/smile/,/c/en/mouth_position/]\n",
      "izquierda\n",
      "smile - movement | related_to /a/[/r/related_to/,/c/en/smile/,/c/en/movement/]\n",
      "izquierda\n",
      "smile - now | related_to /a/[/r/related_to/,/c/en/smile/,/c/en/now/]\n",
      "izquierda\n",
      "smile - now_frown | related_to /a/[/r/related_to/,/c/en/smile/,/c/en/now_frown/]\n",
      "izquierda\n",
      "smile - on_face | related_to /a/[/r/related_to/,/c/en/smile/,/c/en/on_face/]\n",
      "izquierda\n",
      "smile - on_lips | related_to /a/[/r/related_to/,/c/en/smile/,/c/en/on_lips/]\n",
      "izquierda\n",
      "smile - pictures | related_to /a/[/r/related_to/,/c/en/smile/,/c/en/pictures/]\n",
      "izquierda\n",
      "smile - pleasant | related_to /a/[/r/related_to/,/c/en/smile/,/c/en/pleasant/]\n",
      "izquierda\n",
      "smile - pleasant_expression | related_to /a/[/r/related_to/,/c/en/smile/,/c/en/pleasant_expression/]\n",
      "izquierda\n",
      "smile - position | related_to /a/[/r/related_to/,/c/en/smile/,/c/en/position/]\n",
      "izquierda\n",
      "smile - rainbow | related_to /a/[/r/related_to/,/c/en/smile/,/c/en/rainbow/]\n",
      "izquierda\n",
      "smile - response | related_to /a/[/r/related_to/,/c/en/smile/,/c/en/response/]\n",
      "izquierda\n",
      "smile - say | related_to /a/[/r/related_to/,/c/en/smile/,/c/en/say/]\n",
      "izquierda\n",
      "smile - say_cheese | related_to /a/[/r/related_to/,/c/en/smile/,/c/en/say_cheese/]\n",
      "izquierda\n",
      "smile - shine | related_to /a/[/r/related_to/,/c/en/smile/,/c/en/shine/]\n",
      "izquierda\n",
      "smile - show | related_to /a/[/r/related_to/,/c/en/smile/,/c/en/show/]\n",
      "izquierda\n",
      "smile - show_happiness | related_to /a/[/r/related_to/,/c/en/smile/,/c/en/show_happiness/]\n",
      "izquierda\n",
      "smile - show_happy | related_to /a/[/r/related_to/,/c/en/smile/,/c/en/show_happy/]\n",
      "izquierda\n",
      "smile - show_teeth | related_to /a/[/r/related_to/,/c/en/smile/,/c/en/show_teeth/]\n",
      "izquierda\n",
      "smile - showing | related_to /a/[/r/related_to/,/c/en/smile/,/c/en/showing/]\n",
      "izquierda\n",
      "smile - showing_happiness | related_to /a/[/r/related_to/,/c/en/smile/,/c/en/showing_happiness/]\n",
      "izquierda\n",
      "smile - showing_teeth | related_to /a/[/r/related_to/,/c/en/smile/,/c/en/showing_teeth/]\n",
      "izquierda\n",
      "smile - sign | related_to /a/[/r/related_to/,/c/en/smile/,/c/en/sign/]\n",
      "izquierda\n",
      "smile - silent | related_to /a/[/r/related_to/,/c/en/smile/,/c/en/silent/]\n",
      "izquierda\n",
      "smile - silent_laughing | related_to /a/[/r/related_to/,/c/en/smile/,/c/en/silent_laughing/]\n",
      "izquierda\n",
      "smile - silently | related_to /a/[/r/related_to/,/c/en/smile/,/c/en/silently/]\n",
      "izquierda\n",
      "smile - small | related_to /a/[/r/related_to/,/c/en/smile/,/c/en/small/]\n",
      "izquierda\n",
      "smile - small_grin | related_to /a/[/r/related_to/,/c/en/smile/,/c/en/small_grin/]\n",
      "izquierda\n",
      "smile - small_laugh | related_to /a/[/r/related_to/,/c/en/smile/,/c/en/small_laugh/]\n",
      "izquierda\n",
      "smile - spread | related_to /a/[/r/related_to/,/c/en/smile/,/c/en/spread/]\n",
      "izquierda\n",
      "smile - spreading | related_to /a/[/r/related_to/,/c/en/smile/,/c/en/spreading/]\n",
      "izquierda\n",
      "smile - spreading_lips | related_to /a/[/r/related_to/,/c/en/smile/,/c/en/spreading_lips/]\n",
      "izquierda\n",
      "smile - stretched | related_to /a/[/r/related_to/,/c/en/smile/,/c/en/stretched/]\n",
      "izquierda\n",
      "smile - stretched_face | related_to /a/[/r/related_to/,/c/en/smile/,/c/en/stretched_face/]\n",
      "izquierda\n",
      "smile - subtle | related_to /a/[/r/related_to/,/c/en/smile/,/c/en/subtle/]\n",
      "izquierda\n",
      "smile - subtle_laughter | related_to /a/[/r/related_to/,/c/en/smile/,/c/en/subtle_laughter/]\n",
      "izquierda\n",
      "smile - teeth | related_to /a/[/r/related_to/,/c/en/smile/,/c/en/teeth/]\n",
      "izquierda\n",
      "smile - teeth_lips | related_to /a/[/r/related_to/,/c/en/smile/,/c/en/teeth_lips/]\n",
      "izquierda\n",
      "smile - teeth_showing | related_to /a/[/r/related_to/,/c/en/smile/,/c/en/teeth_showing/]\n",
      "izquierda\n",
      "smile - tiny | related_to /a/[/r/related_to/,/c/en/smile/,/c/en/tiny/]\n",
      "izquierda\n",
      "smile - tiny_laugh | related_to /a/[/r/related_to/,/c/en/smile/,/c/en/tiny_laugh/]\n",
      "izquierda\n",
      "smile - upturned | related_to /a/[/r/related_to/,/c/en/smile/,/c/en/upturned/]\n",
      "izquierda\n",
      "smile - upturned_lips | related_to /a/[/r/related_to/,/c/en/smile/,/c/en/upturned_lips/]\n",
      "izquierda\n",
      "smile - upward | related_to /a/[/r/related_to/,/c/en/smile/,/c/en/upward/]\n",
      "izquierda\n",
      "smile - upward_lips | related_to /a/[/r/related_to/,/c/en/smile/,/c/en/upward_lips/]\n",
      "izquierda\n",
      "smile - verve | related_to /a/[/r/related_to/,/c/en/smile/,/c/en/verve/]\n",
      "izquierda\n",
      "smile - when | related_to /a/[/r/related_to/,/c/en/smile/,/c/en/when/]\n",
      "izquierda\n",
      "smile - when_happy | related_to /a/[/r/related_to/,/c/en/smile/,/c/en/when_happy/]\n",
      "izquierda\n",
      "smile - with_lips | related_to /a/[/r/related_to/,/c/en/smile/,/c/en/with_lips/]\n",
      "izquierda\n",
      "smile - with_mouth | related_to /a/[/r/related_to/,/c/en/smile/,/c/en/with_mouth/]\n",
      "izquierda\n",
      "smile - make_friends | used_for /a/[/r/used_for/,/c/en/smile/,/c/en/make_friends/]\n",
      "izquierda\n",
      "smile - amusement | related_to /a/[/r/related_to/,/c/en/smile/n/,/c/en/amusement/]\n",
      "izquierda\n",
      "smile - anxiety | related_to /a/[/r/related_to/,/c/en/smile/n/,/c/en/anxiety/]\n",
      "izquierda\n",
      "smile - expression | related_to /a/[/r/related_to/,/c/en/smile/n/,/c/en/expression/]\n",
      "izquierda\n",
      "smile - flex | related_to /a/[/r/related_to/,/c/en/smile/n/,/c/en/flex/]\n",
      "izquierda\n",
      "smile - happiness | related_to /a/[/r/related_to/,/c/en/smile/n/,/c/en/happiness/]\n",
      "izquierda\n",
      "smile - mouth | related_to /a/[/r/related_to/,/c/en/smile/n/,/c/en/mouth/]\n",
      "izquierda\n",
      "smile - muscle | related_to /a/[/r/related_to/,/c/en/smile/n/,/c/en/muscle/]\n",
      "izquierda\n",
      "smile - pleasure | related_to /a/[/r/related_to/,/c/en/smile/n/,/c/en/pleasure/]\n",
      "izquierda\n",
      "smile - tooth | related_to /a/[/r/related_to/,/c/en/smile/n/,/c/en/tooth/]\n",
      "izquierda\n",
      "smile - vocalisation | related_to /a/[/r/related_to/,/c/en/smile/n/,/c/en/vocalisation/]\n",
      "izquierda\n",
      "smile - facial_expression | is_a /a/[/r/is_a/,/c/en/smile/n/wn/communication/,/c/en/facial_expression/n/wn/communication/]\n",
      "izquierda\n",
      "smile - grimace | manner_of /a/[/r/manner_of/,/c/en/smile/v/wn/body/,/c/en/grimace/v/wn/body/]\n",
      "izquierda\n",
      "smile - express | manner_of /a/[/r/manner_of/,/c/en/smile/v/wn/communication/,/c/en/express/v/wn/communication/]\n"
     ]
    }
   ],
   "source": [
    "cn.relacion_word(\"smile\") #tug-of-war"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checar nuevos diccionarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set()\n",
      "{'humour', 'remembering', 'thanking', 'tasting_sweet', 'listening_to_music', 'meeting_people', 'enjoying_day', 'buying_shirt', 'going_to_performance', 'hiking'}\n"
     ]
    }
   ],
   "source": [
    "print(cn.df_diccionario_generales[\"smile\"][\"causes\"])\n",
    "print(cn.df_diccionario_especificas[\"smile\"][\"causes\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'related_to': {'beam',\n",
       "  'besmile',\n",
       "  'clown',\n",
       "  'crack_smile',\n",
       "  'duchenne_smile',\n",
       "  'expression',\n",
       "  'face',\n",
       "  'fish_eating_grin',\n",
       "  'frown',\n",
       "  'grin',\n",
       "  'grin_like_cheshire_cat',\n",
       "  'grip_and_grin',\n",
       "  'half_smile',\n",
       "  'happy',\n",
       "  'laugh',\n",
       "  'lip',\n",
       "  'mouth',\n",
       "  'nonsmile',\n",
       "  'nonsmiling',\n",
       "  'outsmile',\n",
       "  'pickin_and_grinnin',\n",
       "  'say_cheese',\n",
       "  'shit_eating_grin',\n",
       "  'simper',\n",
       "  'smile_from_ear_to_ear',\n",
       "  'smile_smirk',\n",
       "  'smiled',\n",
       "  'smileful',\n",
       "  'smileless',\n",
       "  'smilelike',\n",
       "  'smiler',\n",
       "  'smiles',\n",
       "  'smilest',\n",
       "  'smilet',\n",
       "  'smileth',\n",
       "  'smiley_face',\n",
       "  'smiling',\n",
       "  'smilingly',\n",
       "  'smilingness',\n",
       "  'smilish',\n",
       "  'smirk',\n",
       "  'smize',\n",
       "  \"straighten_one's_face\",\n",
       "  'unsmilingness'},\n",
       " 'form_of': {\"smil'd\",\n",
       "  'smiled',\n",
       "  'smiledst',\n",
       "  'smiles',\n",
       "  'smilest',\n",
       "  'smileth',\n",
       "  'smiling'},\n",
       " 'is_a': {'grin', 'simper', 'smirk'},\n",
       " 'part_of': set(),\n",
       " 'has_a': set(),\n",
       " 'used_for': set(),\n",
       " 'capable_of': {'human', 'infant_humans'},\n",
       " 'at_location': set(),\n",
       " 'entails': set(),\n",
       " 'causes': {'buying_shirt',\n",
       "  'enjoying_day',\n",
       "  'going_to_performance',\n",
       "  'hiking',\n",
       "  'humour',\n",
       "  'listening_to_music',\n",
       "  'meeting_people',\n",
       "  'remembering',\n",
       "  'tasting_sweet',\n",
       "  'thanking'},\n",
       " 'has_subevent': {'attending_classical_concert',\n",
       "  'conquer_nation',\n",
       "  'dreaming',\n",
       "  'eating_ice_cream',\n",
       "  'enjoy_company_of_friends',\n",
       "  'enjoy_day',\n",
       "  'enjoying_day',\n",
       "  'enjoying_film',\n",
       "  'expressing_information',\n",
       "  'flirt',\n",
       "  'flirting',\n",
       "  'get_good_grade',\n",
       "  'getting',\n",
       "  'getting_good_grade',\n",
       "  'getting_physical_activity',\n",
       "  'happy',\n",
       "  'have_fun',\n",
       "  'having_fun',\n",
       "  'having_good_time',\n",
       "  'hearing_crowd_go_mad',\n",
       "  'helping',\n",
       "  'howling_with_laughter',\n",
       "  'improving_image',\n",
       "  'killing_people',\n",
       "  'kiss',\n",
       "  'laugh',\n",
       "  'like',\n",
       "  'looking_better',\n",
       "  'making_friends',\n",
       "  'meet_girls',\n",
       "  'meet_people',\n",
       "  'meeting_friend',\n",
       "  'meeting_people',\n",
       "  'open_gift',\n",
       "  'opening_gift',\n",
       "  'people_happy',\n",
       "  'perform',\n",
       "  'performing',\n",
       "  'pretending',\n",
       "  'reading_letter',\n",
       "  'receiving_degree',\n",
       "  'remembering',\n",
       "  'see_favorite_show',\n",
       "  'sell_new_book',\n",
       "  'serve_customers',\n",
       "  'serving_customers',\n",
       "  'socialize',\n",
       "  'surprising',\n",
       "  'talking_to',\n",
       "  'taste_sweet',\n",
       "  'tasting_sweet',\n",
       "  'telling_many_people_about',\n",
       "  'thank',\n",
       "  'thanking',\n",
       "  'walk_in_rain',\n",
       "  'win_baseball_game',\n",
       "  'winning_baseball_game'},\n",
       " 'has_first_subevent': {'celebrate',\n",
       "  'chat_with_friends',\n",
       "  'copulate',\n",
       "  'enjoy_company_of_friends',\n",
       "  'enjoy_day',\n",
       "  'express_how_funny',\n",
       "  'flirt',\n",
       "  'get_good_grade',\n",
       "  'get_money_from',\n",
       "  'happy',\n",
       "  'have_food',\n",
       "  'have_fun',\n",
       "  'hear_news',\n",
       "  'hear_singing',\n",
       "  'lie',\n",
       "  'look_better',\n",
       "  'meet_friend',\n",
       "  'pass_university_exams',\n",
       "  'please_parents',\n",
       "  'reach_tentative_agreement',\n",
       "  'see_favorite_show',\n",
       "  'see_idea_become_reality',\n",
       "  'sell_new_book',\n",
       "  'serve_customers',\n",
       "  'socialize',\n",
       "  'stay_healthy',\n",
       "  'taste_sweet',\n",
       "  'thank',\n",
       "  'win_baseball_game'},\n",
       " 'has_last_subevent': {'agree_with',\n",
       "  'buy_hamburger',\n",
       "  'clean_house',\n",
       "  'copulate',\n",
       "  'entertain',\n",
       "  'flirt',\n",
       "  'give_assistance',\n",
       "  'kiss',\n",
       "  'listen_to_radio',\n",
       "  'look_better',\n",
       "  'read_letter',\n",
       "  'satisfy_appetites_with_minimum_effort',\n",
       "  'see_favorite_show'},\n",
       " 'has_prerequisite': {'agree_with',\n",
       "  'enjoy_day',\n",
       "  'entertain',\n",
       "  'have_fun',\n",
       "  'make_friends',\n",
       "  'serve_customers',\n",
       "  'work_box_office',\n",
       "  'working_box_office'},\n",
       " 'has_property': set(),\n",
       " 'motivated_by_goal': {'hear_music'},\n",
       " 'desires': set(),\n",
       " 'synonym': {'beam', 'grin', 'grinning', 'smiling'},\n",
       " 'antonym': {'cry', 'frown', 'scowl', 'sob'},\n",
       " 'distinct_from': set(),\n",
       " 'derived_from': {'archaic_smile',\n",
       "  'besmile',\n",
       "  'chelsea_smile',\n",
       "  'glasgow_smile',\n",
       "  'nonsmile',\n",
       "  'outsmile',\n",
       "  'semi_smile',\n",
       "  'smile_smirk',\n",
       "  'smileful',\n",
       "  'smileless',\n",
       "  'smilelike',\n",
       "  'smiler',\n",
       "  'smilest',\n",
       "  'smilet',\n",
       "  'smileth',\n",
       "  'smiley',\n",
       "  'smilie',\n",
       "  'smilish',\n",
       "  'smize',\n",
       "  'vertical_smile'},\n",
       " 'defined_as': set(),\n",
       " 'manner_of': {'beam', 'dimple', 'grin', 'smirk', 'sneer'},\n",
       " 'located_near': set(),\n",
       " 'has_context': set(),\n",
       " 'similar_to': set(),\n",
       " 'etymologically_related_to': set(),\n",
       " 'causes_desire': set(),\n",
       " 'made_of': set(),\n",
       " 'receives_action': set(),\n",
       " 'created_by': set()}"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cn.df_diccionario_especificas[\"smile\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'related_to': {'bite_and_smile',\n",
       "  'boat_person',\n",
       "  'bon_vivant',\n",
       "  'brook',\n",
       "  \"can't_get_enough\",\n",
       "  'delight',\n",
       "  'delight_in',\n",
       "  'disenjoy',\n",
       "  'dominate',\n",
       "  'employ',\n",
       "  'employment',\n",
       "  'enjoyed',\n",
       "  'enjoyer',\n",
       "  'enjoyest',\n",
       "  'enjoyeth',\n",
       "  'enjoying',\n",
       "  'enjoyment',\n",
       "  'enjoys',\n",
       "  'feel',\n",
       "  'fooder',\n",
       "  'foretaste',\n",
       "  'freedom_of_city',\n",
       "  'frowst',\n",
       "  'fruitive',\n",
       "  'fuck_with',\n",
       "  'fun_loving',\n",
       "  'get_behind',\n",
       "  'go',\n",
       "  'happy',\n",
       "  'have_ball',\n",
       "  'have_good_one',\n",
       "  'have_whale_of_time',\n",
       "  'home_field_advantage',\n",
       "  'joy',\n",
       "  'lap_up',\n",
       "  'like',\n",
       "  'live_and_let_live',\n",
       "  'make_whoopee',\n",
       "  'masochist',\n",
       "  'oneg',\n",
       "  'overenjoy',\n",
       "  'party',\n",
       "  'play',\n",
       "  'playable',\n",
       "  'property',\n",
       "  'read',\n",
       "  'reading',\n",
       "  'recreate',\n",
       "  'reenjoy',\n",
       "  'rejoy',\n",
       "  'relish',\n",
       "  'savour',\n",
       "  'seize_day',\n",
       "  'sensualist',\n",
       "  'stop_and_smell_roses',\n",
       "  \"suspend_one's_disbelief\",\n",
       "  'take_cash_and_let_credit_go',\n",
       "  'take_delight_in',\n",
       "  'tequilero',\n",
       "  'thirds',\n",
       "  'unbend',\n",
       "  'unenjoyed'},\n",
       " 'form_of': {\"enjoy'd\", 'enjoyed', 'enjoyest', 'enjoying', 'enjoys'},\n",
       " 'is_a': set(),\n",
       " 'part_of': set(),\n",
       " 'has_a': set(),\n",
       " 'used_for': {'cigars'},\n",
       " 'capable_of': {'young'},\n",
       " 'at_location': set(),\n",
       " 'entails': set(),\n",
       " 'causes': {'eating_hamburger',\n",
       "  'going_to_sporting_event',\n",
       "  'seeing_art',\n",
       "  'seeing_exhibits'},\n",
       " 'has_subevent': {'cogitating',\n",
       "  'eat_cookie',\n",
       "  'eating_cookie',\n",
       "  'getting',\n",
       "  'making_people_laugh',\n",
       "  'see_exciting_story',\n",
       "  'soaking_in_hotspring'},\n",
       " 'has_first_subevent': {'taste_sweet'},\n",
       " 'has_last_subevent': {'return_to_work'},\n",
       " 'has_prerequisite': {'attend_classical_concert', 'eat_ice_cream'},\n",
       " 'has_property': set(),\n",
       " 'motivated_by_goal': {'eat_ice_cream',\n",
       "  'fish',\n",
       "  'fly_kite',\n",
       "  'have_friends_over',\n",
       "  'listen_to_music',\n",
       "  'paint',\n",
       "  'play_baseball',\n",
       "  'play_chess',\n",
       "  'play_hockey',\n",
       "  'ride_horse',\n",
       "  'see_art',\n",
       "  'sew',\n",
       "  'sing',\n",
       "  'talk_to',\n",
       "  'taste_sweet',\n",
       "  'view_video',\n",
       "  'watch_musician_perform'},\n",
       " 'desires': {'person'},\n",
       " 'synonym': {'bask',\n",
       "  'delight_in',\n",
       "  'have_high_regard_for',\n",
       "  'like',\n",
       "  'relish',\n",
       "  'savor',\n",
       "  'savour',\n",
       "  'think_highly_of',\n",
       "  'think_well_of'},\n",
       " 'antonym': set(),\n",
       " 'distinct_from': set(),\n",
       " 'derived_from': {'disenjoy',\n",
       "  'enjoy_oneself',\n",
       "  'enjoyability',\n",
       "  'enjoyable',\n",
       "  'enjoyer',\n",
       "  'enjoyest',\n",
       "  'enjoyful',\n",
       "  'enjoyment',\n",
       "  'overenjoy',\n",
       "  'reenjoy'},\n",
       " 'defined_as': set(),\n",
       " 'manner_of': {'devour', \"feast_one's_eyes\"},\n",
       " 'located_near': set(),\n",
       " 'has_context': set(),\n",
       " 'similar_to': set(),\n",
       " 'etymologically_related_to': set(),\n",
       " 'causes_desire': set(),\n",
       " 'made_of': set(),\n",
       " 'receives_action': set(),\n",
       " 'created_by': set()}"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cn.df_diccionario_especificas[\"enjoy\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'lady', 'women', 'boy', 'female', 'girl', 'woman'}\n",
      "{'lady', 'person', 'machine', 'animal'}\n"
     ]
    }
   ],
   "source": [
    "print(cn.df_diccionario_generales[\"man\"][\"distinct_from\"])\n",
    "print(cn.df_diccionario_especificas[\"man\"][\"distinct_from\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'the'}\n",
      "{'outman', 'sportsman', 'unman', 'hundredman', 'aircraftman', 'nonman', 'warman', 'mangenue', 'munt', 'layman', 'freedman', 'manstopping', 'truckman', 'mansformation', 'villageman', 'axeman', 'manrope', 'sawman', 'tenderman', 'murse', 'manstopper', 'beadsman', 'manable', 'manwise', 'manhunting', 'manslut', 'shipman', 'manly', 'keyman', 'pivotman', 'manfiction', 'almsman', 'walkman', 'mancubine', 'mancart', 'manties', 'ragman', 'mankiller', 'pointsman', 'freshman', 'hillman', 'demiman', 'newsman', 'wicker_man', 'manroot', 'mankini', 'goodman', 'manboy', 'deman', 'mankind', 'mancession', 'niggerman', 'man_bag', 'fieldman', 'bassman', 'mizzentopman', 'manness', 'masman', 'gownman', 'manimal', 'baseman', 'mothman', 'cis_man', 'backman', 'manbote', 'manface', 'mansplaining', 'mancation', 'mailman', 'mandom', 'manfriend', 'hardman', 'bonesman', 'songman', 'manway', 'longbowman', 'halseman', 'manquelling', 'leatherman', 'mangina', 'earthman', 'frontman', 'jujuman', 'manorexia', 'manshake', 'puppetman', 'manlift', 'muscleman', 'herdman', 'lawman', 'gunman', 'beman', 'manparts', 'manslaughterer', 'nurseryman', 'questman', 'maneen', 'junkman', 'highwayman', 'decoyman', 'deskman', 'swineman', 'manlet', 'yeggman', 'mirdle', 'manizer', 'yes_man', 'swagman', 'pitchman', 'remainder_man', 'mantrum', 'discman', 'centreman', 'manrent', 'boilerman', 'wombman', 'swingman', 'backwoodsman', 'manhair', 'eighthman', 'manpris', 'man_date', 'netherman', 'manpower', 'draftman', 'hireman', 'manwards', 'manimony', 'chapelman', 'mankin', 'peterman', 'counterman', 'manling', 'manthing', 'disman', 'fremman', 'highman', 'whoreman', 'hypeman', 'commonwealthman', 'fireman', 'tripeman', 'rebbetzman', 'manpurse', 'triggerman', 'manpage', 'talesman', 'manscara', 'taximan', 'mannequin', 'mansicle', 'leman', 'manbitch', 'badman', 'jewman', 'utilityman', 'workman', 'manarchist', 'lizardman', 'rocketman', 'manlock', 'manship', 'manstealing', 'iceman', 'mansome', 'newman', 'manhole', 'committeeman', 'he_man', 'moneyman', 'stick_it_to_man', 'manwich', 'freeman', 'manhunter', 'manopause', 'candyman', 'brotherman', 'derrickman', 'manhunt', 'cyberman', 'rain_man', 'manqueller', 'mannie', 'maneating', 'manspreading', 'deadman', 'silkman', 'chineseman', 'maness', 'lemman', 'manhater', 'manboob', 'manslaughtering', 'manwhore', 'addyman', 'mantastic', 'linesman', 'manslaughter', 'wolfman', 'mantyhose', 'manhating', 'canoeman', 'megaman', 'horseman', 'careerman', 'airman', 'underman', 'batman', 'caveman', 'mansplain', 'mantuary', 'gentleman', 'apeman', 'bossman', 'manbulge', 'vatman', 'manlike', 'cattleman', 'manpack', 'manward', 'manservant', 'meatman', 'beggarman', 'manbaby', 'manbot', 'subman', 'screwman', 'taxman', 'marshman', 'blindman', 'manslaying', 'frontiersman', 'foreman', 'mantrap', 'alongshoreman', 'sandman', 'townman', 'hoodman', 'manpanzee', 'transman', 'meggings', 'scienceman', 'foeman', 'mermin', 'rastaman', 'manslaught', 'overman', 'aidman', 'spoilsman', 'crossbowman', 'antiman', 'ironman', 'manzilian', 'manpain', 'manful', 'manscape', 'manorexic', 'paceman', 'manlover', 'loverman', 'manther', 'manstealer', 'manhood', 'anchorman', 'youthman', 'moob', 'locoman', 'limerickman', 'mandem', 'man_made', 'manny', 'bogeyman', 'bookman', 'merm', 'panman', 'seaman', 'superman', 'dangerman', 'mantit', 'frogman', 'infantryman', 'rescueman', 'yesman', 'nepman', 'mandrill', 'portman', 'moonman', 'north_countryman', 'hellman', 'manchild', 'manstress', 'skyman', 'reman'}\n"
     ]
    }
   ],
   "source": [
    "print(cn.df_diccionario_generales[\"man\"][\"derived_from\"])\n",
    "print(cn.df_diccionario_especificas[\"man\"][\"derived_from\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'weapon_of_mass_destruction'}\n",
      "set()\n"
     ]
    }
   ],
   "source": [
    "print(cn.df_diccionario_generales[\"wmd\"][\"synonym\"])\n",
    "print(cn.df_diccionario_especificas[\"wmd\"][\"synonym\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'rupture', 'collapse', 'transgress', 'contravene', 'dampen', 'burst', 'interrupt', 'demote', 'pause', 'violate', 'break_dance', 'open_frame', 'better', 'split', 'fault', 'break_down', 'break_in', 'breach', 'disclose', 'interruption', 'crack', 'time_out', 'recess', 'fail', 'break', 'breakage', 'respite', 'bankrupt', 'separate', 'fracture'}\n",
      "{'snap_off', 'jailbreak', 'burst', 'break_up', 'kill', 'percussion_break', 'day_off', 'gap', 'rest', 'prison_breaking', 'stop', 'get_out', 'bust', 'split_up', 'wear', 'break_away', 'discontinue', 'happy_chance', 'gaolbreak', 'breach', 'fall_apart', 'break_off', 'erupt', 'disruption', 'lucky_break', 'interruption', 'breakout', 'good_luck', 'cave_in', 'break_of_serve', 'break_out', 'recess', 'get_around', 'hiatus', 'destroy', 'recrudesce', 'develop', 'thrust', 'wear_out', 'come_apart', 'separate', 'prisonbreak'}\n"
     ]
    }
   ],
   "source": [
    "print(cn.df_diccionario_generales[\"break\"][\"synonym\"])\n",
    "print(cn.df_diccionario_especificas[\"break\"][\"synonym\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'possess', 'must', 'have_got', 'accept', 'own', 'take', 'hold', 'rich_person', 'get', 'know', 'eat', 'induce', 'give_birth', 'partake', 'suffer', 'bear', 'sleep_with', 'receive', 'conquer', 'experience', 'consume'}\n",
      "{'got', 'hold', 'appear', 'get', 'experience', 'feature', 'acquire', 'have_got', 'rook', \"i've\", 'take', 'haven’t', 'make', 'carry'}\n"
     ]
    }
   ],
   "source": [
    "print(cn.df_diccionario_generales[\"have\"][\"synonym\"])\n",
    "print(cn.df_diccionario_especificas[\"have\"][\"synonym\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'mechanical_device', 'electronic_device', 'contestant', 'person', 'consumer_electronics'}\n",
      "{'playmaker', 'volleyball_player', 'movie_projector', 'jukebox', 'pool_player', 'video_game_platform', 'server', 'audio_tape_player', 'billiard_player', 'lacrosse_player', 'shooter', 'stringer', 'mini_disc_player', 'card_player', 'slide_projector', 'digital_audio_file_player', 'video_tape_deck_mechanism', 'dvd_player', 'bowler', 'seeded_player', 'tennis_player', 'compact_disc_player', 'laser_disc_player', 'record_player', 'most_valuable_player', 'soccer_player', 'hockey_player', 'ballplayer', 'golfer', 'overhead_projector', 'chess_player', 'football_player', 'scorer', 'pitcher', 'dart_player', 'grandmaster'}\n"
     ]
    }
   ],
   "source": [
    "print(cn.df_diccionario_generales[\"player\"][\"is_a\"])\n",
    "print(cn.df_diccionario_especificas[\"player\"][\"is_a\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'love', 'delight'}\n",
      "{'smiling', 'grin', 'grinning', 'beam'}\n"
     ]
    }
   ],
   "source": [
    "print(cn.df_diccionario_generales[\"enjoy\"][\"synonym\"])\n",
    "print(cn.df_diccionario_especificas[\"smile\"][\"synonym\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'homo', 'timepiece', 'human', 'body', 'arm'}\n",
      "{'metacarpal_vein', 'digital_arteries', 'index_finger', 'fingernail', 'long_suit', 'palm', 'finger', 'intercapitular_vein', 'ball', 'metacarpal_artery', 'metacarpus'}\n"
     ]
    }
   ],
   "source": [
    "print(cn.df_diccionario_generales[\"hand\"][\"part_of\"])\n",
    "print(cn.df_diccionario_especificas[\"hand\"][\"part_of\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cattle', 'heard'}\n",
      "{'poll', 'udder'}\n"
     ]
    }
   ],
   "source": [
    "print(cn.df_diccionario_generales[\"cow\"][\"part_of\"])\n",
    "print(cn.df_diccionario_especificas[\"cow\"][\"part_of\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'floor', 'kitchen', 'library', 'bedroom', 'dining_room', 'livingroom', 'rug', \"neighbor's_house\", 'house', 'room', 'demonstration', 'doctor', 'apartment', 'meeting', 'internet_cafe', 'resturant', 'furniture_store', 'card_room', 'corner', 'conference', 'building', 'loft'}\n",
      "{'floor', 'mess', 'shoes', 'hungry_dog', 'knife', 'magazine', 'picture_in_frame', 'wax', 'stuff', 'apricot', 'dictionary', 'coaster', 'food_scraps', 'dinner_plate', 'bottle_of_vodka', 'setting', 'people_eating', 'crisps', 'knife_and_fork', 'place_setting', 'unpolished_surface', 'magazines', 'lamp', 'salt_and_pepper', 'begging_dog_or_cat', 'book', 'napkin_holder', 'lots_of_books', 'carpet', 'computer_monitor', 'knees', 'pets', 'eating_utensils', 'column', 'salt_shaker', 'ashtray', 'place_mat', 'cutlery', 'mail', 'used_gum', 'forks', 'apples', 'leg', 'center_piece', 'placemat', 'dog', 'computer', 'legs', 'polish', 'food', 'legs_of_table', 'flowers', 'note', 'dinner', 'plant', 'crumbs', 'pool_balls', 'picture', 'place_mats', 'tea_spoon', 'book_of_pictures', 'game', 'pen', 'side_chair', 'handout', 'vase_with_flowers', 'cloth', 'candelabra', 'flower_arrangement', 'tablecloth_covering', 'bottle_of_wine', 'space', 'vase', 'varnish', 'table_cloth', 'napkin', 'sign_out_sheet', 'books', 'business_report', 'patient', 'map', 'papers', 'dish', 'message', 'row', 'salt_and_pepper_shakers', 'poker_chips', 'office_equipment', 'furniture_polish', 'plate', 'fork', 'rug', 'cat', 'campaign_finance_reform', 'bowl', 'menu', 'gum', 'ants', 'felt', 'tablecloth_and_napkins', 'breakfast', 'table_cloth_clock', 'puzzle', 'tablecloth', 'kitten', 'glass', 'pen_or_pencil', 'coffee_table_book', 'spoon', 'candelabera', 'sign_up_sheet', 'spilled_milk', 'dishes', 'potted_plant', 'families_feet', 'glass_of_water', 'bowl_of_flowers', 'restaurant_bill', 'cup_of_coffee', 'doodads', 'pepper', 'keys', 'vase_of_flowers', 'fresh_cut_flowers', 'candle', 'cards', 'log_book', 'pool_sticks', 'foot', 'dog_begging', 'clock', 'centerpiece', 'plates', 'cup', 'yours_if_nobody_else_claims', 'dust', 'onions', 'paper_and_pencil', 'silverware', 'toy', 'sign_in_sheet', 'jigsaw_puzzle', 'rub', 'persons_legs', 'brief_case', 'plates_and_forks', 'feet', 'tableware_or_dishes', 'dried_up_food', 'bowl_of_fruit', 'reference_book', 'puppy', 'table_legs', 'candlestick', 'condiments', 'condimemts', 'set_of_silverware', 'support_pole', 'dog_waiting_to_get_lucky', 'salt', 'list'}\n"
     ]
    }
   ],
   "source": [
    "print(cn.df_diccionario_generales[\"table\"][\"at_location\"])\n",
    "print(cn.df_diccionario_especificas[\"table\"][\"at_location\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'playing_games', 'having_fun', 'inflating', 'bearings', 'bowling', 'tennis', 'throwing', 'game', 'playing_basketball', 'catching', 'hitting', 'play', 'playing_football', 'kicking', 'playing_baseball', 'play_golf', 'rolling', 'bouncing', 'bearing', 'playing_game', 'bounce', 'roll', 'dancing_at', 'playing_soccer', 'play_with_dog', 'do_pilate_exercises'}\n",
      "{'dressing_nice'}\n"
     ]
    }
   ],
   "source": [
    "print(cn.df_diccionario_generales[\"ball\"][\"used_for\"])\n",
    "print(cn.df_diccionario_especificas[\"ball\"][\"used_for\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'travel_through_many_time_zones', 'reach_destination', 'circle_airfield', 'land_at_airstrip', 'arrive_at_airport', 'roll_down_runway', 'cross_ocean', 'circle_airport_before_landing', 'circle_airport', 'land_in_field', 'lift_person', 'carry_immigrants', 'crash_if_goes_wrong', 'circle_around_airport', 'land_on_landing_strip', 'travel_faster_than_car', 'fly_upwards', 'travel_across_world', 'seat_passengers', 'bank_quickly'}\n",
      "set()\n"
     ]
    }
   ],
   "source": [
    "print(cn.df_diccionario_generales[\"airplane\"][\"capable_of\"])\n",
    "print(cn.df_diccionario_especificas[\"knife\"][\"capable_of\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'take_drug', 'close_eyes', 'go_to_sleep', 'asleep', 'sleep_peacefully', 'sleep', 'fall_asleep', 'smoke_opium', 'got_to_sleep', 'smoke_pot'}\n",
      "{'awake'}\n"
     ]
    }
   ],
   "source": [
    "print(cn.df_diccionario_generales[\"dream\"][\"has_prerequisite\"])\n",
    "print(cn.df_diccionario_especificas[\"dream\"][\"has_prerequisite\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'cold_to_touch', 'water_turned_solid', 'solid', 'cold', 'frozen_liquid', 'melting', 'less_dense_than_water', 'hard_and_cold', 'better_to_drink_champagne', 'hard', 'cool', 'frigid'}\n",
      "{'flowers', 'consequence_of_freezing_water', 'solid_water', 'frozen_water', 'solid_form_of_water'}\n"
     ]
    }
   ],
   "source": [
    "print(cn.df_diccionario_generales[\"ice\"][\"has_property\"])\n",
    "print(cn.df_diccionario_especificas[\"ice\"][\"has_property\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'challenge_youself_against_other', 'gain', 'natural_selection_tells_to', 'try_to_win', 'good_at', 'achiever', 'win', 'improve_own_performance', 'fun', 'pleasure_of_winning', 'test_own_powers', 'like_to_win', 'win_against_opponent', 'there_only_one_opening_available', 'victory_over_opponent'}\n",
      "{'play_soccer', 'play_football', 'play_cards', 'win_baseball_game', 'play_chess', 'reach_advantage', 'play_game_of_chess', 'conquer_opponent', 'fight_war', 'play_game', 'play_games', 'prepare_for_vote', 'play_sports', 'play_poker', 'advance_into_battle', 'play_hockey', 'compete', 'run_after_ball'}\n"
     ]
    }
   ],
   "source": [
    "print(cn.df_diccionario_generales[\"compete\"][\"motivated_by_goal\"])\n",
    "print(cn.df_diccionario_especificas[\"win\"][\"motivated_by_goal\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'feel_in_charge', 'sunny_days', 'have_opinion_respected', 'wise_and_experienced_teacher', 'bare_truth', 'others_to_content', 'have_fun_in_life', 'luck', 'time_to_think', 'hunger', 'complaints_to_addressed', 'goals', 'functioning_internal_organs', 'go_forth_into_wilderness', 'look_attractive', 'fitting_answer', 'stay_young', 'accomplish_things', 'safe_inebriants', 'good_salary', \"feel_satisfied_with_one's_self\", 'tobacco', 'write_memoirs', 'sleep_in_in_morning', 'romance', 'sing_and_dance', 'useful_objects', 'admire_solar_eclipse', 'return_borrowed_key', 'individual', 'respected_by_boss', 'good_feelings', 'just_right_amount_of_food', 'good_night_of_sleep', 'what_person_wants', 'avoid_opera', 'wisdom', 'meet_expectations', 'green_grass', 'understand_difficult_situations', 'warm_weather', 'keen_sense_of_smell', 'good_well_paying_job', 'sing_and_dance_in_parade', 'leave_mark', 'exact_answer', 'meet_other_persons', 'fascinating_rhythm', 'part_of_loving_family', 'dominate_other_people', 'have_good_memory', 'see_sunset_and_sunrise', 'things_others_have', 'sexual_encounters', 'hot_nasty_cybersex', 'salvation', 'life_and_liberty', 'staple_papers', 'shiny_hair', 'have_alot_of_money', 'sitting_by_stream', 'feel_ecstatic', 'have_friends', 'traction_from_tires', 'complimented', 'enjoy_fruits_of_labor', 'human_contact', 'hear_nice_music', 'good_lover', 'go_home', 'drink_fresh_water', \"get_good_night's_sleep\", 'get_ahead', 'go_beyond_dreams', 'esteem', 'frosting_on_cake', 'anykind_of_sex_life', 'essential_answer', 'sleep_in_sometimes', 'computer_to_find_pattern', 'love_and_respect', 'go_around_corner', 'clean_dishes', 'understand_motives', 'control_of_own_property', 'jewellery', 'pick_herself_up_off_ground', 'fast_internet_access', 'see_peace_in_lifetime', 'nice_computer', 'keep_secrets_to_themselves', 'car_runs_well', 'live_without_government_oversite', 'everything_to_go_smoothly', 'know_topic_being_discussed', 'roam_free', 'french_food', 'jog', 'sexual_gratification', 'interesting_experiences', 'eat_good_tasting_food', 'revenge_for_serious_wrong', 'food_clothing_and_shelter', 'intoxicated', 'help_relatives', 'interact_with_other_people', 'avoid_discomfort', 'land', 'english_house', 'satisfactory_results', 'status_among_peers', 'have_loving_family', 'dignity', 'interesting_books', 'live_long_healthy_life', 'get_education', 'surprised', 'sucessful', 'short_travel_times', 'college_degree', 'look_nice_for_significant_other', 'peace_in_time', 'loyalty_of_friends', 'connection_and_vital_psychological_well_being', 'feet', 'inner_peace', 'end_to_world_hunger', 'good_things_in_life', 'feel_rested', 'safty', 'cable_modem', 'get_fresh_air', 'meet_mot_hoople', 'understand_themselves', 'itch_to_scratched', 'more_intelligent_than_computer', 'crispy_potato_chip', 'praised', 'swin', 'buy_more_for_less_money', 'reliability', 'see_meteor_shower', 'sense_of_community', 'enjoyable_sex_with_lover', 'flash_car', 'promoted_in_job', 'live_long_and_happy_life', 'keep_in_touch_with_family_members', 'run', 'loving_friends_and_family', 'soft_skin', 'blowjob', 'pretzels_and_cheese', 'have_diamonds', 'ability_to_travel', 'explanations', 'fresh_food', 'other_people_to_like', 'eat_lunch', 'respond_to_complaint', 'stable', 'freindship', 'pretty', 'interesting_things_to_happen', 'fair_wage', 'mockingbirds_outside', 'nothing_to_do_with_computer_ai', 'laws_actually_work', 'physically_near_beloveds', 'live_in_comfort', 'good_prognosis', 'cup_of_really_good_coffee', 'drink_cool_clear_water', 'opportunity_to_defend_herself', 'celebrate', 'belief', 'understand_how_own_brains_work', 'close_sale', 'appropriate_medical_care', 'porn', 'honest_appreciation', 'independence', 'live_in_nice_place', 'have_easy', 'answers_to_universe', 'have_clear_conscious', 'cuddle_up_with', 'oral_sex', 'patient_spouse', 'have_femily', 'worldwide_peace_and_love', 'traction_to_draw_broken_bones_together', 'cut_cheese', 'good_interest_rates', 'choices', 'score_winning_goal', 'time_away', 'confident', 'mate_to_find_attractive', 'things_to_better', 'good_science', 'sec', 'vengence', 'feel_bliss', 'inoculated', 'embrace_all_universal_philosophies', 'richer_than_anybody_else', 'pug_dog', 'finish_crossword_puzzle', 'songbirds_in_yard', 'lots_of_cuddles', 'elegant', 'trophy_wife', 'other_people_to_smile_at', 'people_to_pronounce_name_correctly', 'friends_to_overlook_imperfections', 'soft_blanket', 'sucseed', 'giraffe', 'spouse', 'evidence', 'discover_meaning_in_life', 'moral', 'good_desserts', 'good_guard_dog', 'continue_to_learn_until_die', 'able_to_care_for_themselves', 'have_alot_of_sex', \"mom's_cooking\", 'material_things', 'contradict_lies', 'silliness_and_laughter', 'best_things_for_family', 'hear_sincerity', 'receive_token_of_affection', 'hot_water_for_bathing', 'communicate', 'just_laws', 'face_accuser', 'good_family', 'mobile', 'sing', 'lunch_money', 'supporting_frame_to_remain_firm', 'strong_body', 'hot_meal', 'eat_in_peace', 'pug', 'trust_people_love', 'tacos', 'healthy_family_members', 'fair', 'time', 'ice_cream_cone', \"learn_from_other_people's_experiences\", 'fortune_to_smile_on', 'diversity', 'else_to_clean_up_messes', 'broadband_internet_connection', 'avoid_effort', 'sweets', 'clean', 'nice_haircut', 'have_name', 'express_gratitude', 'significant_other', 'win_lots_of_money', 'know_secrets', 'more_of_material_things', 'things_to_last_long', 'free_refill', 'save_money', 'ask_questions', 'get_more_vacation_days', 'breathe_clean_air', 'dish_pan_hands', 'have_shelter_from_weather', 'loving_pets', 'live_long_and_healthy_life', 'book_on_subject', 'support_from_loved_ones', 'snack_from_time_to_time', 'snacks', 'fill_in_collection', 'with_friendly_people', 'mental_challenge', 'lose_weight', 'antiques', 'die_with_dignity', \"other_people's_company\", 'well_fed', 'understand_themself', 'opponent_to_play_fairly', 'get_way', 'conversation', 'thing_way', 'self_reliant', 'eat_good_food_each_day', 'reach_goals', 'acknowledgment', 'reach_summit', 'ward_off_evil_forces', 'company_of_other_people', 'more_money', 'children_to_succeed', 'taller', 'healthy_communication_between_countries_leaders', 'live_life_on_own_terms', 'motivation', 'paid_promptly', 'naked_and_free', 'all_can_get', 'accurate_results', 'useful_information', 'respond_to_charge', 'sleep_comfortably', 'others_to_understand', 'explain', 'travel_and_see_world', 'stretch', 'de_different_from_other_people', 'calm_mind', 'maintain_balance', 'good_credit', 'peace_and_harmony', \"good_night's_sleep\", 'have_choice', 'put_different_pieces_of_information_together', 'fresh_air', 'cherish', 'enjoy_nature_to_fullest', 'clever', 'considered_attractive', 'intellectual_challenge', 'garlic', 'generous_friends', 'good_chinese_food', 'everything', 'sleep_undisturbed', 'hot_wife', 'find_aesthetically_pleasing_objet_trouves', 'breathe_fresh_air', 'blowjobs', 'food_shelter_and_security', 'more_of_everthing', 'good_shoes', 'feel_proud_of_children', 'go_economy', 'flowers', 'alone_when_using_toilet', 'success_for_others', 'quality_of_life', 'prettiest', 'working_refrigerator', 'beautiful_craftsmanship', 'closure', 'clean_and_healthy_skin', 'live_with_respect', 'taken_seriously', 'caring_parents', 'get_on_wth_others', 'live_long_and_prosper', 'agreement', 'flattered', 'team_to_win', 'teach_others_truth', 'bagel', 'understand_why_things_happen', 'shade_trees', 'show_off', 'good_career', 'rewarding_job', 'successful_life', 'never_fail', 'caring_relationship', 'forgiven_for_sins', 'fresh_brea', 'constantly_learn_new_things', 'eat_pizza', 'understand_laws_govern', 'able_to_visit_with_friends', 'uninterrupted_sleep', 'toys', 'good_vision', 'collection_of_plush_hedgehogs', 'raise', 'american_dream', 'tv', 'have_good_feelings', 'go_through_life_happy', 'buy_car', 'new_experiences', 'happy_and_healthy', 'beautiful_things_to_happen', 'better_place_to_live', 'good_well', 'teach_others', 'science_and_art', 'procrastinate', 'tart_green_apples', 'able_to_stand', 'live_in_loving_enviroment', 'read_good_books', 'convenient_parking', 'volcanic_eruptions', 'own_nice_things', 'enjoy_work_from_distance', 'perfect_10', 'fed', 'live_for_at_least_80_years', 'loyalty', 'keep_clean', 'able_to_walk', 'clean_underwear', 'good_eyesight', 'everything_everyone_else_has', 'nice_pet', 'picture_of_friend', 'make_contribution_to_world', 'sed', 'video_game', 'have_good_friends', 'poetry_music_literature_in_lives', 'enlightment', 'prompt_responses_from_computers', 'preserve_antique', 'good_looks', 'jannah', 'beautiful_surroundings', 'transcend_corporal_limits', 'forget_bad_memories', 'cake', 'talk_to_interesting_person', 'financial_security', 'fulfilling_career', 'autonomous', 'live_in_harmony_with_other_people', 'liked_by_other_people', 'feel_wanted', 'money_power_or_love', 'remain_active_when_old', 'food_every_day', 'smell_nice', \"what_other_people_don't_have\", 'honest_person', 'intelligence_logic_and_reason', 'understood_by_others', 'warned_of_danger', 'orgasm', 'blonde_hair', 'spare_time', 'get_through_tough_times', 'get_interesting_information', 'told_truth', 'cold_beer_and_warm_women', 'relaxation', 'kind_and_generous_to_others', 'bargain', 'box_of_foxes', 'hike_with_companion', 'find_missing_painting', 'of_sound_body_and_mind', 'soft_carpet', 'divorce_abusive_spouse', 'quick_response_to_inquiry', 'accomplish_goals', 'heal_quickly', 'applause', 'vacation', 'solutions', 'cassette_player_in_truck', 'magic', 'play_sports', 'recognised', 'have_good_relationships_with_others', 'think', 'nice_boquet_of_flowers', 'more_then', 'ice_cream_to_frozen', 'sunny_day', 'comfort_from_others', 'self_confidence', 'live_someplace_dry_and_warm', 'sustain_or_existence', 'variety_but_consistency_as_well', 'influential', 'pleasant_things', 'fullfill_goals', 'cherokee', 'good_education', 'good_body', 'fancy_coffee', 'best_things_in_life', 'feel_understood', 'understand_effect_of_action', 'many_good_friends', 'beautiful_wife', 'other_than_television_for_entertainment', 'around_spiritual_people', 'eat_food', 'live_long_pain_free_life', 'vacations', 'quick_response', 'satisfy_sexual_appetite', 'eat_everyday', 'reach_mountain_summit', 'free_will', 'jump_in_puddles', 'flat_stomach', 'high_achiever', 'kisses_from_mate', 'warm_place_to_sleep', 'know_what_other_people_thinking', 'read_interesting_book', 'white_cheese_and_red_wine', 'have_large_vocabulary', 'call_jenny_at_867_5309', 'make_good_decision', 'cut_paper', 'life_forever', 'icecream', 'understand_place_in_world', 'love_another', 'try_wings', 'contented', 'day_off', 'become_rich', 'fresh_water_to_drink', 'tastey_food', 'accomplish_goal', 'traction_for_foot_race', 'smell_flowers', 'tobe_well_and_live_well', 'explore_space', 'rich_and_full_life', 'trust_other_people', 'support_from_others', 'understand_nature_of_things', 'read_slashdot', 'spiritual_fulfillment', 'accept_bodies_as', 'see_life_mean', 'write_poems', 'live_healthy', 'love_of_mother', 'land_in_right_place', 'pool_on_hot_dat', 'latest_technology', 'eat_good_meal', 'mere_truth', 'find_right_partner', 'read_and_read_and_read', 'sense_of_well_being', 'watch_good_movies', 'big_yacht', 'know_about_ancestors', 'education', 'temper_justice_with_mercy', 'philanthropy_to_help_others', 'fucked', 'praised_and_loved', 'get_in_accident', 'good_memory', 'well_thought_of', 'self_esteem', 'create_beauty_in_world', 'loved_ones_to_succeed', 'own_home', 'respected_in_society', 'run_barefoot_through_grass', 'action_not_luck', 'good_benefits_package', 'slim_waist', 'earn_living', 'go_to_baseball_game', 'help', 'part_of_group', 'fever_to_break', 'good_things_to_happen', 'go_to_good_movie', 'treated_lumber_when_building_deck', 'belongings_to_have_maximum_life', 'large_computer_monitor', 'discover_new_things', 'goal_to_achieve', 'knowledgable', 'convenience', 'see_lunar_eclipse', 'in_mail', 'assfucking', 'competent_teacher', 'computers_to_work_properly', 'freedom_from_want', 'venture_into_world', 'bodies_to_healthful', \"pedicure_because_it's_heavenly\", 'screwed_in_passage', 'parents_to_happy', 'see_budding_tree', 'pure_drinking_water', 'pleasant_memories', 'spice_in_life', 'candles_with_burning_wicks', 'catch_brass_ring', 'invent_new_tools', 'home_to_live_in', 'marry_sometime_in_there_life', 'money_fun_life', 'known_s_good_person', 'find_four_leaf_clover', 'go_through_life_in_fine_health', 'not_to_feel_patronised', \"other_people's_respect\", 'traction_on_track', 'catch_flying_trapeze', 'pure_truth', 'applaud_excellent_performance', 'confidence', 'forgiveness', 'things_to_clean', 'cry_at_times', 'perfect_life', 'smooth_plane_ride', 'favorable_success', 'respected_by_colleagues', 'variety_of_activities', 'perform', 'positive_attention_from_others', 'run_out_of_gas', 'sugar', 'housebroken_pet', 'simple_truth', 'get', 'know_where_came_from', 'enough_money_to_live_without_worry', 'relieve_themselves', 'ice_cream_in_summer', 'move', 'bring_others_to_jesus_christ', 'know_what_time', 'steamed_milk_in_latte', 'find_solution_to_problem', 'experience_good_things_in_life', 'discounts', 'create_beautiful', 'wake_up_everyday', 'strong_joints', 'maps_when_travelling', 'good_house', 'like_idols', 'other_people_to_believe_in', 'get_back_to_nature', 'both_love_and_sex', \"everyone's_approval\", 'good_pet', 'washing_face', 'health_insurance', 'feel_desirable', 'good_batteries', 'nice_hug', 'explore', 'come_home_from_war', 'tropical_vacations', 'good_home', 'honor_individuality_in_others', 'orgasms', 'have_influence', 'study', 'see_full_crop', 'ba_monarch', 'never_to_sleep', 'clothes_fit', 'proof', 'feel_like_s_belongs', 'tickled', 'satisfy_self_interests', 'sunlight', 'give_happiness_to_others', 'self_gratification', 'escape', 'fellowship', 'children_to_respected', 'shiny_leather_shoes', 'impress_opposite_sex', 'nourishment', 'hear_themselves_think', 'pet_cat', 'good_car', 'smooth_airplane_ride', 'free_booze', 'love_money_success', 'accurate_watch', 'free_will_and_choice', '1337', 'mind_blowing_sex', 'best_can', 'satisfied_with_professional_life', 'feel_comfort', 'god', 'tax_refund', 'respect_from_others', 'live_as_long_as_possible', 'few_close_friends', 'find_similarties_between_things', 'travel_around_universe', 'please_themselves', 'science', 'able_to_tell_time', 'quality_bicycle', 'touch_soft_skin', 'win_lottery', \"what_person_doesn't_have\", 'perfect_answer', 'around_others_like', 'prosperous_life', 'succed', 'have_good_bones', 'snake', 'enjoy_work', 'know_truth', 'hold', 'ride_horses', 'clarity', 'acceptance_in_society', 'relax_sometimes', 'indoor_plumbing', 'thick_hair', 'added', 'on_time', 'good_kids', 'good_vodka', 'drive', 'hapiness', 'good_warranty_on_products', 'complete_collection', 'not_expend_effort', 'fulfill_need_for_acheivement', 'clear_vision', 'valued_by_others', 'eat_sweets', 'love_and_security', 'mangos', 'gain_respect', 'big_family', 'own_space', 'food_and_water', 'plenty_of_sleep', 'three_hot_meals_day', 'morals', 'work_to_efficient', 'learn_new_words', 'prosperity', 'best_things_from_life', 'happy_times', 'dance_in_meadow', 'watch_jetsons', 'understand_world', 'step_over_cord', 'peaceful', 'please_senses', 'see_naked_women', 'afford_food', 'introduce_people_to_each_other', 'free_of_fear', 'children_to_safe', 'relationship_with_younger_man', 'accomplish_good_things', 'find_cure_for_lupus', 'look_around', 'drink_wine', 'build', 'honored', 'good_grades', 'celebrated', 'imac', \"relax_after_hard_day's_work\", 'oranges', 'new', 'sometimes_to_left_alone', 'loved_by_other_people', 'ease_suffering', 'really_good_food', 'self_sufficient', 'face_challenge', 'cheesecake_because_tastes_good', 'live_in_beautiful_house', 'brains', 'complemented', 'have_shelter', 'rely_on_computer', 'savings', 'avoid_very_loud_noises', 'breakfast_in_bed', 'partner', 'make_others_understand', 'answers', 'accepted_by_around', 'conflict_resolution', 'get_lawn_mower_started', 'take_shower', 'good_books_to_read', 'go_hom', 'retire_rich', 'sanity', 'good_to_other_people', 'appropiate_empathy', 'popcorn_at_movie', 'achieve_enlightenment', 'space', 'win_stanley_cup', 'computer_from_va_linux', 'enough_sleep', 'warm_greeting', 'wise', 'o_pay_heir_bills', 'express_opinion', 'blue_skies_and_warm_day', 'have_happy_family', 'eat_tasty_mushroom', 'keep_learning_new_things', 'global_freedom', 'see_things_for_herself', 'caregiver', 'sweet_fruit', 'sated', 'do_fun_activities', 'see', 'appropriate_answer', 'find_soulmate', 'have_tax_refund', 'more_than_person_needs', 'able_to_eat', 'appear_confident', 'attract_song_birds_to_yards', 'good_clean_fun', 'quiet', 'good_hair_day', 'company', 'fight_infections', 'butterflies_in_garden', 'feed_appetites', 'fresh_water', 'able_to_express_thoughts_freely', 'great_capabilities', 'eat_pussy', 'experience', 'find_differences', 'warm_clothing_in_cold_weather', 'ride_horse', 'keep_all_body_parts', 'lead_another_person_to_freedom', 'observe', 'talk_about_themselves', 'put_words_together', 'impress_mentors', 'have_right_words', 'freedom_of_expression', 'active_social_life', 'umbrella_when_rains', 'recover', 'eat_variety_of_foods', 'attractive_to_opposite_sex', 'contribute_to_society', 'more_for_less', 'privacy', 'loving_companion', 'taste_good_things', 'have_sexual_relations_with_another', 'find_simple_solution', 'with_cat', 'eat_well', 'have_wonderful_spouse', 'back_rub', 'personal_comfort', 'keep_date_interested', 'well_received', 'challenges_to_interesting', 'manage_well', 'upper_hand', 'more_leisure_time', 'safe_place_to_live', 'with_famous_people', 'good_rueben_now_and_then', 'do_own_thing', 'other_people_to_polite', 'house_to_live_in', 'comfortable_bed', 'dressing_on_salad', 'obtain_goal', 'real_thing', 'wonderful_memories', 'intelligent', 'literate', 'climb_mountain', 'caress_person_of_opposite_sex', 'hear_nice_news', 'fresh_eggs', 'boadband_internet_access', 'in_chareg', 'acute_hearing', 'alone_sometimes', 'take_vacations', 'easy_to_understand_instructions', 'king', 'clean_bathroom', 'healthy_attitude', 'have_pleasant_vacation', 'new_set_of_teeth', 'happieness', 'freedom_to_travel', 'oral_pleasure', 'desireable', 'witty', 'expert_advice', 'feel_hopeful', 'gifts', 'puppy_dog_kisses', 'have_fast_internet_acess', 'absolute_certainty', 'feel_fit', 'surrounded_by_beautiful_things', 'attractive_spouse', 'mail', 'avoid_constant_repetition', 'self_respect', 'opponent_to_lose', 'family_to_healthy', 'great_body', 'fresh_tomatoes', 'die_sometimes', 'dedication', 'succeed_in_life_and_feel_fulfilled', 'serene_environment', 'balanced_checkbook', 'enjoy_beauty', 'pleasant_atmosphere', 'around_other_persons', 'feel_adoration_of_many', 'take_care_of_children', 'trust_himself', 'improve_method', 'meet_extraterrestials', 'not_hear_parents_hastle', 'save_world', 'clean_air', 'technology_to_help', 'attain_goals', 'have_day_in_court', 'live_with_comforts', 'sweet_taste', 'have_happy_childhood', 'win_power_ball_lottery', 'headache', 'understand_computers', 'leave_legacy', 'accomplish_doog_things', 'tip_correctly', 'facts', \"good_night's_rest\", 'perfect_world', 'healthy_family', 'keep_family_safe', 'plenty_of_wood_for_fire', 'children_to_humanistically_understand_limitations', 'sleep_regularly', 'source_of_income', 'genuine_smile', 'dsl_or_cable_modem', 'live_comfortably', 'intelligent_conversations', 'see_children_grow_independent', 'lemonade', 'suck_on_ice_cube', 'see_taj_mahal', 'achieve', 'good_listener', 'favorite_team_to_win', 'loving_relationship', 'hear_about_good_qualities', 'freedom_from_injustice', 'catch_parade', 'have_fun_weekend', 'full_body_massage', 'save_earth', 'rescue_drowning_person', 'reproduce', 'gifts_at_christmas', 'recognition_of_own_importance', 'credit_to_family', 'give_heart_completely', 'improve_performance', 'feel_accomplished', 'imaginitive', 'personal_freedom', 'meal_on_time', 'gratification', 'talent', 'live_successful_life', 'lot_of_fun', 'central_heat', 'material_wealth', 'red_socks_for_christmas', 'not_bored', 'sweet', 'very_diverse_choice_of_foods', 'best_things_for_friends', 'fulfill_desires', 'honest_politicians', 'laugh_sometimes', 'understand_laws_of_physics', 'world_peace', 'repeatitive', 'ones_family_to_safe', 'receive_letter_from_friend', 'look_clever', 'feel_needed_and_helpful', 'achieve_potential', 'brethe', 'breakdance', 'see_clearly', 'links', 'work_for_pay', 'what_person_needs', 'parental_approval', 'have_treasure', 'escape_monotony', 'become_strong', 'lots_of_stuff', 'friends_to_judge_fairly', 'good_family_person', 'wellness', 'meed_deadline', 'self_knowledge', 'recover_from_illness', 'live_to_ripe_old_age', 'pregnant', 'actions_to_efficacious', 'technology', 'intellectual_stimulation', 'understand_as_much_science_as_possible', 'new_bicycle', 'good_diagnosis', 'see_new_things', 'love_money_and_respect', 'care_about', 'see_beauty_in_world', 'fast_internet_service', 'vindicate_actions', 'true_love', 'understand_art', 'laugh', 'wet_clothes_to_dry', 'others_to_think_well_of', 'bed_to_sleep_in', 'influence', 'noodles', 'paramour_to_match', 'feel_valuable', 'avoid_traps_of_life', 'white_christmas', 'free_of_illness', 'parents', 'good_steak', 'more', 'clean_air_to_breathe', 'confident_child', 'hope', 'know_which_way_home', 'have_lasting_friendships', 'cured_rather_than_just_palliated', 'read_in_peace', 'honor', 'sane', 'professionally_cleaned_blowfish', 'good_camera', 'bruce_boxleitner_in_life', 'money_and_fame', 'fortune', 'precise_answer', 'grow_beard', 'exhalted', 'free_rein', 'easy_journey', 'minimize_waste', 'stroked', 'adoration', 'cute_children', 'die_painlessly', 'read_interesting_books', 'include_respect_for_all_world_populations', 'lots_of_windows', 'part_of', 'able_to_run', 'acknowledgement', 'create', 'live_unoppressed', 'substenance', 'challenge_herself', 'give_and_take', 'good_vacation', 'lose_control', 'meet_people_who_have_great_status', 'discover_new_bands', 'friends_to_call', 'good_life', 'easy_money', 'base_in_science_and_nature', 'better_than_everybody_else', 'map_to_show_place', 'meaningful_life', 'courage', 'receive_right_answer', 'move_body', 'breakfast', 'lot_of_cheese_on_pizza', 'take_easy', 'hamburgers', 'lover', 'other_peoples_respect', 'see_effiel_tower', \"pen_doesn't_leak\", 'valued', 'play_ball', 'liberty', 'too_much_usually', 'sexual_relationships', 'neat', 'big_breasted_women', 'flowers_and_jewelry', 'alot', 'nice_hair', 'dialogue', 'goal', 'better_job', 'swim', 'proud_of_accomplishments', 'fresh_food_to_eat', 'swim_in_lagoon', 'christmas_gifts', 'musical_stimulation', 'simplicity', 'sufficiently_hydrated', 'have_hot_water', 'eat_cereal', 'listening_to_music', 'professional_satisfaction', 'speed', 'other_people', 'repeat_good_experience', 'share_knowledge', 'catch_sale', 'freedom_to_pursue_happiness', 'cup_of_tea', 'medium_rare_steak', 'fuck', 'three_square_meals_day', 'testify_to_beliefs', 'pet_gold_fish', 'fine_wine_and_good_conversation', 'look_beautiful', 'unexpected_pleasures', 'intellegence', 'life_partner', 'feel_pleasure', 'watch_parade', 'physical_pain_to_ease', 'answer_different_desire', 'wake_up_well_rested', 'rockstar', 'have_good_eyesight', 'green_thumb', 'like_job', 'understood_by_friends', 'watch_wizard_of_oz', 'free_from_oppression', 'spouse_to_faithful', 'understand_rights_and_responsibilities', 'see_ocean', 'well_groomed', 'other_people_to_love', 'egg_sandwich', 'dessert_sometimes', 'hugs', 'flowers_in_yard', 'fair_society', 'clothes', 'long_eyelashes', 'considered_beautiful', 'son_to_go_to_bed', 'feel_good_about_herself', 'worthy_opponent', 'self_realization', 'life_with_purpose', 'dance', 'pictures', 'pursue_truth_and_learning', 'of_good_will', 'good_deal', 'express_herself', 'die_when_in_extreme_pain', 'coffee_break', 'develop_talents', 'good_music', 'shelter', 'reliable_car', 'feel_beauty_in_commonplace_actions', 'have_money_to_buy_chocolate', 'work_at_home', 'fair_treatment', 'enjoy', 'reach_new_level_of_being', 'nanotechnology', 'improve_looks', 'pants_fit', 'entertainment_such_as_discover_magazine', 'feel_smart', 'populart', 'have_read_great_many_books', 'get_handle_on_difficulties', 'coke_or_beer_with_pizza', 'clean_socks_and_underware', 'proper_decorum', 'results', 'barbie_doll', 'things_to_taste', 'see_beauty', 'much_money', 'enjoy_pets', 'new_shoes', 'specific_instructions', 'open_mind', 'have_sparetime', 'know_different_places', 'understanding_of_mind_body_connection', 'full', 'wonderful_life', 'feel_making_difference', 'mystery', 'great_things', 'good_breakfast', 'in_right_circles', 'hear_secrets', '20_20_vision', 'see_growth_in_thier_career', 'consistency', 'have_real_friends', 'talk_in_numbers_so_there_416741971987291', 'stimulation', 'enemy_to_fight_fair', 'fulfilling_work', 'nice_cup_of_tea', 'brush_teeth_regularly', 'say_alive', 'stay_slim', 'firm_thighes', 'candies', 'live_fantasy', 'enjoy_things', 'create_beautiful_things', 'tto_have_knowledge', 'writing_to_clear', 'hobbies', 'nourishing_food', 'have_good_life', 'have_peace_in_life', 'things_to_happen_fast', 'make_difference', 'enough_to_eat', 'hear_good_news', 'cynisism_in_stupid_questions_like', 'seen_by_oncoming_traffic', 'see_falling_star', 'drink_water', 'what_do_enjoy', 'life_of_party', 'pack_for_travel', 'baby_to_smile', 'grow_older_and_mature_hpoefully', 'keep_on_breathing', 'fairness', 'stop_answering_question', 'husband_or_wife', 'go_to_heaven', 'luxury', 'chair_on_occasion', 'seed_to_sprout', 'understand_own_drawbacks', 'material_items', 'functional_use_of_bodies', 'have_cat', 'sweet_little_puppy', 'control_over_or_destiny', 'soothing_music', 'play', 'knowledgeable', 'muscular_figure', 'avoid_pain', 'fulfill_dreams', 'clean_hair', 'peaceful_nights_sleep', 'freedom_of_information', 'time_to_relax', 'thirty_hour_work_week', 'compassion', 'hear_complements', 'understand_cause_of_action', 'drive_train', 'fame_and_fortune', 'inspired', 'mankind_to_explore_universe', 'move_towards_light', 'coffee_hot_usually', 'lazy_sometimes', 'sleep_soundly', 'clean_bill_of_health', 'running_water', 'part_of_winning_team', 'see_good_movie', 'open_minded', 'bridge_space_separates_us', 'people_to_listen_to', 'abolish_irs', 'love_and_company', 'see_art', 'reach_zen', 'mutual_respect', 'heard_clearly', 'butter_on_toast', 'earn_respect', 'somewhere_to_have_fun', 'control_other_people', 'fly_kite', 'validation', 'conclusive_data', 'cotton_candy', 'have_strong_muscles', 'excel', 'cry_sometimes', 'understand_world_around', 'perfect', 'get_off_merry_go_round', 'go_sledding', 'good_luck', 'faith', 'comfortable_clothes', 'less_hassles', 'talented', 'mate_to_notice', 'jizz_in_girls_mouths', 'strong_bones', 'good_drinks', 'dinner', 'share_with_special', 'working_computer', 'have_best_friend', 'find_true_love', 'sufficient_sleep', 'go_on_yearly_vacation', 'air_conditioning_in_summer', 'treated_as_individual', 'take_vacation', 'breathe_fresh_clean_air', 'licked', 'phase_out_hatred', 'else_to_do_dishes', 'harmonious_history', 'make_others_happy', 'exciting_music', 'advantages', 'wounds_to_treated', 'better_than_others', 'drive_best_car', 'go_have_free_time', 'pretzels', 'clean_air_to_breath', 'make_movies', 'smooth_sailing', 'jewelry', 'healthy_life', 'successful_career', 'recognition_for_job_well_done', 'good_havana_cigar', 'know_if_there_extraterrestrials', \"good_at_one's_job\", 'leader', 'helpful_and_knowledgeable_clerks', 'go_to_zoo', 'spouse_for_life', 'independent', 'music', 'play_old_time_music', 'personal_fulfillment', 'fulfilling_job', 'see_good_play', 'fruits_from_labors', 'part_of_team', 'get_bargain', 'escape_earths_bounds', 'helpful_hints', 'number_1', 'gracious_host', 'others_to_respect_privacy', 'financial_safety', 'accepted_by_others', 'have_more', 'look_like_movie_star', 'money_for_talents', 'individuality_respected', 'reward', 'investigate_discrepancies', 'able_to_express_themself_freely', 'stable_relationship', 'live_good_life', 'juicy_berries', 'diamonds', 'plenty_of_sleed', 'caring_nurses', 'more_time', 'large_dick', 'co_operation_from_other_people_and_machines', 'within_budget', 'hug', 'large_house', 'stability_in_life', 'praise', 'right', 'listen_to_symphony', 'breath_clean_air', 'leather_seats', 'listen_to_wind_chime', 'ambitions', 'unafraid', 'good_values', 'accomplish', \"society's_approbation\", 'nice_dinner', 'organized_workspace', 'actualization', 'bring_credit_on_family', 'laugh_out_loud', 'alone_with_thoughts', 'commitment_from_others', 'achieve_self_actualization', 'help_sometimes', 'help_other_people', 'cold_beer', 'excited', 'not_make_mistakes', 'good_conversationalist', 'successful_in_life', 'nice_view_from_window', 'ripe_fruit', 'financial_stability', 'fresh_bread', 'prosper', 'forgiven', 'smart', 'facial', 'little_tenderness', 'feel_important', 'comfortable_bed_at_night', 'hospitality', 'communication', 'dog', 'birthday_party', 'feel_involved', 'paid_for_work', 'have_clean_clothes', 'quiet_life', 'silk_underwear', 'select_right_tool', 'positive_relationships', 'healty', 'sympathy', 'run_sometimes', 'close_to_children', 'know_time', 'validation_from_others', 'nice_shoes', 'sit_in_sun', 'rest_quietly_when_ill', 'prove_other_people_wrong', 'kill_annoying_singers', 'new_computer', 'travel_and_enjoy_new_things', 'women_atleast_2_at_time', 'treated_with_deference', 'go_fast', 'eat_cake_and_not_gain_weight', 'sing_with_joy', 'recognized', 'flat_tummy', 'with_of_like_mind', 'visit_world', 'fast_car', 'experience_everlasting_orgasm', 'act_within_law', 'prestige', 'happy_family', 'honest_and_loyal', 'quality', 'way_to_express_feelings', 'foods_affriole_fresh_from_garden', 'fresh_breath', 'opponent_to_losee', 'find_or_other_half', 'play_with', 'computers', 'busy', 'traction_on_mountain_roads', 'understandment_about_life', 'develop', 'strong_fingernails', 'live_with_minimal_fear', 'satisfying_work', 'order', 'good_looking', 'thought_of_as_attractive', 'respected_by_workmates', 'live_life_to_fullest', 'clean_fingernails', \"belive_what_he's_been_told\", 'spicy_foods', 'succeed_in_what_attempt', 'construe_each_sentence_correctly', 'comfortable_shoes', 'kill_enemy', 'good_sex_life', 'depend_on_word_of_others', 'good_tasting_food', 'watch_sports', 'make_connections_between_images', 'go_to_france', 'exercise_mind', 'open_data_standards', 'free_checking', 'analyze_things_in_context', 'support_from_spouse', 'enjoy_friendship_of_others', 'live_in_clean_home', 'pay_off_debt', 'find_soul_mate', 'critical_thinking', 'identity', 'full_belly', 'remembered_after_death', 'understand_sequence_of_past_events', 'understanding_mom', 'trust_people', 'plenty_of_rest', 'world_peace_amongst_all_nations', 'positive_thoughts', 'juicy_pears', 'impress_smart_people', 'plenty_of_money', 'fragrant_smells', 'know_everything', 'have_enough_to_eat', 'affluence', 'low_electric_bill', 'larger_house', 'mom', 'choclate', 'creativity', 'kind_to_everyone', 'loyal_friends', 'together_with_other_people', 'soulmate', 'listen_to_loud_music', 'clean_kitchen', 'several_ways_to_escape_wage_slavery', 'hamburger', 'own_things', 'known', 'able_to_wiggle_fingers', 'given_new_challenges', 'good_brakes', 'with_winners', 'fresh_fruit', 'safety_hapiness', 'graduate_from_school', 'regular_access_to_food', 'children_to_happy', 'nicotine', 'virtuous', 'shoes_to_stay_tied', 'burger_king', 'make_progress', 'know_blood_type', 'art', 'pornography', 'good_conversation', 'get_right_price', 'great_weather_over_long_weekend', 'preferences', 'experiment', 'unpolluted_water', 'finish_first', 'love_and_understanding', 'people_to_confide_in', 'experience_rich_diversity_of_life', 'fly_safely', 'trim_waist', 'held_in_high_esteem', 'have_firm_body', 'parents_to_agree', 'get_laid_as_often_as_possible', 'recognition_of_efforts', 'basic_necessities', 'know_what_happens_when_dies', 'nice_place_to_live', 'fresh_coffee_in_morning', 'everyone_to_happy', 'learn_to_swim', 'know_new_things', 'have_healthy_life', 'sweet_things_to_ear', 'guiltless', 'feel_beautiful_and_special', 'hugged', 'equal_opportunity', 'with_other_people', 'positive_reinforcement', 'honest_answer', 'salad_with_dinner', 'attention_but_not_all_time', 'with_people_like_themselves', 'continuously_learn', 'good_insulation', 'invention_and_innovation_put_into_practice', 'healthy_bones', 'surrounded_by_beauty', 'talk_with_friends', 'gracious_guest', 'low_crime', 'better_life', 'participate_on_orgy', 'fair_trial', 'longevity', 'great_sex', 'nice_weather', 'reliable_friends', 'yummy_drink', 'enjoyable_hobby', 'understand_events_of_history', 'warm_coat', 'have_home_to_live_in', 'parking_place', 'see_children_healthy', 'good_parents', 'fit_clothes', 'young_forever', 'clear_rules', 'understand_or_life', 'get_enough_sleep', 'worship_freely', 'clean_house', 'free_stuff', 'decide_what_wants', 'less', 'comfortable_mattress', 'live_free_of_worries', 'read_well_written_book', 'have_fortune', 'noticed', 'have_orgasm_during_sexual_intercourse', 'warm_bed', 'delicious_food_and_good_company', 'fruitful_and_multiply', 'not_told_what_to_do', 'working_toilet', 'receive_phd', 'sleep_well_at_night', 'life', 'have_good_memories', 'smooth_skin', 'tiptoe_through_tulips', 'delicious_food', 'own_unique_things', 'reach_destination', 'very_high_speed_internet_connection', 'healthy_environment', 'sense_of_purpose', 'have_meaningful_life', 'life_to_pleasure', 'challenged_but_not_too_much', 'have_nice_place_to_live', 'link_ideas_together', 'nice_boss', 'rights', 'complements', 'boss_not_being_asshole', 'good_parking_place', 'become_famous', 'japanese_wife', 'cheese_and_crackers', 'stay_comfortably_warm', 'energy', 'loving_family_members', 'war', 'health_coverage', 'spirituality', 'others_to_cotton_to', 'teach_computers_to_like_humans', 'good_muscle_tone', 'considered_trustworthy', 'dreams_to_come_true', 'flex_time', 'get_laid', 'good_retirement_plan', 'power_for_own_sake', 'eternal_salvation', 'clothing', 'ipod', 'nice_jewelry', 'forgive_loved_ones', 'cup_of_coffee', 'perfect_body', 'expensive_possesions', 'connect_ideas_together', 'no_crime', 'share_existence', 'friends_who_understand', 'not_to_feel_guilty_when_innocent', 'hear_own_name', 'defeat_enemies', 'contribution_to_society', 'passion', 'not_to_worry_about_money', 'date_with_supermodel', 'time_for_leisure_activities', 'coffee_in_morning', 'spend_time_doing_pleasant_things', 'congratulations', 'have_interesting_job', 'answers_to_questions', 'go_to_wilderness', 'victory', 'integrity', 'status_symbols', 'appreciate_beauty', 'positive_things', 'informed_intelligence', 'stay_hydrated', 'lots_of_fun', 'donate_to_good_cause', 'hot_food_to_hot', 'learn_new_facts', 'win_game', 'food_water_and_shelter', 'enjoy_life_to_fullest', 'educated', 'stay_healthy_and_happy', 'have_soft_skin', 'sexy', 'light', 'well_cooked_pork', 'dsl', 'safe_food_to_eat', 'perform_well', 'own_house', 'in_stable_condition', 'fitness', 'all_knowing', 'able_to_read', 'all_luxuries_of_life', 'else_to_clean_thier_ovens', 'g4', 'well_paying_job', 'fast_cars', 'sing_in_choir', 'have_fond_memories', 'new_ideas', 'know_all', 'expressive', 'details', 'all_good_things_in_life', 'have_everything', 'freedom_just_to', 'computer', 'freedom_to_disagree', 'respected_by_other_people', 'feel_awe', 'strength_in_mind_body_and_spirit', 'have_fulfilling_life', 'worthy_competitor', 'nice_glass_of_wine', 'shorter_working_hours', 'million_dollars', 'touch_vagina', 'beer', 'more_civil_liberties', 'read_nehemiah', 'clean_water_to_drink', 'material_goods', 'meaningful_work', 'all_kinds_of_things', 'mystery_with_intrigue', 'receive_good_news', 'power_over_others', 'weekend_to_arrive', 'scented_candles', 'divorce_violent_spouse', 'judged_on_own_merits', 'communicate_values', 'others_to_like', 'have_all_limbs', 'examples', 'worshiped', 'drink_clean_water', 'nice_smelling_soap', 'protect_family', 'find_easy_ways_to_do_things', 'remembered', 'traction_on_curves', 'feel_sacure', 'suffer_sometimes', 'sweet_things', 'live_healthy_life', 'receive_gifts', 'good_government', 'direct_own_life', 'know_things', 'consummate_relationship', 'useful_advice', 'room_to_spread_out', 'eat_yummy_food', 'free_exchange_of_information', 'view_landscapes', 'good_health_products', 'love_other_people', 'justice_and_order', 'able_to_see', 'interesting_challenges', 'future', 'people_to_understand_what_says', 'trim_figure', 'logical_answers_to_questions', 'straight_teeth', 'eating_every_day', 'sufficient_rain', 'enough_time', 'truckload_of_money', 'clean_clothes', 'cherished_notion', 'see_bruce_boxleitner_in_person', 'have_multiple_orgasms', 'have_minimum_necessities_of_life', 'to', 'sexy_clothing', 'feel_safe', 'feel_joy', 'not_killed', 'free_money', 'comfortable_socks', 'avoid_premature_burial', 'know_who_parents', 'peace_in_life', 'feel_happy', 'feel_beautiful', 'sincere', 'fly', 'feel_special', 'eat_peanut_butter', 'productive_member_of_society', 'well_paid_job', 'happy_and_contented', 'grow_wings_and_fly', 'cherished', 'power_over_own_life', 'smart_clothes', 'follow_heart', 'fast_service', 'champion', 'massage', 'pretty_clothes', 'unrequited_love', 'work_sometimes', 'left_alone_sometimes', 'world_conquest', 'solve_problem', 'not_to_shoot_another_person', 'laughter', 'achieve_financial_independence', 'good_citizen', 'high_wages', 'protect_ears_from_cold', 'single_malt_scotch', 'eat_food_every_day', 'hero', 'eat_fat_and_sugar', 'have_successful_career', 'find_parking_space', 'life_to_easy', 'air', 'racist', 'loved_ones_close_by', 'different_things_all_time', 'sky_dive', 'convertible', 'live_in_good_neighborhood', 'beef', 'german_car', 'hear_live_music', 'fulfilled', 'easy_life', 'season_food_to_taste', 'answer_phone_when_rings', 'low_airfares', 'have_talents_recognized', 'coca_cola', 'run_business', 'scratch_head_when_itches', 'clean_dirty_glass', 'able_to_tell_good_stories', 'have_sight', 'popularity', 'go_on_holiday', 'coffee', 'have_long_live', 'see_hummingbirds', 'sunshine', 'get_away_from_work', 'have_good_day', 'good_relationship', 'nice_apartment', 'cuddle_other_people', 'lots_of_children', 'have_free_time', 'strength', 'slim', 'succed_goal', 'loving_partner', 'inspiration', 'beautiful_hair', 'acquire_wealth', 'bladder_control', 'trustworthy_friends', 'improve_lot_in_life', 'speed_up_process', 'no_outstanding_debt', 'good_sex', 'care_for_wounded_animal', 'condiments_available_with_food', 'depend_on_promises_of_others', 'in_under_wire', 'perfect_skin', 'not_to_feel_tired', 'feel_alright', 'bliss', 'interaction', 'able_to_swim', 'newest_model_of_item', 'patience', 'big_front_porch', 'fly_kites_in_springtime', 'sexual_release', 'nice_family', 'good_sense_of_humor', 'world', 'clean_shirt', 'superhuman', 'make_more_money', 'open_software', 'recover_from_nervous_breakdown', 'gestures_of_love', 'have_enough_food', 'surprises', 'particular_answer', 'better_themself', 'feel_good_about_himself', 'syrup_on_waffles', 'meet_soulmate', 'good_fitting_clothes', 'good_selection_of_reading_material', 'passing_grade', 'hasppiness', 'pretty_fingernails', 'pot', 'beer_and_sluts', 'num_nums', 'suger', 'strive', 'novelty', 'free_time', 'more_information', 'live_for_eternity', 'avoid_brutes', 'air_to_comfortably_dry', 'have_healthy_sex_life', 'question_opinions_of_other_people', 'stable_relationships_with_interesting_people', 'have_nice_job', 'electricity', 'bath_in_admiration_of_others', 'treated_like_special', 'autonomy', 'have_many_good_friends', 'reliable_flatmates', 'leisure_time', 'do_what_preferred', 'other_people_to_respect', 'loved_and_cared_for', 'peaceful_sleep', 'free_food', 'breathe', 'devotion', 'hip', 'tenderness', 'clean_place_to_step', 'see_things', 'play_sometimes', 'build_trusting_relationship', 'be', 'happines', 'get_by_unnoticed_sometimes', 'secure_home', 'have_everyone_happy_with', 'achievements_to_recognized', 'toast_marshmallows_over_campfire', 'trick_others', 'walk_outside_during_quiet_snowstorm', 'clean_bedding', 'understand_different_modalities_of_medicine', 'get_away_with_everything', 'sun_to_keep_rising', 'weekends_to_last_forever', 'loved_by_parents', 'finish_on_time', 'meaningful_employment', 'care', 'feel_in_control', 'comfort_food', 'lot_of_land', 'warm_bath', 'plain_truth', 'win_medal', 'soothe_crying_baby', 'good_cup_of_coffee', 'nice_surroundings', 'well_adjusted_children', 'respected_for_work', 'have_whatever_want', 'love_and_affection_from_spouse', 'fresh_strawberries', 'solve_problems', 'shorter_work_day', 'happy_and_productive', 'white_teeth', 'witness_kindness', 'fix_what_broken', 'experience_life', 'friends_and_acceptance', 'gold', 'drive_fast_car', 'pleasant_dreams', 'have_future', 'peace_not_war', 'see_loved_ones_again', 'materialism', 'good_entertainment', 'stay_healthy', 'always_more', 'go_first_class', 'health_and_happiness', 'bio_lumensent_acne', 'find_good_parking_space', 'hold_remote_control', 'love_opposite_sex_person', 'have_inner_peace', 'use_talents', 'save_environment', 'beautiful_day', 'pass_on_ideas_to_future_generations', 'find_new_uses_for_old_tools', 'security_and_empathy', 'healthy_children', 'hear_good_song', 'children_to_well', 'paid_well', 'sponsor', 'have_to_hold', 'intelligent_children', 'many_choices', 'admiration', 'feel_breeze_on_face', 'weekend_at_beach', 'keep_things_private', 'obtain_gol', 'see_astronauts', 'hopes_for_future', 'harmony_between_all', 'stay_alive', 'other_activities', 'backup_of_hard_drive', 'star_of_show', 'not_feel_pain', 'not_to_die', 'asked_opinion', 'receive_credit_for_work', 'hug_thier_kids', 'clean_apartment', 'equal_pay_for_equal_work', 'live_love_and_happy', 'no_parking_tickets', 'read_good_book', 'become_wise', 'more_rice', 'study_artifacts', 'use_muscles', 'value', 'wisedom', 'money_happines', 'patient', 'sleep_peacefully', 'thought_of_with_kindness', 'fell_good', 'listen_to_popular_music', 'polite_waiter', 'mailman_to_bring', 'nightlight', 'good_shelter', 'happy_more_than', 'clean_cloths', 'clear_in_thought', 'affectionate_relationships', 'live_well', 'credible', 'have_fun_on_weekends', 'look_over_edge', 'able_to_do_many_things', 'see_good_art', 'have_great_sex', 'haircut', 'affect_attitude', 'snow_in_winter', 'arrange_ideas', 'create_artificial_intelligence', 'contribute_meaningful_to_life', 'wishes_to_come_true', 'g4_laptop', 'open_season_on_telemarketers', 'defend_actions', 'beat_peace', 'have_remission', 'good_neighbors', 'compliments', 'candy', 'different', 'suitable_answer', 'live_evil_life', 'turtle', 'look_pleasing_to_others', 'attainable_standards', 'things_to_improve', 'spell_checker', 'prevent_static_cling', 'total_honesty', 'learn_new_concepts', 'find', 'necessary_answer', 'learn_about_world_around', 'lick_stamps', 'good_self_image', 'beat_competition', 'know_how_to_read', 'acheivement', 'home_grown_tomatoes', 'get_on_with_others', 'eat_every_day', 'comfortable_surroundings', 'positive_environment', 'good_posture', 'liked_by_friends', 'fullfilled', 'free_ride', 'full_lips', 'acceptance_from_or_peers', 'pass_exams', 'act_prudently', 'more_than_or_has', 'specific_answer', 'safe', 'have_vacation_time', 'hear_warnings', 'belong_to_group_of_people', 'know_why_things_happen', 'know_how_to_spell_correctly', 'sheer_truth', 'loving_family', 'learn_to_dance', 'angry_sometimes', 'money_and_power', 'enough', 'special', 'successful_in_job', 'keep_in_touch_with_kinfolks', 'good_boss', 'feel_sunshine_on_skin', 'not_destroy_only_habitat_terra', 'way_to_move', 'long_vacation', 'control_over_own_lives', 'sincere_appreciation', 'time_alone', 'promoted', 'traction_on_acceleration', 'good_review', 'own_own_business', 'extra_toes_and_fingers', 'use_imagination_to_invent_things', 'read_book', 'have_there_all_time', 'drive_responsibility', 'know_why_exists', 'warm_home', 'feel_connected_to_others', 'please_god', 'time_to_rest', 'new_toys', 'score_well_on_tests', 'good_weather_when_go_camping', 'good_friends', 'have_two_good_legs', 'evidence_to_clear', 'conquer_unreasonable_fear', 'learn_about_world', 'job_security', 'kind_neighbors', 'beautiful_workmanship', 'reliable_information', 'world_to_peaceful', 'have_safe_home', 'live_life', 'smartest_in_class', 'get_good_education', 'investments_to_appreciate', 'understand_what_being_told', 'playfulness', 'ripe_strawberries', 'with', 'have_pda', 'passionate_kisses', 'find_good_bargain', 'support', 'equality', 'everything_to_work', 'another_prompt', 'clean_water', 'meth', 'snow_to_fall_on_christmas', 'ice_cream', 'contribute_to_world_peace', 'more_peanuts', 'know_where_at', 'donuts', 'do_little_dance_sometimes', 'avoid_unpleasant_music', 'control_pique', 'commitment', 'informed', 'geet_enough_sleep', 'affluent', 'make_world_better_place', 'good_computer', 'person_to_love', 'others_to_happy', 'treated_with_respect', 'finished_lab_report', 'more_time_off', 'gift', 'wine_women_and_song', 'piece_of_cake', 'examined_by_qualified_physician', 'behave_lawfully', 'have_day_off', 'jewelery', 'opportunity', 'spend_time_with_family', 'fresh_produce', 'second_chances', 'act_with_skill', 'leisure', 'porsche_959', 'have_good_skin', 'pass_on_genes', 'carry_lucky_charm', 'sexual_intercourse', 'eat_fresh_fruit', 'congratulated', 'unconditional_love', 'sound_sleep', 'fight_good_fight', 'feel_warm', 'french_fries_to_hot', 'medium_rare_steaks', 'lots_of_space', 'ridicule', '10_fingers', 'personal_care', 'people_to_like', 'hug_ones_loves', 'succeed_in_talents', 'dick_sucked', 'medical_care', 'tan', 'people_to_not_ignore', 'mentally_challenged', 'make_decent_living', 'complete_data_set', 'justice', 'needed', 'treated_with_consideration', 'get_high', 'maintain_self_respect', 'nice_friends', 'understand_place_in_universe', 'meaning', 'know_god', 'find_pattern', 'live_in_luxury', 'logic_and_reason', 'retire_young', 'water_to_drink', 'pat_on_back', 'understood_by_other_people', 'all_simple_things_to_survive'}\n",
      "{'people', 'person'}\n"
     ]
    }
   ],
   "source": [
    "print(cn.df_diccionario_generales[\"person\"][\"desires\"])\n",
    "print(cn.df_diccionario_especificas[\"dance\"][\"desires\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set()\n",
      "set()\n"
     ]
    }
   ],
   "source": [
    "print(cn.df_diccionario_generales[\"win\"][\"defined_as\"])\n",
    "print(cn.df_diccionario_especificas[\"win\"][\"defined_as\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'affordable'}\n",
      "{'bookbindery', 'hymnbook', 'bookstave', 'hyperbook', 'bookland', 'quizbook', 'daybook', 'bookchest', 'prebook', 'hornbook', 'lookbook', 'bookful', 'sportsbook', 'midbook', 'booklover', 'promptbook', 'bookstaff', 'ebook', 'book_of_travel', 'bookholder', 'classbook', 'tablebook', 'booklike', 'cookbook', 'megabook', 'by_book', 'bookrack', 'needlebook', 'mook', 'book_pahlavi', 'gamebook', 'paybook', 'e_book', 'audiobook', 'scientibook', 'bookish', 'booklet', 'bookhoard', 'scorebook', 'wishbook', 'booklist', 'artbook', 'bookstore', 'cashbook', 'bookwright', 'holobook', 'videobook', 'bookshelf', 'bookwork', 'tunebook', 'book_ridden', 'textbook', 'bookaholic', 'bookshop', 'songbook', 'wastebook', 'webbook', 'bookdealer', 'datebook', 'bookmatch', 'forebook', 'bookery', 'timebook', 'formbook', 'book_jacket', 'bookhood', 'deskbook', 'bookmate', 'bookjacket', 'sketchbook', 'fairybook', 'playbook', 'bookless', 'bookazine', 'booklegger', 'booker', 'bookworm', 'bookend', 'fakebook', 'callbook', 'workbook', 'book_knowledge', 'codebook', 'bookstack', 'booklight', 'holy_book', 'talebook', 'rebook', 'stud_book', 'ultrabook', 'bookbinder', 'passbook', 'booklouse', 'photobook', 'storybook', 'schoolbook', 'underbook', 'factbook', 'pitchbook', 'notebook', 'bookman', 'bookstall', 'bookshelving', 'handbook', 'bookwise', 'bookhouse', 'clipbook', 'pillowbook', 'rulebook', 'herdbook', 'bookly', 'racebook', 'bookselling', 'logbook', 'bookhound', 'overbook', 'book_learned', 'bookmaking', 'bookbag', 'bookware', 'bookcase', 'blook', 'bookseller', 'chartbook', 'bookable', 'roadbook', 'bluebook', 'booky', 'multibook', 'slambook', 'booketeria', 'wordbook', 'pocketbook', 'quotebook', 'bookmobile', 'bookbus', 'splatbook', 'booklegging', 'sourcebook', 'bookplate', 'nonbook', 'viewbook', 'bookrest', 'bookkeeper', 'bookstop', 'copybook', 'scrapbook', 'bookmonger', 'psalmbook', 'facebook', 'cyberbook', 'bookmarker', 'antibook', 'bookflap', 'salesbook', 'book_bag', 'bookstand', 'booktuber', 'stylebook', 'unbook', 'bookbinding', 'casebook', 'bookcross', 'bookbuild', 'coursebook', 'bookling', 'bookroom', 'newsbook', 'jestbook', 'bookrunning', 'spellbook', 'bookcraft', 'shopbook', 'databook', 'lawbook', 'phonebook', 'bookrunner', 'phrasebook', 'vook', 'yearbook', 'mag_book', 'fake_book', 'booklined', 'chequebook', 'flybook'}\n"
     ]
    }
   ],
   "source": [
    "print(cn.df_diccionario_generales[\"unaffordable\"][\"derived_from\"])\n",
    "print(cn.df_diccionario_especificas[\"book\"][\"derived_from\"]) #hiponimos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'table', 'desk'}\n",
      "{'chair'}\n"
     ]
    }
   ],
   "source": [
    "print(cn.df_diccionario_generales[\"chair\"][\"located_near\"])\n",
    "print(cn.df_diccionario_especificas[\"table\"][\"located_near\"]) #hiponimos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'listen'}\n",
      "{'listen'}\n"
     ]
    }
   ],
   "source": [
    "print(cn.df_diccionario_generales[\"talk\"][\"similar_to\"])\n",
    "print(cn.df_diccionario_especificas[\"talk\"][\"similar_to\"]) #hiponimos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set()\n",
      "set()\n"
     ]
    }
   ],
   "source": [
    "print(cn.df_diccionario_generales[\"man\"][\"receives_action\"])\n",
    "print(cn.df_diccionario_especificas[\"man\"][\"receives_action\"]) #hiponimos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set()\n",
      "{'riksdag', 'ericsson'}\n"
     ]
    }
   ],
   "source": [
    "print(cn.df_diccionario_generales[\"stockholm\"][\"at_location\"])\n",
    "print(cn.df_diccionario_especificas[\"stockholm\"][\"at_location\"]) #hiponimos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set()\n",
      "set()\n"
     ]
    }
   ],
   "source": [
    "print(cn.df_diccionario_generales[\"stockholm\"][\"located_near\"])\n",
    "print(cn.df_diccionario_especificas[\"stockholm\"][\"located_near\"]) #hiponimos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'in_sweden'}\n",
      "set()\n"
     ]
    }
   ],
   "source": [
    "print(cn.df_diccionario_generales[\"stockholm\"][\"is_a\"])\n",
    "print(cn.df_diccionario_especificas[\"stockholm\"][\"is_a\"]) #hiponimos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'sweden', 'södermanland', 'uppland'}\n",
      "set()\n"
     ]
    }
   ],
   "source": [
    "print(cn.df_diccionario_generales[\"stockholm\"][\"part_of\"])\n",
    "print(cn.df_diccionario_especificas[\"stockholm\"][\"part_of\"]) #hiponimos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'in_europe', 'country', 'part_of_europe'}\n",
      "set()\n"
     ]
    }
   ],
   "source": [
    "print(cn.df_diccionario_generales[\"sweden\"][\"is_a\"])\n",
    "print(cn.df_diccionario_especificas[\"sweden\"][\"is_a\"]) #hiponimos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'beautiful_continent', 'continent_on_earth', 'collection', 'station', 'continent_in_northern_hemisphere', 'magazine', 'continent', 'eurasia'}\n",
      "{'northern_europe', 'british_isles', 'eastern_europe', 'caucasus', 'pyrenees', 'central_europe', 'alps', 'balkans', 'benelux', 'carpathians', 'apennine_peninsula', 'western_europe', 'baltic', 'iberia', 'southern_europe', 'scandinavia'}\n"
     ]
    }
   ],
   "source": [
    "print(cn.df_diccionario_generales[\"europe\"][\"is_a\"])\n",
    "print(cn.df_diccionario_especificas[\"europe\"][\"is_a\"]) #hiponimos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'calorie'}\n",
      "set()\n"
     ]
    }
   ],
   "source": [
    "print(cn.df_diccionario_generales[\"calorie\"][\"part_of\"])\n",
    "print(cn.df_diccionario_especificas[\"calorie\"][\"part_of\"]) #hiponimos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'related_to': {'acquaintance',\n",
       "  'amuse',\n",
       "  'birth_defect',\n",
       "  'competitively',\n",
       "  'dalliance',\n",
       "  'disabled_sports',\n",
       "  'display',\n",
       "  'divert',\n",
       "  'feature',\n",
       "  'friend',\n",
       "  'game',\n",
       "  'good_sport',\n",
       "  'make_merry',\n",
       "  'match_fixing',\n",
       "  'mutant',\n",
       "  'plant',\n",
       "  'play',\n",
       "  'species',\n",
       "  'sportsman',\n",
       "  'sportsmanship'},\n",
       " 'form_of': set(),\n",
       " 'is_a': {'athlete',\n",
       "  'devised_structured_activity',\n",
       "  'diversion',\n",
       "  'exercise',\n",
       "  'game',\n",
       "  'newspaper',\n",
       "  'occupation',\n",
       "  'person',\n",
       "  'physical_activity',\n",
       "  'physical_activity_with_goal',\n",
       "  'recreational_activity',\n",
       "  'strenuous_form_of_exercise',\n",
       "  'vacationer'},\n",
       " 'part_of': set(),\n",
       " 'has_a': set(),\n",
       " 'used_for': {'playing', 'watching'},\n",
       " 'capable_of': set(),\n",
       " 'at_location': {'field', 'sporting_event'},\n",
       " 'entails': set(),\n",
       " 'causes': {'injury'},\n",
       " 'has_subevent': set(),\n",
       " 'has_first_subevent': set(),\n",
       " 'has_last_subevent': set(),\n",
       " 'has_prerequisite': set(),\n",
       " 'has_property': {'playing_by_rules', 'violent'},\n",
       " 'motivated_by_goal': set(),\n",
       " 'desires': set(),\n",
       " 'synonym': {'frolic', 'fun', 'mutant', 'sport'},\n",
       " 'antonym': set(),\n",
       " 'distinct_from': set(),\n",
       " 'derived_from': set(),\n",
       " 'defined_as': set(),\n",
       " 'manner_of': {'have'},\n",
       " 'located_near': set(),\n",
       " 'has_context': {'biology', 'botany', 'slang', 'zoology'},\n",
       " 'similar_to': {'sports'},\n",
       " 'etymologically_related_to': {'disport'},\n",
       " 'causes_desire': set(),\n",
       " 'made_of': set(),\n",
       " 'receives_action': {'played_by_humans', 'played_by_opposing_teams'},\n",
       " 'created_by': set()}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cn.df_diccionario_generales[\"sport\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "set()\n",
      "set()\n"
     ]
    }
   ],
   "source": [
    "print(cn.df_diccionario_generales[\"house\"][\"located_near\"])\n",
    "print(cn.df_diccionario_especificas[\"house\"][\"made_of\"]) #hiponimos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'still', 'stand_still', 'walk_slowly', 'crawl', 'walking', 'stop', 'stand', 'rise', 'walk', 'slowly'}\n",
      "{'step', 'walk'}\n"
     ]
    }
   ],
   "source": [
    "print(cn.df_diccionario_generales[\"run\"][\"antonym\"])\n",
    "print(cn.df_diccionario_especificas[\"run\"][\"antonym\"]) #hiponimos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'getting', 'up', 'getting_up', 'stand', 'standing'}\n",
      "{'chilly', 'cold', 'cool'}\n"
     ]
    }
   ],
   "source": [
    "print(cn.df_diccionario_generales[\"sit\"][\"antonym\"])\n",
    "print(cn.df_diccionario_especificas[\"warm\"][\"distinct_from\"]) #hiponimos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# (Validación) Nuevo proceso para obtener relaciones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "punct=string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "ut.load_vectors_in_lang(nlp,\"data/numberbatch-en-17.04b.txt\") # carga de vectores en nlp.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['related_to',\n",
       " 'form_of',\n",
       " 'is_a',\n",
       " 'part_of',\n",
       " 'has_a',\n",
       " 'used_for',\n",
       " 'capable_of',\n",
       " 'at_location',\n",
       " 'entails',\n",
       " 'causes',\n",
       " 'has_subevent',\n",
       " 'has_first_subevent',\n",
       " 'has_last_subevent',\n",
       " 'has_prerequisite',\n",
       " 'has_property',\n",
       " 'motivated_by_goal',\n",
       " 'desires',\n",
       " 'synonym',\n",
       " 'antonym',\n",
       " 'distinct_from',\n",
       " 'derived_from',\n",
       " 'defined_as',\n",
       " 'manner_of',\n",
       " 'located_near',\n",
       " 'has_context',\n",
       " 'similar_to',\n",
       " 'etymologically_related_to',\n",
       " 'causes_desire',\n",
       " 'made_of',\n",
       " 'receives_action',\n",
       " 'created_by']"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(cn.df_diccionario_generales[\"player\"].keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MÉTODOS PARA COMPROBAR TIPOS DE RELACIONES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "relaciones_g1=[\"form_of\",\"is_a\",\"used_for\",\"entails\",\"causes\",\"synonym\",\"manner_of\"]\n",
    "relaciones_g2=[\"antonym\",\"distinct_from\"]\n",
    "relaciones_g3=[\"related_to\",\"similar_to\"]\n",
    "relaciones_adicionales=[\"part_of\",\"at_location\",\"has_a\",\"has_last_subevent\",\"has_property\",\n",
    "                            \"defined_as\",\"located_near\",\"receives_action\",\"made_of\",\"has_subevent\",\n",
    "                            \"has_first_subevent\",\"has_prerequisite\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_entail_syn(wt,wh):\n",
    "    # guardo conjuntos de sinonimos para un uso posterior\n",
    "    synt_i=cn.df_diccionario_generales[wt][\"synonym\"]\n",
    "    synt_i=synt_i.union(cn.df_diccionario_especificas[wt][\"synonym\"])\n",
    "    synt_i=synt_i.union(cn.df_diccionario_generales[wt][\"form_of\"])\n",
    "    synh_i=cn.df_diccionario_generales[wh][\"synonym\"]\n",
    "    synh_i=synh_i.union(cn.df_diccionario_especificas[wh][\"synonym\"])\n",
    "    synh_i=synh_i.union(cn.df_diccionario_generales[wh][\"form_of\"])\n",
    "    #print(\"---\")\n",
    "    #print(synt_i)\n",
    "    #print(synh_i)\n",
    "    if wt ==wh:                #COMPARACION DIRECTA DE wt y wh\n",
    "        return True,\"same\"\n",
    "    elif wh in synt_i:\n",
    "        return True,\"synonym\"\n",
    "    elif wt in synh_i:\n",
    "        return True,\"synonym\"\n",
    "    elif len(synt_i.intersection(synh_i))>0:\n",
    "        return True,\"synonym\"\n",
    "    else:\n",
    "        return False,\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_entail_gen(wt,wh): # t->h\n",
    "    relaciones_g1=[\"form_of\",\"is_a\",\"used_for\",\"entails\",\"causes\",\"synonym\",\"manner_of\"]\n",
    "    for r_g in relaciones_g1:\n",
    "        if wh in cn.df_diccionario_generales[wt][r_g]:\n",
    "            return True,r_g\n",
    "            #print(wt,c_c,r_g,\"generales\")\n",
    "    return False,\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_contradiction_ant(wt,wh):\n",
    "    relaciones_g2=[\"antonym\",\"distinct_from\"]\n",
    "    # guardo conjuntos de sinonimos para un uso posterior\n",
    "    synt_i=cn.df_diccionario_generales[wt][\"synonym\"]\n",
    "    synt_i=synt_i.union(cn.df_diccionario_especificas[wt][\"synonym\"])\n",
    "    synt_i=synt_i.union(cn.df_diccionario_generales[wt][\"form_of\"])\n",
    "    synh_i=cn.df_diccionario_generales[wh][\"synonym\"]\n",
    "    synh_i=synh_i.union(cn.df_diccionario_especificas[wh][\"synonym\"])\n",
    "    synh_i=synh_i.union(cn.df_diccionario_generales[wh][\"form_of\"])\n",
    "    for r_g in relaciones_g2:\n",
    "        if wh in cn.df_diccionario_generales[wt][r_g] or len(synh_i.intersection(cn.df_diccionario_generales[wt][r_g]))>0:\n",
    "            return True,r_g\n",
    "        elif wh in cn.df_diccionario_especificas[wt][r_g] or len(synh_i.intersection(cn.df_diccionario_especificas[wt][r_g]))>0:\n",
    "            return True,r_g\n",
    "    for r_g in relaciones_g2:\n",
    "        if wt in cn.df_diccionario_generales[wh][r_g] or len(synt_i.intersection(cn.df_diccionario_generales[wh][r_g]))>0:\n",
    "            return True,r_g\n",
    "        elif wt in cn.df_diccionario_especificas[wh][r_g] or len(synt_i.intersection(cn.df_diccionario_especificas[wh][r_g]))>0:\n",
    "            return True,r_g   \n",
    "    return False,\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_info_adicional(wt,wh):\n",
    "    relaciones_adicionales=[\"part_of\",\"at_location\",\"has_a\",\"has_last_subevent\",\"has_property\",\n",
    "                            \"defined_as\",\"located_near\",\"receives_action\",\"made_of\",\"has_subevent\",\n",
    "                            \"has_first_subevent\",\"has_prerequisite\"]\n",
    "    setT_ia=set()\n",
    "    setH_ia=set()\n",
    "    for r_g in relaciones_adicionales:\n",
    "        setT_ia=setT_ia.union(cn.df_diccionario_generales[wt][r_g])\n",
    "        setH_ia=setH_ia.union(cn.df_diccionario_especificas[wh][r_g])\n",
    "    temSetT=set()\n",
    "    for r_g in [\"part_of\"]:\n",
    "        for e in setT_ia:\n",
    "            if e in cn.df_diccionario_generales:\n",
    "                temSetT=temSetT.union(cn.df_diccionario_generales[e][r_g])\n",
    "    setT_ia=setT_ia.union(temSetT)\n",
    "    #print(\"texto\",setT_ia)\n",
    "    #print(\"hipotesis\",setH_ia)\n",
    "    \n",
    "    intersec=(setT_ia).intersection(setH_ia)\n",
    "    if len(intersec)>0:\n",
    "        #print(\"inter\",intersec)\n",
    "        return True,\"part_of\"\n",
    "    return False,\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_neutral_rel(wt,wh):\n",
    "    relaciones_g3=[\"related_to\",\"similar_to\"]\n",
    "    for r_g in relaciones_g3:\n",
    "        if wh in cn.df_diccionario_generales[wt][r_g]:\n",
    "            return True,r_g\n",
    "    for r_g in relaciones_g3:\n",
    "        if wh in cn.df_diccionario_especificas[wt][r_g]:\n",
    "            return True,r_g\n",
    "    return False,\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Método para checar atributos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Crear modulo matemático\n",
    "# numero=\"\"\n",
    "#     for h in h_atributos:\n",
    "#         sin=cn.df_diccionario_generales[h][\"synonym\"].union(cn.df_diccionario_especificas[h][\"synonym\"])\n",
    "#         print(sin)\n",
    "#         for e in sin:\n",
    "#             print(e)\n",
    "#             try:\n",
    "#                 numero=int(e)\n",
    "#                 print(numero)\n",
    "#             except:\n",
    "#                 pass\n",
    "#         if numero!=\"\":\n",
    "#             break\n",
    "#     print(\"numero\",numero)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "palabras_negacion_adicionales = [\n",
    "    \"no\", \"not\", \"n't\", \"never\", \"none\", \"nobody\", \"nowhere\", \"nothing\", \n",
    "    \"neither\", \"nor\", \"without\", \"cannot\", \"can't\", \"did not\", \"didn't\", \n",
    "    \"does not\", \"doesn't\", \"do not\", \"don't\", \"will not\", \"won't\", \n",
    "    \"would not\", \"wouldn't\", \"could not\", \"couldn't\", \"should not\", \"shouldn't\", \n",
    "    \"must not\", \"mustn't\", \"might not\", \"mightn't\", \"may not\", \"lack\", \n",
    "    \"absent\", \"fail\", \"deny\", \"refuse\", \"against\", \"opposite\", \"exclude\", \n",
    "    \"except\", \"prevent\", \"avoid\", \"prohibit\", \"ban\", \"restrict\", \"decline\", \n",
    "    \"reject\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def found_neg(at_t):\n",
    "    for t in at_t:\n",
    "        if t in palabras_negacion_adicionales:\n",
    "            return True,t\n",
    "    return False,\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOP_WORDS_RT={\"'d\",\"'ll\",\"'m\",\"'re\",\"'s\",\"'ve\",'a','am','an','and','are','as','at', 'be','i', 'if', 'in', 'is', 'it', 'its',\n",
    "             'itself','me','my','of','or','our', 'ours', 'ourselves','so', 'than', 'that', 'the','their', 'them',\n",
    "              'themselves','there', 'thereafter', 'thereby','they','this', 'those','to','thus','us','was', 'we','were',\n",
    "              'you', 'your', 'yours', 'yourself', 'yourselves', '‘d', '‘ll', '‘m', '‘re', '‘s', '‘ve', '’d', '’ll', '’m',\n",
    "               '’re', '’s', '’ve'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_atributos(at_t,at_h):\n",
    "    t_temp = set(ptxt.eliminacion_espacios(at_t.split(\",\")))\n",
    "    h_temp = set(ptxt.eliminacion_espacios(at_h.split(\",\")))\n",
    "    \n",
    "    t_atributos =set()\n",
    "    h_atributos =set()\n",
    "    \n",
    "    for t_ in t_temp:\n",
    "        if t_ not in STOP_WORDS_RT and t_!=\"\" and t_!=\" \" and t_!=\",\":\n",
    "            t_atributos.add(t_)\n",
    "    for h_ in h_temp:\n",
    "        if h_ not in STOP_WORDS_RT and h_!=\"\" and h_!=\" \" and h_!=\",\":\n",
    "            h_atributos.add(h_)\n",
    "    print(\"atributos de T\",t_atributos)\n",
    "    print(\"atributos de H\",h_atributos)\n",
    "    vt,fN_t=found_neg(t_atributos)\n",
    "    vh,fN_h=found_neg(h_atributos)\n",
    "    if vt!=False:\n",
    "        return False,fN_t,\"\"\n",
    "    if vh!=False:\n",
    "        return False,\"\",fN_h\n",
    "    # Checar cuantos atributos de h están contenidos en T\n",
    "    found_att_t=[]\n",
    "    found_att_h=[]\n",
    "    matches=0\n",
    "    for h_a in h_atributos:\n",
    "        for t_a in t_atributos:\n",
    "            # relaciones generales\n",
    "            verificacion,tupla=check_entail_syn(t_a,h_a) #COMPARACION SINONIMOS DE wt y wh\n",
    "            if verificacion:\n",
    "                matches+=1\n",
    "                found_att_t.append(t_a)\n",
    "                found_att_h.append(h_a)\n",
    "                break\n",
    "            # checo en relaciones generales si se encuentra el token de la hipótesis\n",
    "            verificacion,tupla=check_entail_gen(t_a,h_a) #COMPARACION GENERAL DE wt y wh\n",
    "            if verificacion:\n",
    "                found_att_t.append(t_a)\n",
    "                found_att_h.append(h_a)\n",
    "                matches+=1\n",
    "                break\n",
    "            # contradicciones\n",
    "            verificacion,tupla=check_contradiction_ant(t_a,h_a) #COMPARACION CONTRA DE wt y wh\n",
    "            if verificacion:\n",
    "                return False,t_a,h_a\n",
    "            #conjuntos de información adicional part_of escalar y subir\n",
    "            verificacion,tupla=check_info_adicional(t_a,h_a) #COMPARACION INFO ADICIONAL DE wt y wh\n",
    "            if verificacion:\n",
    "                found_att_t.append(t_a)\n",
    "                found_att_h.append(h_a)\n",
    "                matches+=1\n",
    "                break\n",
    "            #especificas\n",
    "            verificacion,tupla=check_neutral_rel(t_a,h_a) #COMPARACION NEUTRAL DE wt y wh\n",
    "            if verificacion:\n",
    "                return False,t_a,h_a\n",
    "    if len(h_atributos)==0:\n",
    "        return True, \"\",\"\"\n",
    "    elif len(t_atributos)==0 and len(h_atributos)==0:\n",
    "        return True, \"\",\"\"\n",
    "    elif matches==len(h_atributos):\n",
    "        return True, \" \".join(found_att_t),\" \".join(found_att_h)\n",
    "    else:\n",
    "        return False, \"\",\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\"'d\",\n",
       " \"'ll\",\n",
       " \"'m\",\n",
       " \"'re\",\n",
       " \"'s\",\n",
       " \"'ve\",\n",
       " 'a',\n",
       " 'am',\n",
       " 'an',\n",
       " 'and',\n",
       " 'are',\n",
       " 'as',\n",
       " 'at',\n",
       " 'be',\n",
       " 'i',\n",
       " 'if',\n",
       " 'in',\n",
       " 'is',\n",
       " 'it',\n",
       " 'its',\n",
       " 'itself',\n",
       " 'me',\n",
       " 'my',\n",
       " 'of',\n",
       " 'or',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'so',\n",
       " 'than',\n",
       " 'that',\n",
       " 'the',\n",
       " 'their',\n",
       " 'them',\n",
       " 'themselves',\n",
       " 'there',\n",
       " 'thereafter',\n",
       " 'thereby',\n",
       " 'they',\n",
       " 'this',\n",
       " 'those',\n",
       " 'thus',\n",
       " 'to',\n",
       " 'us',\n",
       " 'was',\n",
       " 'we',\n",
       " 'were',\n",
       " 'you',\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " '‘d',\n",
       " '‘ll',\n",
       " '‘m',\n",
       " '‘re',\n",
       " '‘s',\n",
       " '‘ve',\n",
       " '’d',\n",
       " '’ll',\n",
       " '’m',\n",
       " '’re',\n",
       " '’s',\n",
       " '’ve'}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#stop_words = spacy.lang.en.stop_words.STOP_WORDS\n",
    "#stop_words\n",
    "STOP_WORDS_RT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# la real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Two young women walking in the street.\n",
      "['NUM', 'ADJ', 'NOUN', 'VERB', 'ADP', 'DET', 'NOUN', 'PUNCT']\n",
      "['two', 'young', 'woman', 'walk', 'in', 'the', 'street', '.']\n",
      "['two', 'young', 'women', 'walking', 'in', 'the', 'street', '.']\n",
      "['women', 'women', 'women', 'women', 'walking', 'street', 'in', 'women']\n",
      "[[], [], [two, young, walking, .], [in], [street], [], [the], []]\n",
      "Diccionario_T {'young': '', 'woman': 'two,young,', 'walk': '', 'street': ''}\n",
      "Claves_T ['young', 'woman', 'walk', 'street']\n",
      "Lemmas T ['two', 'young', 'woman', 'walk', 'in', 'the', 'street', '.']\n",
      "pos T ['NUM', 'ADJ', 'NOUN', 'VERB', 'ADP', 'DET', 'NOUN', 'PUNCT']\n",
      "The three women are racing down the road.\n",
      "['DET', 'NUM', 'NOUN', 'AUX', 'VERB', 'ADP', 'DET', 'NOUN', 'PUNCT']\n",
      "['the', 'three', 'woman', 'be', 'race', 'down', 'the', 'road', '.']\n",
      "['the', 'three', 'women', 'are', 'racing', 'down', 'the', 'road', '.']\n",
      "['women', 'women', 'racing', 'racing', 'racing', 'racing', 'road', 'down', 'racing']\n",
      "[[], [], [the, three], [], [women, are, down, .], [road], [], [the], []]\n",
      "Diccionario_H {'woman': 'three', 'race': 'be,down,', 'road': ''}\n",
      "Claves_H ['woman', 'race', 'road']\n",
      "lemmas_H ['the', 'three', 'woman', 'be', 'race', 'down', 'the', 'road', '.']\n",
      "pos H ['DET', 'NUM', 'NOUN', 'AUX', 'VERB', 'ADP', 'DET', 'NOUN', 'PUNCT']\n",
      "[{'the', 'ye'}, {'terzetto', 'iii', 'deuce_ace', 'triplet', 'trinity', 'triad', 'triduan', 'leash', 'ternion', '3', 'ternary', 'trine', 'threesome', 'trey', 'trio', 'tercet', 'tierce', 'three', 'troika'}, {'lady', 'adult_female', 'cowgirl', 'birther', 'female', 'womanhood', 'charwoman', 'carline', 'woman'}, {'live', 'cost', 'embody', 'exist', 'beryllium', 'sit', 'rest', 'constitute', 'be_going_to', 'hunger', 'equal', 'be', 'need', 'get', 'follow', 'future', 'bean’t'}, {'rush', 'dl', 'slipstream', 'subspecies', 'hare', 'raceway', 'breed', 'run', 'race', 'variety'}, {'down_feather', 'polish', 'toss_off', 'cut_down', 'pile', 'shoot_down', 'land', 'knock_down', 'downwardly', 'john_l_h_down', 'pull_down', 'down_in_mouth', 'devour', 'down_pat', 'depressed', 'push_down', 'downward'}, {'the', 'ye'}, {'route', 'street', 'road', 'roadway', 'pass', 'away', 'high_road', 'way'}, set()]\n",
      "--------------------------------------------------------\n",
      "two young woman walk in the street .\n",
      "[]\n",
      "        woman  race  road\n",
      "young    0.29  0.03  0.00\n",
      "woman    1.00  0.07  0.06\n",
      "walk     0.01  0.14  0.30\n",
      "street   0.03  0.12  0.63\n",
      "proceso lexico\n",
      "        woman  race  road\n",
      "young    0.29  0.03  0.00\n",
      "woman    1.00  0.07  0.06\n",
      "walk     0.01  0.14  0.30\n",
      "street   0.03  0.12  0.63 Index(['woman', 'race', 'road'], dtype='object')\n",
      "columna a checar woman\n",
      "atributos de T {'two', 'young'}\n",
      "atributos de H {'three'}\n",
      "columna a checar race\n",
      "columna a checar road\n",
      "atributos de T set()\n",
      "atributos de H set()\n",
      "*************************************\n",
      "g1 [(' street', 'synonym', ' road')]\n",
      "g2 [('two woman', 'distinct_from', 'three woman'), ('walk', 'antonym', 'race')]\n",
      "g3 []\n",
      "g4 []\n",
      "st [(' street', 'synonym', ' road'), ('two woman', 'distinct_from', 'three woman'), ('walk', 'antonym', 'race')]\n",
      "*************************************\n"
     ]
    }
   ],
   "source": [
    "texto_i=\"Two young women walking in the street.\"\n",
    "hipotesis_i=\"The three women are racing down the road.\"\n",
    "\n",
    "# limpieza de texto e hipótesis\n",
    "s1=str(texto_i).lower()\n",
    "for c in punct:\n",
    "    s1 = s1.lower().replace(c, \" \")\n",
    "t_lem=s1.split()\n",
    "\n",
    "s2=str(hipotesis_i).lower()\n",
    "for c in punct:\n",
    "    s1 = s1.lower().replace(c, \" \")\n",
    "t_lem=s1.split()\n",
    "\n",
    "# Agrupación sintáctica con diccionarios\n",
    "print(texto_i)\n",
    "r_t,t_clean_m,lemmas_t,pos_t=ptxt.representacion_entidadesDavid(nlp,texto_i)\n",
    "print(\"Diccionario_T\",r_t)\n",
    "print(\"Claves_T\",t_clean_m)\n",
    "print(\"Lemmas T\",lemmas_t)\n",
    "print(\"pos T\",pos_t)\n",
    "\n",
    "print(hipotesis_i)\n",
    "r_h,h_clean_m,lemmas_h,pos_h=ptxt.representacion_entidadesDavid(nlp,hipotesis_i)\n",
    "print(\"Diccionario_H\",r_h)\n",
    "print(\"Claves_H\",h_clean_m)\n",
    "print(\"lemmas_H\",lemmas_h)\n",
    "print(\"pos H\",pos_h)\n",
    "\n",
    "# lista de relaciones\n",
    "lista_rel_G1=[]\n",
    "lista_rel_G2=[]\n",
    "lista_rel_G3=[]\n",
    "lista_rel_G4=[]\n",
    "\n",
    "# primero evaluar si existen acronimos que se puedan identificar\n",
    "sinT=[]\n",
    "sinH=[]\n",
    "for t in lemmas_h:\n",
    "    if t in cn.df_diccionario_generales:\n",
    "        sinH.append((cn.df_diccionario_generales[t][\"synonym\"]).union(cn.df_diccionario_especificas[t][\"synonym\"]))\n",
    "print(sinH)\n",
    "new_text=\" \".join(lemmas_t)\n",
    "words_found=[]\n",
    "print(\"--------------------------------------------------------\")\n",
    "for e_i in range(len(sinH)):\n",
    "    for e_syn in sinH[e_i]:\n",
    "        if \"_\" in e_syn:\n",
    "            nsin=str(e_syn).replace(\"_\",\" \")\n",
    "            #print(nsin)\n",
    "            if(\" \"+nsin+\" \" in new_text):\n",
    "                lista_rel_G1.append((lemmas_h[e_i],\"synonym\",nsin))\n",
    "                words_found.append(lemmas_h[e_i])\n",
    "                print(\"se encontro\",lemmas_h[e_i],nsin)\n",
    "print(new_text)\n",
    "print(words_found)\n",
    "\n",
    "# Matriz de alineamiento para probar la contención de las entidades\n",
    "\n",
    "t_vectors_n=ut.get_matrix_rep2(t_clean_m, nlp, normed=True)\n",
    "h_vectors_n=ut.get_matrix_rep2(h_clean_m, nlp, normed=True)\n",
    "\n",
    "redondeo=2\n",
    "ma_n=np.dot(t_vectors_n,h_vectors_n.T)\n",
    "ma_n = np.clip(ma_n, 0, 1).round(redondeo)\n",
    "ma=pd.DataFrame(ma_n,index=t_clean_m,columns=h_clean_m)\n",
    "print(ma)\n",
    "\n",
    "top_k=3\n",
    "# # #PARA REVISAR SI EXISTEN RELACIONES DE SIMILITUD SEMÁNTICA A TRAVÉS DEL USO DE CONCEPNET\n",
    "print(\"proceso lexico\")\n",
    "print(ma,ma.columns)\n",
    "borrar_g=[]\n",
    "borrar_c=[]\n",
    "borrar_e=[]\n",
    "for c_c in ma.columns:\n",
    "    if c_c not in words_found:\n",
    "        print(\"columna a checar\",c_c)\n",
    "        # filtrar el top 3 de los mejores similitud coseno para cada token de H vs tokens de T que sean mayores a 0\n",
    "        # una vez que encontremos quien se sale del ciclo\n",
    "        temp=ma[c_c].sort_values(ascending=False)\n",
    "        ranks=list(temp[:top_k].index)\n",
    "        #valranks=list(temp[:top_k].values)\n",
    "        #print(valranks,ranks)\n",
    "        matching=False\n",
    "        for r_i in range(len(ranks)): \n",
    "            # relaciones generales\n",
    "            verificacion,rel_found=check_entail_syn(ranks[r_i],c_c) #COMPARACION SINONIMOS DE wt y wh\n",
    "            if verificacion:\n",
    "                verif_att,att_t,att_h=check_atributos(r_t[ranks[r_i]],r_h[c_c])\n",
    "                if verif_att:\n",
    "                    lista_rel_G1.append((att_t+\" \"+ranks[r_i],rel_found,att_h+\" \"+c_c))\n",
    "                    matching=True\n",
    "                    break\n",
    "                else:\n",
    "                    lista_rel_G2.append((att_t+\" \"+ranks[r_i],\"distinct_from\",att_h+\" \"+c_c))\n",
    "                    matching=True\n",
    "                    break\n",
    "            # checo en relaciones generales si se encuentra el token de la hipótesis\n",
    "            verificacion,rel_found=check_entail_gen(ranks[r_i],c_c) #COMPARACION GENERAL DE wt y wh\n",
    "            if verificacion:\n",
    "                verif_att,att_t,att_h=check_atributos(r_t[ranks[r_i]],r_h[c_c])\n",
    "                if verif_att:\n",
    "                    lista_rel_G1.append((att_t+\" \"+ranks[r_i],rel_found,att_h+\" \"+c_c))\n",
    "                    matching=True\n",
    "                    break\n",
    "                else:\n",
    "                    lista_rel_G2.append((att_t+\" \"+ranks[r_i],\"distinct_from\",att_h+\" \"+c_c))\n",
    "                    matching=True\n",
    "                    break\n",
    "            # contradicciones\n",
    "            verificacion,rel_found=check_contradiction_ant(ranks[r_i],c_c) #COMPARACION CONTRA DE wt y wh\n",
    "            if verificacion:\n",
    "                lista_rel_G2.append((ranks[r_i],rel_found,c_c))\n",
    "                matching=True\n",
    "                break\n",
    "            #conjuntos de información adicional part_of escalar y subir\n",
    "            verificacion,rel_found=check_info_adicional(ranks[r_i],c_c)#COMPARACION NEUTRAL DE wt y wh\n",
    "            if verificacion:\n",
    "                verif_att,att_t,att_h=check_atributos(r_t[ranks[r_i]],r_h[c_c])\n",
    "                if verif_att:\n",
    "                    lista_rel_G1.append((att_t+\" \"+ranks[r_i],rel_found,att_h+\" \"+c_c))\n",
    "                    matching=True\n",
    "                    break\n",
    "                else:\n",
    "                    lista_rel_G2.append((att_t+\" \"+ranks[r_i],\"distinct_from\",att_h+\" \"+c_c))\n",
    "                    matching=True\n",
    "                    break\n",
    "            #especificas\n",
    "            verificacion,rel_found=check_neutral_rel(ranks[r_i],c_c)\n",
    "            if verificacion:\n",
    "                lista_rel_G3.append((ranks[r_i],rel_found,c_c))\n",
    "                matching=True\n",
    "                break\n",
    "        if matching==False:\n",
    "            lista_rel_G4.append((\"\",\"unknown\",c_c))\n",
    "print(\"*************************************\")\n",
    "print(\"g1\",lista_rel_G1)\n",
    "print(\"g2\",lista_rel_G2)\n",
    "print(\"g3\",lista_rel_G3)\n",
    "print(\"g4\",lista_rel_G4)\n",
    "lista_rel_ST=[]\n",
    "lista_rel_ST.extend(lista_rel_G1)\n",
    "lista_rel_ST.extend(lista_rel_G2)\n",
    "lista_rel_ST.extend(lista_rel_G3)\n",
    "lista_rel_ST.extend(lista_rel_G4)\n",
    "print(\"st\",lista_rel_ST)\n",
    "print(\"*************************************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# todas las relaciones con top-k = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Musician playing guitar without the drummer.\n",
      "['NOUN', 'VERB', 'NOUN', 'ADP', 'DET', 'NOUN', 'PUNCT']\n",
      "['musician', 'play', 'guitar', 'without', 'the', 'drummer', '.']\n",
      "['musician', 'playing', 'guitar', 'without', 'the', 'drummer', '.']\n",
      "['guitar', 'guitar', 'guitar', 'guitar', 'drummer', 'without', 'guitar']\n",
      "[[], [], [musician, playing, without, .], [drummer], [], [the], []]\n",
      "Diccionario_T {'musician': '', 'play': '', 'guitar': 'without', 'drummer': ''}\n",
      "Claves_T ['musician', 'play', 'guitar', 'drummer']\n",
      "Lemmas T ['musician', 'play', 'guitar', 'without', 'the', 'drummer', '.']\n",
      "pos T ['NOUN', 'VERB', 'NOUN', 'ADP', 'DET', 'NOUN', 'PUNCT']\n",
      "Music is being played with no drummer.\n",
      "['NOUN', 'AUX', 'AUX', 'VERB', 'ADP', 'DET', 'NOUN', 'PUNCT']\n",
      "['music', 'be', 'be', 'play', 'with', 'no', 'drummer', '.']\n",
      "['music', 'is', 'being', 'played', 'with', 'no', 'drummer', '.']\n",
      "['played', 'played', 'played', 'played', 'played', 'drummer', 'with', 'played']\n",
      "[[], [], [], [music, is, being, with, .], [drummer], [], [no], []]\n",
      "Diccionario_H {'music': '', 'play': 'be,be,,with,', 'drummer': 'no'}\n",
      "Claves_H ['music', 'play', 'drummer']\n",
      "lemmas_H ['music', 'be', 'be', 'play', 'with', 'no', 'drummer', '.']\n",
      "pos H ['NOUN', 'AUX', 'AUX', 'VERB', 'ADP', 'DET', 'NOUN', 'PUNCT']\n",
      "[{'euphony', 'melody', 'vibe', 'music', 'medicine'}, {'follow', 'be_going_to', 'embody', 'exist', 'rest', 'future', 'constitute', 'sit', 'beryllium', 'hunger', 'be', 'get', 'bean’t', 'need', 'cost', 'live', 'equal'}, {'follow', 'be_going_to', 'embody', 'exist', 'rest', 'future', 'constitute', 'sit', 'beryllium', 'hunger', 'be', 'get', 'bean’t', 'need', 'cost', 'live', 'equal'}, {'playing_period', 'toy', 'play', 'maneuver', 'free_rein', 'fun', 'act', 'turn', 'caper', 'looseness', 'gambling', 'bet', 'dally', 'ludic', 'instrument', 'meet', 'run', 'dramatic_play', 'romp', 'recreate', 'reenact', 'gambol', 'theatre_play', 'bring', 'drama', 'bid', \"child's_play\", 'swordplay', 'spiel', 'frolic', 'shimmer'}, {'by_means_of', 'c̄', 'gas', 'in_case_of', 'in_exchange_for', 'opposed_to', 'w', 'across_from'}, {'nah', 'nope', 'zero', 'uh_uh', 'no', 'negative', 'nay', 'ordinal_number', 'unsupported_titles_number_sign', 'non', 'no_way', 'fuck_all', 'yeah_no', 'no_more', 'yeah_right', 'nobelium'}, {'trumpeter_pigeon', 'percussionist', 'hawker', 'drummer'}, set()]\n",
      "------------------\n",
      "musician play guitar without the drummer .\n",
      "[]\n",
      "          music  play  drummer\n",
      "musician   0.66  0.19     0.67\n",
      "play       0.24  1.00     0.12\n",
      "guitar     0.55  0.27     0.61\n",
      "drummer    0.43  0.12     1.00\n",
      "proceso lexico\n",
      "          music  play  drummer\n",
      "musician   0.66  0.19     0.67\n",
      "play       0.24  1.00     0.12\n",
      "guitar     0.55  0.27     0.61\n",
      "drummer    0.43  0.12     1.00 Index(['music', 'play', 'drummer'], dtype='object')\n",
      "columna a checar music\n",
      "[0.66, 0.55, 0.43] ['musician', 'guitar', 'drummer']\n",
      "texto {'orchestra', 'theater', 'tour_bus', 'journey', 'symphony', 'tour', 'stage', 'studio'}\n",
      "hipotesis {'playing_violin', 'note', 'beats_per_minute', 'dancing', 'play_piano', 'woodwind_family', 'playing_piano', 'hearing_singing', 'having_party', 'dancing_samba', 'perform', 'song', 'cd', 'sound'}\n",
      "('musician', 'related_to', 'music')\n",
      "music musician related_to especificas\n",
      "guitar music used_for generales\n",
      "texto {'rock_band', 'demonstration'}\n",
      "hipotesis {'playing_violin', 'note', 'beats_per_minute', 'dancing', 'play_piano', 'woodwind_family', 'playing_piano', 'hearing_singing', 'having_party', 'dancing_samba', 'perform', 'song', 'cd', 'sound'}\n",
      "columna a checar play\n",
      "[1.0, 0.27, 0.19] ['play', 'guitar', 'musician']\n",
      "play play same\n",
      "texto {'toy_store', 'six_strings', 'store', 'strings', '6_strings', 'wood', 'band', 'difficult_to_play', 'rock_band', 'one_kind_of_stringed_instrument', 'concert', 'case', 'music_room', 'garment', 'guitaroo_man'}\n",
      "hipotesis {'do_housework', 'scene', 'intermission', 'have_fun', 'act', 'stage_direction', 'role', 'actors', 'performance', 'line'}\n",
      "('guitar', 'related_to', 'play')\n",
      "play guitar related_to especificas\n",
      "texto {'orchestra', 'theater', 'tour_bus', 'journey', 'symphony', 'tour', 'stage', 'studio'}\n",
      "hipotesis {'do_housework', 'scene', 'intermission', 'have_fun', 'act', 'stage_direction', 'role', 'actors', 'performance', 'line'}\n",
      "columna a checar drummer\n",
      "[1.0, 0.67, 0.61] ['drummer', 'musician', 'guitar']\n",
      "drummer drummer same\n",
      "texto {'orchestra', 'theater', 'tour_bus', 'journey', 'symphony', 'tour', 'stage', 'studio'}\n",
      "hipotesis set()\n",
      "texto {'toy_store', 'six_strings', 'store', 'strings', '6_strings', 'wood', 'band', 'difficult_to_play', 'rock_band', 'one_kind_of_stringed_instrument', 'concert', 'case', 'music_room', 'garment', 'guitaroo_man'}\n",
      "hipotesis set()\n",
      "*************************************\n",
      "g1 [('guitar', 'used_for', 'music'), ('play', 'same', 'play'), ('drummer', 'same', 'drummer')]\n",
      "g2 []\n",
      "g3 [('music', 'related_to', 'musician'), ('play', 'related_to', 'guitar')]\n",
      "g4 []\n",
      "*************************************\n"
     ]
    }
   ],
   "source": [
    "texto_i=\"Musician playing guitar without the drummer.\"\n",
    "hipotesis_i=\"Music is being played with no drummer.\"\n",
    "\n",
    "# limpieza de texto e hipótesis\n",
    "s1=str(texto_i).lower()\n",
    "for c in punct:\n",
    "    s1 = s1.lower().replace(c, \" \")\n",
    "t_lem=s1.split()\n",
    "\n",
    "s2=str(hipotesis_i).lower()\n",
    "for c in punct:\n",
    "    s1 = s1.lower().replace(c, \" \")\n",
    "t_lem=s1.split()\n",
    "\n",
    "# Agrupación sintáctica con diccionarios\n",
    "print(texto_i)\n",
    "r_t,t_clean_m,lemmas_t,pos_t=ptxt.representacion_entidadesDavid(nlp,texto_i)\n",
    "print(\"Diccionario_T\",r_t)\n",
    "print(\"Claves_T\",t_clean_m)\n",
    "print(\"Lemmas T\",lemmas_t)\n",
    "print(\"pos T\",pos_t)\n",
    "\n",
    "print(hipotesis_i)\n",
    "r_h,h_clean_m,lemmas_h,pos_h=ptxt.representacion_entidadesDavid(nlp,hipotesis_i)\n",
    "print(\"Diccionario_H\",r_h)\n",
    "print(\"Claves_H\",h_clean_m)\n",
    "print(\"lemmas_H\",lemmas_h)\n",
    "print(\"pos H\",pos_h)\n",
    "\n",
    "# lista de relaciones\n",
    "lista_rel_G1=[]\n",
    "lista_rel_G2=[]\n",
    "lista_rel_G3=[]\n",
    "lista_rel_G4=[]\n",
    "\n",
    "# primero evaluar si existen acronimos que se puedan identificar\n",
    "sinT=[]\n",
    "sinH=[]\n",
    "for t in lemmas_h:\n",
    "    if t in cn.df_diccionario_generales:\n",
    "        sinH.append((cn.df_diccionario_generales[t][\"synonym\"]).union(cn.df_diccionario_especificas[t][\"synonym\"]))\n",
    "print(sinH)\n",
    "new_text=\" \".join(lemmas_t)\n",
    "words_found=[]\n",
    "print(\"------------------\")\n",
    "for e_i in range(len(sinH)):\n",
    "    for e_syn in sinH[e_i]:\n",
    "        if \"_\" in e_syn:\n",
    "            nsin=str(e_syn).replace(\"_\",\" \")\n",
    "            #print(nsin)\n",
    "            if(nsin in new_text):\n",
    "                lista_rel_G1.append((lemmas_h[e_i],\"synonym\",nsin))\n",
    "                words_found.append(lemmas_h[e_i])\n",
    "                print(\"se encontro\",lemmas_h[e_i])\n",
    "print(new_text)\n",
    "print(words_found)\n",
    "\n",
    "\n",
    "\n",
    "# Matriz de alineamiento para probar la contención de las entidades\n",
    "\n",
    "t_vectors_n=ut.get_matrix_rep2(t_clean_m, nlp, normed=True)\n",
    "h_vectors_n=ut.get_matrix_rep2(h_clean_m, nlp, normed=True)\n",
    "\n",
    "redondeo=2\n",
    "ma_n=np.dot(t_vectors_n,h_vectors_n.T)\n",
    "ma_n = np.clip(ma_n, 0, 1).round(redondeo)\n",
    "ma=pd.DataFrame(ma_n,index=t_clean_m,columns=h_clean_m)\n",
    "print(ma)\n",
    "\n",
    "top_k=3\n",
    "# # #PARA REVISAR SI EXISTEN RELACIONES DE SIMILITUD SEMÁNTICA A TRAVÉS DEL USO DE CONCEPNET\n",
    "print(\"proceso lexico\")\n",
    "print(ma,ma.columns)\n",
    "borrar_g=[]\n",
    "borrar_c=[]\n",
    "borrar_e=[]\n",
    "for c_c in ma.columns:\n",
    "    if c_c not in words_found:\n",
    "        print(\"columna a checar\",c_c)\n",
    "        # filtrar el top 3 de los mejores similitud coseno para cada token de H vs tokens de T que sean mayores a 0\n",
    "        # una vez que encontremos quien se sale del ciclo\n",
    "        temp=ma[c_c].sort_values(ascending=False)\n",
    "        ranks=list(temp[:top_k].index)\n",
    "        valranks=list(temp[:top_k].values)\n",
    "        print(valranks,ranks)\n",
    "\n",
    "        matching=False\n",
    "        for r_i in range(len(ranks)): \n",
    "            # relaciones generales\n",
    "            if ranks[r_i] ==c_c:\n",
    "                print(ranks[r_i],c_c,\"same\")\n",
    "                lista_rel_G1.append((ranks[r_i],\"same\",c_c))\n",
    "                matching=True\n",
    "                #break\n",
    "            else:\n",
    "                # guardo conjuntos de sinonimos para un uso posterior\n",
    "                synt_i=cn.df_diccionario_generales[ranks[r_i]][\"synonym\"]\n",
    "                synt_i=synt_i.union(cn.df_diccionario_especificas[ranks[r_i]][\"synonym\"])\n",
    "                synh_i=cn.df_diccionario_generales[c_c][\"synonym\"]\n",
    "                synh_i=synh_i.union(cn.df_diccionario_especificas[c_c][\"synonym\"])\n",
    "\n",
    "                if c_c in synt_i:\n",
    "                    lista_rel_G1.append((ranks[r_i],\"synonym\",c_c))\n",
    "                    print(ranks[r_i],c_c,r_g,\"generales\")\n",
    "                    matching=True\n",
    "                    #break\n",
    "                elif ranks[r_i] in synh_i:\n",
    "                    lista_rel_G1.append((ranks[r_i],\"synonym\",c_c))\n",
    "                    print(ranks[r_i],c_c,r_g,\"generales\")\n",
    "                    matching=True\n",
    "                    #break\n",
    "                else:\n",
    "                    found=False\n",
    "                    # checo en relaciones generales si se encuentra el token de la hipótesis\n",
    "                    for r_g in relaciones_g1:\n",
    "                        if c_c in cn.df_diccionario_generales[ranks[r_i]][r_g]:\n",
    "                            lista_rel_G1.append((ranks[r_i],r_g,c_c))\n",
    "                            print(ranks[r_i],c_c,r_g,\"generales\")\n",
    "                            matching=True\n",
    "                            found=True\n",
    "                            break\n",
    "                    if found==False:\n",
    "                        #break\n",
    "                        # contradicciones\n",
    "                        for r_g in relaciones_g2:\n",
    "                            if c_c in cn.df_diccionario_generales[ranks[r_i]][r_g] or len(synh_i.intersection(cn.df_diccionario_generales[ranks[r_i]][r_g]))>0:\n",
    "                                lista_rel_G2.append((ranks[r_i],r_g,c_c))\n",
    "                                print(ranks[r_i],c_c,r_g,\"generales\")\n",
    "                                matching=True\n",
    "                                found=True\n",
    "                                break\n",
    "                            elif c_c in cn.df_diccionario_especificas[ranks[r_i]][r_g] or len(synh_i.intersection(cn.df_diccionario_especificas[ranks[r_i]][r_g]))>0:\n",
    "                                lista_rel_G2.append((ranks[r_i],r_g,c_c))\n",
    "                                print(ranks[r_i],c_c,r_g,\"generales\")\n",
    "                                matching=True\n",
    "                                found=True\n",
    "                                break\n",
    "                        if found==False:\n",
    "                            #break        \n",
    "                            for r_g in relaciones_g2:\n",
    "                                if ranks[r_i] in cn.df_diccionario_generales[c_c][r_g] or len(synt_i.intersection(cn.df_diccionario_generales[c_c][r_g]))>0:\n",
    "                                    lista_rel_G2.append((ranks[r_i],r_g,c_c))\n",
    "                                    print(c_c,ranks[r_i],r_g,\"especificas\")\n",
    "                                    matching=True\n",
    "                                    found=True\n",
    "                                    break\n",
    "                                elif ranks[r_i] in cn.df_diccionario_especificas[c_c][r_g] or len(synt_i.intersection(cn.df_diccionario_especificas[c_c][r_g]))>0:\n",
    "                                    lista_rel_G2.append((ranks[r_i],r_g,c_c))\n",
    "                                    print(c_c,ranks[r_i],r_g,\"especificas\")\n",
    "                                    matching=True\n",
    "                                    found=True\n",
    "                                    break\n",
    "                            if found==False:\n",
    "                                #break\n",
    "                                #conjuntos de información adicional part_of escalar y subir\n",
    "                                setT_ia=set()\n",
    "                                setH_ia=set()\n",
    "                                for r_g in relaciones_adicionales:\n",
    "                                    setT_ia=setT_ia.union(cn.df_diccionario_generales[ranks[r_i]][r_g])\n",
    "                                    #print(ranks[r_i],cn.df_diccionario_generales[ranks[r_i]][r_g],r_g)\n",
    "                                    setH_ia=setH_ia.union(cn.df_diccionario_especificas[c_c][r_g])\n",
    "                                temSetT=set()\n",
    "                                for r_g in [\"part_of\"]:\n",
    "                                    for e in setT_ia:\n",
    "                                        if e in cn.df_diccionario_generales:\n",
    "                                            temSetT=temSetT.union(cn.df_diccionario_generales[e][r_g])\n",
    "                                setT_ia=setT_ia.union(temSetT)\n",
    "                                print(\"texto\",setT_ia)\n",
    "                                print(\"hipotesis\",setH_ia)\n",
    "                                \n",
    "                                intersec=(setT_ia).intersection(setH_ia)\n",
    "                                if len(intersec)>0:\n",
    "                                    print(\"inter\",intersec)\n",
    "                                    lista_rel_G1.append((ranks[r_i],\"part_of\",c_c))\n",
    "                                    print(ranks[r_i],c_c,\"part_of\",\"generales\")\n",
    "                                    matching=True\n",
    "                                    found=True\n",
    "                                #     break\n",
    "                                if found == False:\n",
    "                                    #especificas\n",
    "                                    for r_g in relaciones_g3:\n",
    "                                        if c_c in cn.df_diccionario_generales[ranks[r_i]][r_g]:\n",
    "                                            tupla_temp=(ranks[r_i],r_g,c_c)\n",
    "                                            print(tupla_temp)\n",
    "                                            if tupla_temp not in lista_rel_G3:\n",
    "                                                lista_rel_G3.append((c_c,r_g,ranks[r_i]))\n",
    "                                                print(c_c,ranks[r_i],r_g,\"especificas\")\n",
    "                                                matching=True\n",
    "                                                found=True\n",
    "                                                break\n",
    "                                    if found==False:\n",
    "                                        for r_g in relaciones_g3:\n",
    "                                            if c_c in cn.df_diccionario_especificas[ranks[r_i]][r_g]:\n",
    "                                                tupla_temp=(c_c,r_g,ranks[r_i])\n",
    "                                                print(tupla_temp)\n",
    "                                                if tupla_temp not in lista_rel_G3:\n",
    "                                                    lista_rel_G3.append((c_c,r_g,ranks[r_i]))\n",
    "                                                    print(c_c,ranks[r_i],r_g,\"especificas\")\n",
    "                                                    matching=True\n",
    "                                                    found=True\n",
    "                                                    break\n",
    "        if matching==False:\n",
    "            lista_rel_G4.append((c_c,\"UNK\",\"\"))\n",
    "print(\"*************************************\")\n",
    "print(\"g1\",lista_rel_G1)\n",
    "print(\"g2\",lista_rel_G2)\n",
    "print(\"g3\",lista_rel_G3)\n",
    "print(\"g4\",lista_rel_G4)\n",
    "print(\"*************************************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# sinT=[]\n",
    "# sinH=[]\n",
    "# for t in lemmas_h:\n",
    "#     if t in cn.df_diccionario_generales:\n",
    "#         sinH.append((cn.df_diccionario_generales[t][\"synonym\"]).union(cn.df_diccionario_especificas[t][\"synonym\"]))\n",
    "# print(sinH)\n",
    "\n",
    "\n",
    "# lista_rel_G1=[]\n",
    "\n",
    "\n",
    "# new_text=\" \".join(lemmas_t)\n",
    "# words_found=[]\n",
    "# print(\"------------------\")\n",
    "# for e_i in range(len(sinH)):\n",
    "#     for e_syn in sinH[e_i]:\n",
    "#         if \"_\" in e_syn:\n",
    "#             nsin=str(e_syn).replace(\"_\",\" \")\n",
    "#             #print(nsin)\n",
    "#             if(nsin in new_text):\n",
    "#                 lista_rel_G1.append((lemmas_h[e_i],\"synonym\",nsin))\n",
    "#                 words_found.append(lemmas_h[e_i])\n",
    "#                 print(\"se encontro\",lemmas_h[e_i])\n",
    "# print(new_text)\n",
    "# print(words_found)\n",
    "\n",
    "# # proceso de eliminación de entidades y overlap las cosas que están compartiendo en caso de faltar por eliminar a la matriz \n",
    "# lista_entidades_no_match=[]\n",
    "# lista_entidades_distintas=[]\n",
    "# lista_entidades_contra=[]\n",
    "# lista_entidades_contenidas=[]\n",
    "\n",
    "# lista_relaciones_grupos=[]\n",
    "\n",
    "# lista_rel_G2=[]\n",
    "# lista_rel_G3=[]\n",
    "# lista_rel_G4=[]\n",
    "# # conteo de relaciones de compatibilidad e incompatibilidad\n",
    "# c_compatibilidad=0\n",
    "# c_incompatibilidad=0\n",
    "# c_rel_concep=0\n",
    "    \n",
    "# for clave in r_h.keys():\n",
    "#     #print(\"hipotesis\",clave)\n",
    "#     if clave not in words_found:\n",
    "#         if clave in r_t:\n",
    "#             print(\"si esta\",clave)\n",
    "#             t_atributos = ptxt.eliminacion_espacios(r_t[clave].split(\",\"))\n",
    "#             h_atributos = ptxt.eliminacion_espacios(r_h[clave].split(\",\"))\n",
    "#             print(\"atributos de T\",t_atributos)\n",
    "#             print(\"atributos de H\",h_atributos)\n",
    "#             if \"no\" in h_atributos or \"not\" in h_atributos or \"no\" in t_atributos or \"not\" in t_atributos:\n",
    "#                 print(\"CONTRADICTION\")\n",
    "#                 c_incompatibilidad+=1\n",
    "#                 lista_rel_G2.append((clave,\"distinct_from\",\"no \"+clave))\n",
    "#                 lista_entidades_distintas.append(\"no \"+clave)\n",
    "#             elif len(h_atributos)>0:\n",
    "#                 matches=0\n",
    "#                 for h_a in h_atributos:\n",
    "#                     if h_a in t_atributos:\n",
    "#                         print(\"si esta\",h_a)\n",
    "#                         matches+=1\n",
    "#                     else:\n",
    "#                         print(\"busqueda\",h_a)\n",
    "#                         att_found=False\n",
    "#                         for attT in t_atributos:\n",
    "#                             ctat,rctat=cn.relacion_contra(h_a,attT)\n",
    "#                             if(ctat):\n",
    "#                                 print(\"CONTRADICTION_c\")\n",
    "#                                 lista_entidades_distintas.append(h_a+\" \"+clave)\n",
    "#                                 lista_entidades_contra.append(clave)\n",
    "#                                 lista_entidades_contra.append(h_a)\n",
    "#                                 lista_relaciones_grupos.append((attT+\" \"+clave,\"distinct_from\",h_a+\" \"+clave))\n",
    "#                                 lista_rel_G2.append((attT+\" \"+clave,\"distinct_from\",h_a+\" \"+clave))\n",
    "#                                 c_incompatibilidad+=1\n",
    "#                                 bandera=False\n",
    "#                                 break\n",
    "#                             elif cn.relacion_entailmentF(h_a,attT):\n",
    "#                                 att_found=True\n",
    "#                                 print(\"se encontró en relacion entailment\",attT)\n",
    "#                                 break\n",
    "#                         if att_found:\n",
    "#                             matches+=1\n",
    "#                 if matches==len(h_atributos):\n",
    "#                     print(\"ENTAILMENT\")\n",
    "#                     c_compatibilidad+=1\n",
    "#                     lista_entidades_contenidas.append(clave)\n",
    "#                     lista_relaciones_grupos.append((clave,\"same\",clave))\n",
    "#                     lista_rel_G1.append((clave,\"same\",clave))\n",
    "#                 # else:\n",
    "#                 #     print(\"Entidad tiene más atributos - NEUTRAL\")\n",
    "#                 #     lista_entidades_no_match.append(clave)\n",
    "#             else:\n",
    "#                 print(\"ENTAILMENT\")\n",
    "#                 c_compatibilidad+=1\n",
    "#                 lista_relaciones_grupos.append((clave,\"same\",clave))\n",
    "#                 lista_rel_G1.append((clave,\"same\",clave))\n",
    "#                 lista_entidades_contenidas.append(clave)\n",
    "#         else:\n",
    "#             bandera=True\n",
    "#             for entT in list(r_t.keys()):\n",
    "#                 ct,rct=cn.relacion_contra(clave,entT)\n",
    "#                 if(cn.relacion_noentailmentF(clave,entT)):\n",
    "#                     print(\"CONTRADICTION\")\n",
    "#                     lista_entidades_distintas.append(clave)\n",
    "#                     lista_entidades_contra.append(clave)\n",
    "#                     lista_relaciones_grupos.append((entT,\"distinct_from\",clave))\n",
    "#                     lista_rel_G2.append((entT,\"distinct_from\",clave))\n",
    "#                     c_incompatibilidad+=1\n",
    "#                     bandera=False\n",
    "#                     break\n",
    "#                 elif(ct):\n",
    "#                     print(\"CONTRADICTION_c\")\n",
    "#                     lista_entidades_distintas.append(clave)\n",
    "#                     lista_entidades_contra.append(clave)\n",
    "#                     lista_relaciones_grupos.append((entT,\"distinct_from\",clave))\n",
    "#                     lista_rel_G2.append((entT,\"distinct_from\",clave))\n",
    "#                     bandera=False\n",
    "#                     c_incompatibilidad+=1\n",
    "#                     break\n",
    "#                 elif cn.relacion_entailmentF(clave,entT):\n",
    "#                     matches=0\n",
    "#                     t_atributos = ptxt.eliminacion_espacios(r_t[entT].split(\",\"))\n",
    "#                     h_atributos = ptxt.eliminacion_espacios(r_h[clave].split(\",\"))\n",
    "#                     print(\"atributos de T\",t_atributos)\n",
    "#                     print(\"atributos de H\",h_atributos)\n",
    "#                     if \"no\" in h_atributos or \"no\" in t_atributos or \"not\" in t_atributos:\n",
    "#                         print(\"CONTRADICTION\")\n",
    "#                         lista_entidades_distintas.append(clave)\n",
    "#                         lista_relaciones_grupos.append((entT,\"distinct_from\",clave))\n",
    "#                         lista_rel_G2.append((entT,\"distinct_from\",clave))\n",
    "#                         bandera=False\n",
    "#                         c_incompatibilidad+=1\n",
    "#                         break\n",
    "#                     elif len(h_atributos)>0:\n",
    "#                         for h_a in h_atributos:\n",
    "#                             if h_a in t_atributos:\n",
    "#                                 print(\"si esta\",h_a)\n",
    "#                                 matches+=1\n",
    "#                             else:\n",
    "#                                 print(\"busqueda\",h_a)\n",
    "#                                 att_found=False\n",
    "#                                 for attT in t_atributos:\n",
    "#                                     ctat,rctat=cn.relacion_contra(h_a,attT)\n",
    "#                                     if(ctat):\n",
    "#                                         print(\"CONTRADICTION_c\")\n",
    "#                                         lista_entidades_distintas.append(clave)\n",
    "#                                         lista_entidades_distintas.append(h_a)\n",
    "#                                         lista_relaciones_grupos.append((entT,\"distinct_from\",clave))\n",
    "#                                         lista_rel_G2.append((entT,\"distinct_from\",clave))\n",
    "#                                         bandera=False\n",
    "#                                         c_incompatibilidad+=1\n",
    "#                                         break\n",
    "#                                     elif cn.relacion_entailmentF(h_a,attT):\n",
    "#                                         att_found=True\n",
    "#                                         print(\"se encontro en relacion entailment\",attT)\n",
    "#                                         break\n",
    "#                                 if att_found:\n",
    "#                                     matches+=1\n",
    "#                         if matches==len(h_atributos):\n",
    "#                             print(\"ENTAILMENT\")\n",
    "#                             lista_entidades_contenidas.append(clave)\n",
    "#                             bandera=False\n",
    "#                             c_compatibilidad+=len(h_atributos)+1\n",
    "#                             break\n",
    "#                     else:\n",
    "#                         print(\"ENTAILMENT\")\n",
    "#                         lista_entidades_contenidas.append(clave)\n",
    "#                         lista_rel_G1.append((clave,\"same\",entT))\n",
    "#                         bandera=False\n",
    "#                         c_compatibilidad+=1\n",
    "#                         break\n",
    "#             if(bandera):\n",
    "#                 print(\"no esta\",clave)\n",
    "#                 if clave not in lista_entidades_no_match:\n",
    "#                     lista_entidades_no_match.append(clave)\n",
    "#                 #break\n",
    "# print(\"----------------------------------------------\")\n",
    "# lista_entidades_no_match = list(set(lista_entidades_no_match).difference(lista_entidades_contra))\n",
    "# lista_entidades_contenidas.extend(words_found)\n",
    "# print(\"Contenidas\",lista_entidades_contenidas)\n",
    "# print(\"Faltantes\",lista_entidades_no_match)\n",
    "# print(\"Contradiccion\",lista_entidades_distintas)\n",
    "# print(\"Contradiccion2\",lista_entidades_contra)\n",
    "# print(lista_rel_G1)\n",
    "# print(lista_rel_G2)\n",
    "# print(lista_rel_G3)\n",
    "# print(lista_rel_G4)\n",
    "# print(lista_relaciones_grupos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_text=\" \".join(lemmas_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'related_to': {'abrupt',\n",
       "  'accident',\n",
       "  'accidental',\n",
       "  'accidental_damage',\n",
       "  'action',\n",
       "  'actress',\n",
       "  'actress_discovery',\n",
       "  'adhere',\n",
       "  'altogether',\n",
       "  'apart',\n",
       "  'at_work',\n",
       "  'attitude',\n",
       "  'bad',\n",
       "  'bad_bone',\n",
       "  'bad_broken',\n",
       "  'bad_sprain',\n",
       "  'bankrupt',\n",
       "  'bar',\n",
       "  'barrier',\n",
       "  'beginning',\n",
       "  'better',\n",
       "  'between',\n",
       "  'between_classes',\n",
       "  'billiards',\n",
       "  'bit',\n",
       "  'bone',\n",
       "  'bone_fracture',\n",
       "  'bone_problem',\n",
       "  'bone_tear',\n",
       "  'bone_trauma',\n",
       "  'bones',\n",
       "  'broke',\n",
       "  'broken',\n",
       "  'broken_bone',\n",
       "  'brokenness',\n",
       "  'bust',\n",
       "  'calash',\n",
       "  'car',\n",
       "  'car_pedal',\n",
       "  'car_stopper',\n",
       "  'carriage',\n",
       "  'cease',\n",
       "  'change',\n",
       "  'chocolate',\n",
       "  'cigarette',\n",
       "  'cigarette_time',\n",
       "  'class',\n",
       "  'class_intermission',\n",
       "  'classes',\n",
       "  'coffee',\n",
       "  'coffee_rest',\n",
       "  'coffee_smoke',\n",
       "  'coffee_stop',\n",
       "  'coffee_time',\n",
       "  'counter_attack',\n",
       "  'crack',\n",
       "  'crease',\n",
       "  'crush',\n",
       "  'crush_smash',\n",
       "  'damage',\n",
       "  'damaging',\n",
       "  'dance',\n",
       "  'dance_music',\n",
       "  'demote',\n",
       "  'demulsify',\n",
       "  'design',\n",
       "  'destroy',\n",
       "  'destroy_something',\n",
       "  'destroying',\n",
       "  'destroying_something',\n",
       "  'destruction',\n",
       "  'dis',\n",
       "  'dis_join',\n",
       "  'disclose',\n",
       "  'disconnect',\n",
       "  'discontinuation',\n",
       "  'discovery',\n",
       "  'dish',\n",
       "  'division',\n",
       "  'division_thing',\n",
       "  'dj',\n",
       "  'dropped',\n",
       "  'dropped_glass',\n",
       "  'dropping',\n",
       "  'dropping_glass',\n",
       "  'during',\n",
       "  'during_lunch',\n",
       "  'end',\n",
       "  'escape',\n",
       "  'fall',\n",
       "  'fall_apart',\n",
       "  'falling',\n",
       "  'falling_apart',\n",
       "  'fifteen',\n",
       "  'fifteen_minutes',\n",
       "  'first',\n",
       "  'first_meal',\n",
       "  'five',\n",
       "  'focus',\n",
       "  'fracture',\n",
       "  'fragile',\n",
       "  'fragments',\n",
       "  'from_work',\n",
       "  'function',\n",
       "  'game',\n",
       "  'gap',\n",
       "  'gap_hole',\n",
       "  'glass',\n",
       "  'glass_hit',\n",
       "  'half',\n",
       "  'hip_hop',\n",
       "  'hit',\n",
       "  'hole',\n",
       "  'holiday',\n",
       "  'hour',\n",
       "  'hurt',\n",
       "  'hurt_bone',\n",
       "  'in_two',\n",
       "  'injure',\n",
       "  'injury',\n",
       "  'instrument',\n",
       "  'intermission',\n",
       "  'interrupt',\n",
       "  'interruption',\n",
       "  'interval',\n",
       "  'join',\n",
       "  'kind',\n",
       "  'kind_dance',\n",
       "  'leg',\n",
       "  'leg_hurt',\n",
       "  'legal',\n",
       "  'like',\n",
       "  'like_smash',\n",
       "  'limbs',\n",
       "  'line',\n",
       "  'lunch',\n",
       "  'lunch_hour',\n",
       "  'lunch_time',\n",
       "  'lunchtime',\n",
       "  'make_known',\n",
       "  'man',\n",
       "  'many_pieces',\n",
       "  'meal',\n",
       "  'mend',\n",
       "  'minutes',\n",
       "  'news',\n",
       "  'occupation',\n",
       "  'off',\n",
       "  'open_up',\n",
       "  'out',\n",
       "  'overblow',\n",
       "  'painful',\n",
       "  'painful_injury',\n",
       "  'paragraph',\n",
       "  'past',\n",
       "  'pause',\n",
       "  'pedal',\n",
       "  'perception',\n",
       "  'percussion_break',\n",
       "  'period',\n",
       "  'persistent',\n",
       "  'physical',\n",
       "  'piece',\n",
       "  'pieces',\n",
       "  'pitch',\n",
       "  'player',\n",
       "  'point',\n",
       "  'powerful',\n",
       "  'present',\n",
       "  'present_broke',\n",
       "  'problem',\n",
       "  'properly',\n",
       "  'pull',\n",
       "  'pull_apart',\n",
       "  'purpose',\n",
       "  'rank',\n",
       "  'reassembly',\n",
       "  'receive',\n",
       "  'receiver',\n",
       "  'recess',\n",
       "  'record',\n",
       "  'relax',\n",
       "  'relaxing',\n",
       "  'remove',\n",
       "  'respite',\n",
       "  'rest',\n",
       "  'rest_interval',\n",
       "  'rest_period',\n",
       "  'rest_time',\n",
       "  'restful',\n",
       "  'restful_purpose',\n",
       "  'resting',\n",
       "  'rhythmic',\n",
       "  'ruin',\n",
       "  'ruining',\n",
       "  'ruining_something',\n",
       "  'scale',\n",
       "  'school',\n",
       "  'separate',\n",
       "  'separate_violently',\n",
       "  'separation',\n",
       "  'shallow',\n",
       "  'shards',\n",
       "  'shatter',\n",
       "  'shattered',\n",
       "  'shattering',\n",
       "  'shattering_glass',\n",
       "  'shatters',\n",
       "  'short',\n",
       "  'short_pause',\n",
       "  'short_rest',\n",
       "  'shot',\n",
       "  'siesta',\n",
       "  'smash',\n",
       "  'smash_dish',\n",
       "  'smash_something',\n",
       "  'smash_up',\n",
       "  'smash_wreck',\n",
       "  'smashing',\n",
       "  'smashing_something',\n",
       "  'smoke',\n",
       "  'snaffle',\n",
       "  'snap',\n",
       "  'snap_apart',\n",
       "  'snap_crush',\n",
       "  'snap_destroy',\n",
       "  'snap_fracture',\n",
       "  'snapped',\n",
       "  'snapped_bone',\n",
       "  'space',\n",
       "  'spill',\n",
       "  'spirit',\n",
       "  'split',\n",
       "  'splitting',\n",
       "  'splitting_half',\n",
       "  'sprain',\n",
       "  'spring',\n",
       "  'spring_summer',\n",
       "  'spring_vacation',\n",
       "  'state',\n",
       "  'steady',\n",
       "  'stop',\n",
       "  'stop_working',\n",
       "  'stoppage',\n",
       "  'stopper',\n",
       "  'stopping',\n",
       "  'stopping_work',\n",
       "  'summer',\n",
       "  'surf',\n",
       "  'take',\n",
       "  'take_apart',\n",
       "  'take_five',\n",
       "  'take_rest',\n",
       "  'taking',\n",
       "  'taking_rest',\n",
       "  'tear',\n",
       "  'tear_apart',\n",
       "  'temporarily',\n",
       "  'temporary',\n",
       "  'temporary_vacation',\n",
       "  'ten',\n",
       "  'ten_minutes',\n",
       "  'time',\n",
       "  'time_off',\n",
       "  'time_out',\n",
       "  'tiny',\n",
       "  'tiny_vacation',\n",
       "  'trauma',\n",
       "  'two',\n",
       "  'two_pieces',\n",
       "  'unmaking',\n",
       "  'up',\n",
       "  'vacation',\n",
       "  'violate',\n",
       "  'violently',\n",
       "  'wait',\n",
       "  'wave',\n",
       "  'weather',\n",
       "  'white_water',\n",
       "  'will',\n",
       "  'woodwind',\n",
       "  'work',\n",
       "  'work_pause',\n",
       "  'work_relax',\n",
       "  'work_rest',\n",
       "  'work_stop',\n",
       "  'work_stoppage',\n",
       "  'work_vacation',\n",
       "  'working',\n",
       "  'working_pause',\n",
       "  'wreck'},\n",
       " 'form_of': set(),\n",
       " 'is_a': {'accident',\n",
       "  'change',\n",
       "  'dash',\n",
       "  'delay',\n",
       "  'escape',\n",
       "  'score',\n",
       "  'separation',\n",
       "  'stroke'},\n",
       " 'part_of': {'billiards', 'pool'},\n",
       " 'has_a': set(),\n",
       " 'used_for': set(),\n",
       " 'capable_of': set(),\n",
       " 'at_location': {'breadbox'},\n",
       " 'entails': set(),\n",
       " 'causes': {'break'},\n",
       " 'has_subevent': set(),\n",
       " 'has_first_subevent': set(),\n",
       " 'has_last_subevent': set(),\n",
       " 'has_prerequisite': set(),\n",
       " 'has_property': set(),\n",
       " 'motivated_by_goal': set(),\n",
       " 'desires': set(),\n",
       " 'synonym': {'bankrupt',\n",
       "  'better',\n",
       "  'breach',\n",
       "  'break',\n",
       "  'break_dance',\n",
       "  'break_down',\n",
       "  'break_in',\n",
       "  'breakage',\n",
       "  'burst',\n",
       "  'collapse',\n",
       "  'contravene',\n",
       "  'crack',\n",
       "  'dampen',\n",
       "  'demote',\n",
       "  'disclose',\n",
       "  'fail',\n",
       "  'fault',\n",
       "  'fracture',\n",
       "  'interrupt',\n",
       "  'interruption',\n",
       "  'open_frame',\n",
       "  'pause',\n",
       "  'recess',\n",
       "  'respite',\n",
       "  'rupture',\n",
       "  'separate',\n",
       "  'split',\n",
       "  'time_out',\n",
       "  'transgress',\n",
       "  'violate'},\n",
       " 'antonym': {'accelerate',\n",
       "  'assemble',\n",
       "  'build',\n",
       "  'construct',\n",
       "  'fix',\n",
       "  'fix_it',\n",
       "  'glue',\n",
       "  'glue_together',\n",
       "  'heal',\n",
       "  'heal_mend',\n",
       "  'hold',\n",
       "  'join',\n",
       "  'keep',\n",
       "  'keep_whole',\n",
       "  'mend',\n",
       "  'piece',\n",
       "  'piece_together',\n",
       "  'put',\n",
       "  'put_together',\n",
       "  'putting',\n",
       "  'putting_together',\n",
       "  'repair',\n",
       "  'repair_fix',\n",
       "  'together',\n",
       "  'whole',\n",
       "  'work',\n",
       "  'working'},\n",
       " 'distinct_from': {'accelerate', 'connected', 'fix', 'fixed', 'working'},\n",
       " 'derived_from': set(),\n",
       " 'defined_as': set(),\n",
       " 'manner_of': {'appear',\n",
       "  'become',\n",
       "  'change',\n",
       "  'change_integrity',\n",
       "  'change_state',\n",
       "  'collapse',\n",
       "  'come_forth',\n",
       "  'damage',\n",
       "  'decay',\n",
       "  'decrease',\n",
       "  'destroy',\n",
       "  'detach',\n",
       "  'diphthongize',\n",
       "  'discontinue',\n",
       "  'disperse',\n",
       "  'end',\n",
       "  'escape',\n",
       "  'express_emotion',\n",
       "  'flee',\n",
       "  'happen',\n",
       "  'interrupt',\n",
       "  'invalidate',\n",
       "  'penetrate',\n",
       "  'ruin',\n",
       "  'separate',\n",
       "  'shoot',\n",
       "  'solve',\n",
       "  'switch',\n",
       "  'weaken'},\n",
       " 'located_near': set(),\n",
       " 'has_context': {'backgammon',\n",
       "  'billiards',\n",
       "  'british',\n",
       "  'computing',\n",
       "  'equitation',\n",
       "  'games',\n",
       "  'gaming',\n",
       "  'military',\n",
       "  'most_often_in',\n",
       "  'music',\n",
       "  'pool',\n",
       "  'slang',\n",
       "  'snooker',\n",
       "  'soccer',\n",
       "  'sports',\n",
       "  'storm',\n",
       "  'surfing',\n",
       "  'tennis',\n",
       "  'uk',\n",
       "  'weather'},\n",
       " 'similar_to': {'bork', 'breaking', 'broke', 'broken'},\n",
       " 'etymologically_related_to': {'fraction',\n",
       "  'fracture',\n",
       "  'fragile',\n",
       "  'fragment',\n",
       "  'frail'},\n",
       " 'causes_desire': set(),\n",
       " 'made_of': set(),\n",
       " 'receives_action': set(),\n",
       " 'created_by': set()}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cn.df_diccionario_generales[\"break\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'good_luck'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mcn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdf_diccionario_especificas\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mgood_luck\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'good_luck'"
     ]
    }
   ],
   "source": [
    "cn.df_diccionario_especificas[\"good_luck\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fortune\n"
     ]
    }
   ],
   "source": [
    "cs = cn.df_diccionario_generales[\"luck\"][\"synonym\"]\n",
    "for sinonimos in cs:\n",
    "    nsin=str(sinonimos).replace(\"_\",\" \")\n",
    "    print(nsin)\n",
    "    if(nsin in new_text):\n",
    "        print(new_text.replace(nsin,sinonimos))\n",
    "        print(\"wmd\",\"synonym\",nsin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Break a leg.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "related_to                   {inexperienced, spanking, know, fresh, modern,...\n",
       "form_of                                                                     {}\n",
       "is_a                                                               {adjective}\n",
       "part_of                                                                     {}\n",
       "has_a                                                                       {}\n",
       "used_for                                                                    {}\n",
       "capable_of                                                                  {}\n",
       "at_location                                                                 {}\n",
       "entails                                                                     {}\n",
       "causes                                                                      {}\n",
       "has_subevent                                                                {}\n",
       "has_first_subevent                                                          {}\n",
       "has_last_subevent                                                           {}\n",
       "has_prerequisite                                                            {}\n",
       "has_property                                                                {}\n",
       "motivated_by_goal                                                           {}\n",
       "desires                                                                     {}\n",
       "synonym                      {newborn, current, freshly, young, recent, mod...\n",
       "antonym                      {accustomed, familiar, old, original, former, ...\n",
       "distinct_from                                {antiquarian, used, ancient, old}\n",
       "derived_from                                                                {}\n",
       "defined_as                                                   {opposite_of_old}\n",
       "manner_of                                                                   {}\n",
       "located_near                                                                {}\n",
       "has_context                               {australia, vegetables, linguistics}\n",
       "similar_to                   {inexperienced, new_sprung, refreshing, sunris...\n",
       "etymologically_related_to                                                {now}\n",
       "causes_desire                                                               {}\n",
       "made_of                                                                     {}\n",
       "receives_action                                                             {}\n",
       "created_by                                                                  {}\n",
       "Name: new, dtype: object"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Generales[\"new\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# proceso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ut.load_vectors_in_lang(nlp,\"../OPENAI/data/glove.840B.300d.txt\") # carga de vectores en nlp.wv\n",
    "ut.load_vectors_in_lang(nlp,\"data/numberbatch-en-17.04b.txt\") # carga de vectores en nlp.wv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'cn' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mcn\u001b[49m\u001b[38;5;241m.\u001b[39mdf_diccionario_generales[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtug-of-war\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrelated_to\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cn' is not defined"
     ]
    }
   ],
   "source": [
    "cn.df_diccionario_generales[\"tug-of-war\"]['related_to']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'synonym', 'is_a', 'at_location', 'receives_action', 'capable_of', 'has_context', 'related_to', 'has_prerequisite', 'derived_from', 'part_of', 'created_by', 'form_of'}\n"
     ]
    }
   ],
   "source": [
    "cn.relaciones_word(\"director\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "derecha\n",
      "film - director | created_by /a/[/r/created_by/,/c/en/film/,/c/en/director/]\n",
      "derecha\n",
      "codirector - director | derived_from /a/[/r/derived_from/,/c/en/codirector/,/c/en/director/]\n",
      "derecha\n",
      "directorial - director | derived_from /a/[/r/derived_from/,/c/en/directorial/,/c/en/director/]\n",
      "derecha\n",
      "directorish - director | derived_from /a/[/r/derived_from/,/c/en/directorish/,/c/en/director/]\n",
      "derecha\n",
      "directoritis - director | derived_from /a/[/r/derived_from/,/c/en/directoritis/,/c/en/director/]\n",
      "derecha\n",
      "directorless - director | derived_from /a/[/r/derived_from/,/c/en/directorless/,/c/en/director/]\n",
      "derecha\n",
      "directorly - director | derived_from /a/[/r/derived_from/,/c/en/directorly/,/c/en/director/]\n",
      "derecha\n",
      "directorship - director | derived_from /a/[/r/derived_from/,/c/en/directorship/,/c/en/director/]\n",
      "derecha\n",
      "directress - director | derived_from /a/[/r/derived_from/,/c/en/directress/,/c/en/director/]\n",
      "derecha\n",
      "multidirector - director | derived_from /a/[/r/derived_from/,/c/en/multidirector/,/c/en/director/]\n",
      "derecha\n",
      "nondirector - director | derived_from /a/[/r/derived_from/,/c/en/nondirector/,/c/en/director/]\n",
      "derecha\n",
      "superdirector - director | derived_from /a/[/r/derived_from/,/c/en/superdirector/,/c/en/director/]\n",
      "derecha\n",
      "directors - director | form_of /a/[/r/form_of/,/c/en/directors/n/,/c/en/director/]\n",
      "derecha\n",
      "tape_television_show - director | has_prerequisite /a/[/r/has_prerequisite/,/c/en/tape_television_show/,/c/en/director/]\n",
      "derecha\n",
      "ad - director | related_to /a/[/r/related_to/,/c/en/ad/n/,/c/en/director/]\n",
      "derecha\n",
      "articles_of_association - director | related_to /a/[/r/related_to/,/c/en/articles_of_association/n/,/c/en/director/]\n",
      "derecha\n",
      "astronomer_royal - director | related_to /a/[/r/related_to/,/c/en/astronomer_royal/n/,/c/en/director/]\n",
      "derecha\n",
      "auteur - director | related_to /a/[/r/related_to/,/c/en/auteur/n/,/c/en/director/]\n",
      "derecha\n",
      "auteurism - director | related_to /a/[/r/related_to/,/c/en/auteurism/n/,/c/en/director/]\n",
      "derecha\n",
      "brains - director | related_to /a/[/r/related_to/,/c/en/brains/n/,/c/en/director/]\n",
      "derecha\n",
      "captain - director | related_to /a/[/r/related_to/,/c/en/captain/,/c/en/director/]\n",
      "derecha\n",
      "chapelmaster - director | related_to /a/[/r/related_to/,/c/en/chapelmaster/n/,/c/en/director/]\n",
      "derecha\n",
      "choirmaster - director | related_to /a/[/r/related_to/,/c/en/choirmaster/n/,/c/en/director/]\n",
      "derecha\n",
      "chorister - director | related_to /a/[/r/related_to/,/c/en/chorister/n/,/c/en/director/]\n",
      "derecha\n",
      "company_seal - director | related_to /a/[/r/related_to/,/c/en/company_seal/n/,/c/en/director/]\n",
      "derecha\n",
      "conductor - director | related_to /a/[/r/related_to/,/c/en/conductor/n/,/c/en/director/]\n",
      "derecha\n",
      "cut - director | related_to /a/[/r/related_to/,/c/en/cut/,/c/en/director/]\n",
      "derecha\n",
      "direct - director | related_to /a/[/r/related_to/,/c/en/direct/v/,/c/en/director/]\n",
      "derecha\n",
      "direction - director | related_to /a/[/r/related_to/,/c/en/direction/n/,/c/en/director/]\n",
      "derecha\n",
      "director's_cut - director | related_to /a/[/r/related_to/,/c/en/director's_cut/n/,/c/en/director/]\n",
      "derecha\n",
      "director's_loan - director | related_to /a/[/r/related_to/,/c/en/director's_loan/n/,/c/en/director/]\n",
      "derecha\n",
      "directorate - director | related_to /a/[/r/related_to/,/c/en/directorate/n/,/c/en/director/]\n",
      "derecha\n",
      "directorial - director | related_to /a/[/r/related_to/,/c/en/directorial/a/,/c/en/director/]\n",
      "derecha\n",
      "directorish - director | related_to /a/[/r/related_to/,/c/en/directorish/a/,/c/en/director/]\n",
      "derecha\n",
      "directorless - director | related_to /a/[/r/related_to/,/c/en/directorless/a/,/c/en/director/]\n",
      "derecha\n",
      "directorly - director | related_to /a/[/r/related_to/,/c/en/directorly/a/,/c/en/director/]\n",
      "derecha\n",
      "directors - director | related_to /a/[/r/related_to/,/c/en/directors/n/,/c/en/director/]\n",
      "derecha\n",
      "directorship - director | related_to /a/[/r/related_to/,/c/en/directorship/n/,/c/en/director/]\n",
      "derecha\n",
      "directress - director | related_to /a/[/r/related_to/,/c/en/directress/n/,/c/en/director/]\n",
      "derecha\n",
      "dirigent - director | related_to /a/[/r/related_to/,/c/en/dirigent/n/,/c/en/director/]\n",
      "derecha\n",
      "durassian - director | related_to /a/[/r/related_to/,/c/en/durassian/a/,/c/en/director/]\n",
      "derecha\n",
      "filmmaker - director | related_to /a/[/r/related_to/,/c/en/filmmaker/n/,/c/en/director/]\n",
      "derecha\n",
      "filmography - director | related_to /a/[/r/related_to/,/c/en/filmography/n/,/c/en/director/]\n",
      "derecha\n",
      "gilliamesque - director | related_to /a/[/r/related_to/,/c/en/gilliamesque/a/,/c/en/director/]\n",
      "derecha\n",
      "guide - director | related_to /a/[/r/related_to/,/c/en/guide/n/wikt/en_1/,/c/en/director/]\n",
      "derecha\n",
      "guinea_pig_director - director | related_to /a/[/r/related_to/,/c/en/guinea_pig_director/n/,/c/en/director/]\n",
      "derecha\n",
      "helmer - director | related_to /a/[/r/related_to/,/c/en/helmer/n/,/c/en/director/]\n",
      "derecha\n",
      "homeotropic - director | related_to /a/[/r/related_to/,/c/en/homeotropic/a/,/c/en/director/]\n",
      "derecha\n",
      "ibsenesque - director | related_to /a/[/r/related_to/,/c/en/ibsenesque/a/,/c/en/director/]\n",
      "derecha\n",
      "ibsenian - director | related_to /a/[/r/related_to/,/c/en/ibsenian/a/,/c/en/director/]\n",
      "derecha\n",
      "mandator - director | related_to /a/[/r/related_to/,/c/en/mandator/n/,/c/en/director/]\n",
      "derecha\n",
      "monodomain - director | related_to /a/[/r/related_to/,/c/en/monodomain/n/,/c/en/director/]\n",
      "derecha\n",
      "movie_deal - director | related_to /a/[/r/related_to/,/c/en/movie_deal/n/,/c/en/director/]\n",
      "derecha\n",
      "multidirector - director | related_to /a/[/r/related_to/,/c/en/multidirector/a/,/c/en/director/]\n",
      "derecha\n",
      "nondirector - director | related_to /a/[/r/related_to/,/c/en/nondirector/n/,/c/en/director/]\n",
      "derecha\n",
      "ordinator - director | related_to /a/[/r/related_to/,/c/en/ordinator/n/,/c/en/director/]\n",
      "derecha\n",
      "play - director | related_to /a/[/r/related_to/,/c/en/play/,/c/en/director/]\n",
      "derecha\n",
      "pretilt - director | related_to /a/[/r/related_to/,/c/en/pretilt/n/,/c/en/director/]\n",
      "derecha\n",
      "residual - director | related_to /a/[/r/related_to/,/c/en/residual/n/,/c/en/director/]\n",
      "derecha\n",
      "revlon_mode - director | related_to /a/[/r/related_to/,/c/en/revlon_mode/n/,/c/en/director/]\n",
      "derecha\n",
      "rohmerian - director | related_to /a/[/r/related_to/,/c/en/rohmerian/a/,/c/en/director/]\n",
      "derecha\n",
      "schlockmeister - director | related_to /a/[/r/related_to/,/c/en/schlockmeister/n/,/c/en/director/]\n",
      "derecha\n",
      "showreel - director | related_to /a/[/r/related_to/,/c/en/showreel/n/,/c/en/director/]\n",
      "derecha\n",
      "skipper - director | related_to /a/[/r/related_to/,/c/en/skipper/n/wikt/en_1/,/c/en/director/]\n",
      "derecha\n",
      "stanislavskian - director | related_to /a/[/r/related_to/,/c/en/stanislavskian/a/,/c/en/director/]\n",
      "derecha\n",
      "sterner - director | related_to /a/[/r/related_to/,/c/en/sterner/n/wikt/en_2/,/c/en/director/]\n",
      "derecha\n",
      "subdirector - director | related_to /a/[/r/related_to/,/c/en/subdirector/n/,/c/en/director/]\n",
      "derecha\n",
      "superdirector - director | related_to /a/[/r/related_to/,/c/en/superdirector/n/,/c/en/director/]\n",
      "derecha\n",
      "talkback - director | related_to /a/[/r/related_to/,/c/en/talkback/n/,/c/en/director/]\n",
      "derecha\n",
      "tantième - director | related_to /a/[/r/related_to/,/c/en/tantième/n/,/c/en/director/]\n",
      "derecha\n",
      "theatremaker - director | related_to /a/[/r/related_to/,/c/en/theatremaker/n/,/c/en/director/]\n",
      "derecha\n",
      "tone_meeting - director | related_to /a/[/r/related_to/,/c/en/tone_meeting/n/,/c/en/director/]\n",
      "derecha\n",
      "whedonite - director | related_to /a/[/r/related_to/,/c/en/whedonite/n/,/c/en/director/]\n",
      "derecha\n",
      "zappaesque - director | related_to /a/[/r/related_to/,/c/en/zappaesque/a/,/c/en/director/]\n",
      "derecha\n",
      "board_member - director | synonym /a/[/r/synonym/,/c/en/board_member/n/,/c/en/director/]\n",
      "derecha\n",
      "director - director | synonym /a/[/r/synonym/,/c/en/director/,/c/en/director/]\n",
      "derecha\n",
      "head - director | synonym /a/[/r/synonym/,/c/en/head/n/,/c/en/director/]\n",
      "derecha\n",
      "director_circle - director | derived_from /a/[/r/derived_from/,/c/en/director_circle/,/c/en/director/n/]\n",
      "derecha\n",
      "director_conic - director | derived_from /a/[/r/derived_from/,/c/en/director_conic/,/c/en/director/n/]\n",
      "derecha\n",
      "chairman_of_board - director | is_a /a/[/r/is_a/,/c/en/chairman_of_board/n/,/c/en/director/n/]\n",
      "derecha\n",
      "bank_manager - director | is_a /a/[/r/is_a/,/c/en/bank_manager/n/wn/person/,/c/en/director/n/wn/person/]\n",
      "derecha\n",
      "district_manager - director | is_a /a/[/r/is_a/,/c/en/district_manager/n/wn/person/,/c/en/director/n/wn/person/]\n",
      "derecha\n",
      "manageress - director | is_a /a/[/r/is_a/,/c/en/manageress/n/wn/person/,/c/en/director/n/wn/person/]\n",
      "derecha\n",
      "stage_director - director | is_a /a/[/r/is_a/,/c/en/stage_director/n/wn/person/,/c/en/director/n/wn/person/]\n",
      "derecha\n",
      "manager - director | synonym /a/[/r/synonym/,/c/en/manager/n/wn/person/,/c/en/director/n/wn/person/]\n",
      "derecha\n",
      "managing_director - director | synonym /a/[/r/synonym/,/c/en/managing_director/n/wn/person/,/c/en/director/n/wn/person/]\n",
      "derecha\n",
      "theater_director - director | synonym /a/[/r/synonym/,/c/en/theater_director/n/wn/person/,/c/en/director/n/wn/person/]\n",
      "derecha\n",
      "theatre_director - director | synonym /a/[/r/synonym/,/c/en/theatre_director/n/wn/person/,/c/en/director/n/wn/person/]\n",
      "izquierda\n",
      "director - film_set | at_location /a/[/r/at_location/,/c/en/director/,/c/en/film_set/]\n",
      "izquierda\n",
      "director - chair_meeting | capable_of /a/[/r/capable_of/,/c/en/director/,/c/en/chair_meeting/]\n",
      "izquierda\n",
      "director - cut_scene | capable_of /a/[/r/capable_of/,/c/en/director/,/c/en/cut_scene/]\n",
      "izquierda\n",
      "director - direct_film | capable_of /a/[/r/capable_of/,/c/en/director/,/c/en/direct_film/]\n",
      "izquierda\n",
      "director - direct_movie | capable_of /a/[/r/capable_of/,/c/en/director/,/c/en/direct_movie/]\n",
      "izquierda\n",
      "director - direct_with_such_skill | capable_of /a/[/r/capable_of/,/c/en/director/,/c/en/direct_with_such_skill/]\n",
      "izquierda\n",
      "director - make_movie | capable_of /a/[/r/capable_of/,/c/en/director/,/c/en/make_movie/]\n",
      "izquierda\n",
      "director - pay_actors | capable_of /a/[/r/capable_of/,/c/en/director/,/c/en/pay_actors/]\n",
      "izquierda\n",
      "director - produce_film | capable_of /a/[/r/capable_of/,/c/en/director/,/c/en/produce_film/]\n",
      "izquierda\n",
      "director - showing_screening | capable_of /a/[/r/capable_of/,/c/en/director/,/c/en/showing_screening/]\n",
      "izquierda\n",
      "director - in_charge_of | is_a /a/[/r/is_a/,/c/en/director/,/c/en/in_charge_of/]\n",
      "izquierda\n",
      "director - in_charge_of_group | is_a /a/[/r/is_a/,/c/en/director/,/c/en/in_charge_of_group/]\n",
      "izquierda\n",
      "director - person | is_a /a/[/r/is_a/,/c/en/director/,/c/en/person/n/]\n",
      "izquierda\n",
      "director - person_who_directs_movie | is_a /a/[/r/is_a/,/c/en/director/,/c/en/person_who_directs_movie/]\n",
      "izquierda\n",
      "director - movie_crew | part_of /a/[/r/part_of/,/c/en/director/,/c/en/movie_crew/]\n",
      "izquierda\n",
      "director - found_in_directors_chair | receives_action /a/[/r/receives_action/,/c/en/director/,/c/en/found_in_directors_chair/]\n",
      "izquierda\n",
      "director - chemistry | has_context /a/[/r/has_context/,/c/en/director/n/,/c/en/chemistry/]\n",
      "izquierda\n",
      "director - military | has_context /a/[/r/has_context/,/c/en/director/n/,/c/en/military/]\n",
      "izquierda\n",
      "director - corporate_leader | is_a /a/[/r/is_a/,/c/en/director/n/,/c/en/corporate_leader/n/]\n",
      "izquierda\n",
      "director - executive_director | is_a /a/[/r/is_a/,/c/en/director/n/,/c/en/executive_director/n/]\n",
      "izquierda\n",
      "director - member_of_organisation | is_a /a/[/r/is_a/,/c/en/director/n/,/c/en/member_of_organisation/n/]\n",
      "izquierda\n",
      "director - organization_member | is_a /a/[/r/is_a/,/c/en/director/n/,/c/en/organization_member/n/]\n",
      "izquierda\n",
      "director - person | is_a /a/[/r/is_a/,/c/en/director/n/,/c/en/person/n/]\n",
      "izquierda\n",
      "director - axis | related_to /a/[/r/related_to/,/c/en/director/n/,/c/en/axis/]\n",
      "izquierda\n",
      "director - device | related_to /a/[/r/related_to/,/c/en/director/n/,/c/en/device/]\n",
      "izquierda\n",
      "director - direct | related_to /a/[/r/related_to/,/c/en/director/n/,/c/en/direct/]\n",
      "izquierda\n",
      "director - direction | related_to /a/[/r/related_to/,/c/en/director/n/,/c/en/direction/]\n",
      "izquierda\n",
      "director - graphical | related_to /a/[/r/related_to/,/c/en/director/n/,/c/en/graphical/]\n",
      "izquierda\n",
      "director - indirect | related_to /a/[/r/related_to/,/c/en/director/n/,/c/en/indirect/]\n",
      "izquierda\n",
      "director - information | related_to /a/[/r/related_to/,/c/en/director/n/,/c/en/information/]\n",
      "izquierda\n",
      "director - liquid_crystal | related_to /a/[/r/related_to/,/c/en/director/n/,/c/en/liquid_crystal/]\n",
      "izquierda\n",
      "director - orientate | related_to /a/[/r/related_to/,/c/en/director/n/,/c/en/orientate/]\n",
      "izquierda\n",
      "director - real_time | related_to /a/[/r/related_to/,/c/en/director/n/,/c/en/real_time/]\n",
      "izquierda\n",
      "director - symmetry | related_to /a/[/r/related_to/,/c/en/director/n/,/c/en/symmetry/]\n",
      "izquierda\n",
      "director - target | related_to /a/[/r/related_to/,/c/en/director/n/,/c/en/target/]\n",
      "izquierda\n",
      "director - weapon | related_to /a/[/r/related_to/,/c/en/director/n/,/c/en/weapon/]\n",
      "izquierda\n",
      "director - administrator | is_a /a/[/r/is_a/,/c/en/director/n/wn/person/,/c/en/administrator/n/wn/person/]\n",
      "izquierda\n",
      "director - committee_member | is_a /a/[/r/is_a/,/c/en/director/n/wn/person/,/c/en/committee_member/n/wn/person/]\n",
      "izquierda\n",
      "director - supervisor | is_a /a/[/r/is_a/,/c/en/director/n/wn/person/,/c/en/supervisor/n/wn/person/]\n",
      "izquierda\n",
      "director - conductor | synonym /a/[/r/synonym/,/c/en/director/n/wn/person/,/c/en/conductor/n/wn/person/]\n",
      "izquierda\n",
      "director - film_director | synonym /a/[/r/synonym/,/c/en/director/n/wn/person/,/c/en/film_director/n/wn/person/]\n"
     ]
    }
   ],
   "source": [
    "cn.relacion_word(\"director\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prueba\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "prueba=pd.read_csv(\"../OPENAI/data/MultiNLI/BDIAG/AX-b.csv\")\n",
    "#prueba=pd.read_csv(\"../OPENAI/data/\"+sys.argv[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "textos = prueba[\"sentence1\"].to_list()       # almacenamiento en listas\n",
    "hipotesis = prueba[\"sentence2\"].to_list()\n",
    "clases = prueba[\"gold_label\"].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_data = {'dicEntT':[],'dicEntH':[],'Jaro-Winkler_rit':[],'Jaro-Winkler_rit1':[],'Jaro-Winkler_ritN':[],'Jaro-Winkler_ritN1':[],\n",
    "            'Jaro-Winkler_ritN_':[],'Jaro-Winkler_ritN1_':[],'Jaro-Winkler_ritN__':[],'Jaro-Winkler_ritN1__':[],\n",
    "            'simBoWrel':[],'negT' : [],  'negH' : [],'jaccard':[],'jaccard_rel':[],'entSimilitud':[],'relation':[], 'overlap_ent':[],\n",
    "            'no_matcheadas':[], 'contradiction':[],'entail':[],'contra':[],'neutral':[],'no_match':[],'entropia_relaciones':[],'entropia_total' : [],\n",
    "            'mearts_t':[],'mearts':[],'distancias':[],\n",
    "            'origE':[],'genE':[],'oricE':[],'conE':[],'origM':[],'genM':[],'oricM':[],'conM':[],'orieM':[],\n",
    "            'espM':[],'rest':[],\n",
    "            'entropias' : [],'KL_divergence':[],'max_info_t' : [],'max_info' : [],'sumas_t' : [],'sumas' : [],'mutinf_t' : [],'mutinf' : [], \n",
    "            'diferencias' :[],'semantics':[], 'nlp_semantics':[],'list_comp' : [],  'list_incomp':[],'rel_conceptuales':[],'Jaro-Winkler_rit':[],'Jaro-Winkler_rit1':[],\n",
    "            'ConteosR':[],'ConteosG1':[],'ConteosG2':[],'ConteosG3':[],'ConteosG4':[],\n",
    "            'Texto':[],'Hipotesis':[],'TextoL':[],'HipotesisL':[],\n",
    "            'clases' : []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "The cat sat on the mat.\n",
      "{'cat': '', 'sit': 'on', 'mat': ''} ['cat', 'sit', 'mat']\n",
      "The cat did not sit on the mat.\n",
      "{'cat': '', 'sit': 'do,not,,on,', 'mat': ''} ['cat', 'sit', 'mat']\n",
      "clase not_entailment\n",
      "valores de diccionario ['', 'on', '']\n",
      "valores de diccionario ['', 'do,not,,on,', '']\n",
      "[{'be_seated', 'sit_down', 'baby_sit', 'ride', 'seat', 'model', 'accept', 'be'}, {'computerized_tomography', 'panther', 'catamaran', 'guy', 'big_cat', 'cat_o_nine_tails', 'cat', 'feliform', 'saber_toothed_cat', 'pantherine_cat', 'catfish', 'kat', 'feline_cat', 'vomit', 'ct'}, {'flatness', 'master_of_arts_in_teaching', 'mat', 'matinee', 'felt', 'matt', 'raft_foundation', 'entangle'}, {'about', 'by_dint_of', 'base', 'along', 'ontario'}]\n",
      "[{'bender', 'ˈ', 'non', 'n’t'}, {'doctor_of_osteopathy', 'achieve', 'translate', 'execute', 'get_together', 'ut', 'cozen', 'effect', 'disturbance', 'finish', 'medical_doctor', 'swindle', 'do', 'dress', 'serve', 'suffice', 'terminate', 'chouse', 'bash', 'settle', 'practice', 'cause', 'at', 'complete', 'observe', 'perform', 'act', 'make', 'prepare', 'conduct', 'bustle', 'conclude', 'to', 'accomplish', 'transact', 'cook'}, {'be_seated', 'sit_down', 'baby_sit', 'ride', 'seat', 'model', 'accept', 'be'}, {'computerized_tomography', 'panther', 'catamaran', 'guy', 'big_cat', 'cat_o_nine_tails', 'cat', 'feliform', 'saber_toothed_cat', 'pantherine_cat', 'catfish', 'kat', 'feline_cat', 'vomit', 'ct'}, {'flatness', 'master_of_arts_in_teaching', 'mat', 'matinee', 'felt', 'matt', 'raft_foundation', 'entangle'}, {'about', 'by_dint_of', 'base', 'along', 'ontario'}]\n",
      "{'cat': '', 'sit': 'on', 'mat': ''}\n",
      "{'cat': '', 'sit': 'do,not,,on,', 'mat': ''}\n",
      "-----------\n",
      "['sit', 'cat', 'mat', 'on']\n",
      "['not', 'do', 'sit', 'cat', 'mat', 'on']\n",
      "{'cat': '', 'sit': 'on', 'mat': ''}\n",
      "{'cat': '', 'sit': 'do,not,,on,', 'mat': ''}\n",
      "['sit', 'cat', 'mat', 'on']\n",
      "['not', 'do', 'sit', 'cat', 'mat', 'on']\n",
      "Revision de reemplazo cat cat {'computerized_tomography', 'panther', 'catamaran', 'guy', 'big_cat', 'cat_o_nine_tails', 'cat', 'feliform', 'saber_toothed_cat', 'pantherine_cat', 'catfish', 'kat', 'feline_cat', 'vomit', 'ct'}\n",
      "Revision de reemplazo mat mat {'flatness', 'master_of_arts_in_teaching', 'mat', 'matinee', 'felt', 'matt', 'raft_foundation', 'entangle'}\n",
      "[0, 1, 1, 0, 1, 1]\n",
      "[1, 1, 1, 1, 1, 1]\n",
      "BoW:  0.816496580927726\n",
      "hiponimos do sit\n",
      "hiperonimos sobre sinonimos sit cat\n",
      "mat mat sinonimos\n",
      "on on sinonimos\n",
      "[1, 1, 1, 1]\n",
      "[0, 1, 1, 0, 1, 1]\n",
      "4\n",
      "jaro con antonimos\n",
      "[0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0]\n",
      "0\n",
      "0.0 0.0\n",
      "jaro con relacionadas\n",
      "[0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0]\n",
      "0\n",
      "0.0 0.0\n",
      "hiponimos do sit\n",
      "hiperonimos sobre sinonimos sit cat\n",
      "mat mat sinonimos\n",
      "on on sinonimos\n",
      "[1, 1, 1, 1]\n",
      "[0, 1, 1, 0, 1, 1]\n",
      "4\n",
      "0.6666666666666666 0.8055555555555555\n",
      "{'cat': '', 'sit': 'on', 'mat': ''} ['cat', 'sit', 'mat']\n",
      "{'cat': '', 'sit': 'do,not,,on,', 'mat': ''} ['cat', 'sit', 'mat']\n",
      "negT []\n",
      "negH [('sit', 'not')]\n",
      "1.0\n",
      "s1 ['sit', 'cat', 'mat', 'on']\n",
      "s2 ['not', 'do', 'sit', 'cat', 'mat', 'on']\n",
      "0.6666666666666666\n",
      "t_lem ['sit', 'cat', 'mat', 'on']\n",
      "h_lem ['not', 'do', 'sit', 'cat', 'mat', 'on']\n",
      "      not    do   sit   cat   mat    on\n",
      "sit  0.05  0.24  1.00  0.03  0.13  0.22\n",
      "cat  0.02  0.02  0.03  1.00  0.14  0.01\n",
      "mat  0.05  0.00  0.13  0.14  1.00  0.06\n",
      "on   0.11  0.12  0.22  0.01  0.06  1.00\n",
      "rel nueva categoria [1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Valores para entropia {0, 1}\n",
      "Probabilidades [0.8333333333333334, 0.16666666666666666]\n",
      "si esta cat\n",
      "atributos de T []\n",
      "atributos de H []\n",
      "ENTAILMENT\n",
      "si esta sit\n",
      "atributos de T ['on']\n",
      "atributos de H ['do', 'not', 'on']\n",
      "CONTRADICTION\n",
      "si esta mat\n",
      "atributos de T []\n",
      "atributos de H []\n",
      "ENTAILMENT\n",
      "----------------------------------------------\n",
      "Contenidas ['cat', 'mat']\n",
      "Faltantes []\n",
      "Contradiccion ['no sit']\n",
      "Contradiccion2 []\n",
      "[('cat', 'same', 'cat'), ('mat', 'same', 'mat')]\n",
      "[('cat', 'same', 'cat'), ('mat', 'same', 'mat')]\n",
      "Matriz original\n",
      "      not    do   sit   cat   mat    on\n",
      "sit  0.05  0.24  1.00  0.03  0.13  0.22\n",
      "cat  0.02  0.02  0.03  1.00  0.14  0.01\n",
      "mat  0.05  0.00  0.13  0.14  1.00  0.06\n",
      "on   0.11  0.12  0.22  0.01  0.06  1.00\n",
      "Matriz quitando generalidad\n",
      "      not    do   sit    on\n",
      "sit  0.05  0.24  1.00  0.22\n",
      "cat  0.02  0.02  0.03  0.01\n",
      "mat  0.05  0.00  0.13  0.06\n",
      "on   0.11  0.12  0.22  1.00\n",
      "2.5\n",
      "3.5\n",
      "Matriz quitando contradiction\n",
      "      not    do   sit   cat   mat    on\n",
      "sit  0.05  0.24  1.00  0.03  0.13  0.22\n",
      "cat  0.02  0.02  0.03  1.00  0.14  0.01\n",
      "mat  0.05  0.00  0.13  0.14  1.00  0.06\n",
      "on   0.11  0.12  0.22  0.01  0.06  1.00\n",
      "0\n",
      "3.585\n",
      "Matriz con los que no hicieron match\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: [sit, cat, mat, on]\n",
      "proceso lexico\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: [sit, cat, mat, on] Index([], dtype='object')\n",
      "0.5\n",
      "0.25\n",
      "0.0\n",
      "Matriz con los que no hicieron match\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: [sit, cat, mat, on]\n",
      "proceso de obtención de no relaciones\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: [sit, cat, mat, on] Index([], dtype='object')\n",
      "rel [1 1 0]\n",
      "entropia final 0.918\n",
      "1\n",
      "The cat did not sit on the mat.\n",
      "{'cat': '', 'sit': 'do,not,,on,', 'mat': ''} ['cat', 'sit', 'mat']\n",
      "The cat sat on the mat.\n",
      "{'cat': '', 'sit': 'on', 'mat': ''} ['cat', 'sit', 'mat']\n",
      "clase not_entailment\n",
      "valores de diccionario ['', 'do,not,,on,', '']\n",
      "valores de diccionario ['', 'on', '']\n",
      "[{'bender', 'ˈ', 'non', 'n’t'}, {'doctor_of_osteopathy', 'achieve', 'translate', 'execute', 'get_together', 'ut', 'cozen', 'effect', 'disturbance', 'finish', 'medical_doctor', 'swindle', 'do', 'dress', 'serve', 'suffice', 'terminate', 'chouse', 'bash', 'settle', 'practice', 'cause', 'at', 'complete', 'observe', 'perform', 'act', 'make', 'prepare', 'conduct', 'bustle', 'conclude', 'to', 'accomplish', 'transact', 'cook'}, {'be_seated', 'sit_down', 'baby_sit', 'ride', 'seat', 'model', 'accept', 'be'}, {'computerized_tomography', 'panther', 'catamaran', 'guy', 'big_cat', 'cat_o_nine_tails', 'cat', 'feliform', 'saber_toothed_cat', 'pantherine_cat', 'catfish', 'kat', 'feline_cat', 'vomit', 'ct'}, {'flatness', 'master_of_arts_in_teaching', 'mat', 'matinee', 'felt', 'matt', 'raft_foundation', 'entangle'}, {'about', 'by_dint_of', 'base', 'along', 'ontario'}]\n",
      "[{'be_seated', 'sit_down', 'baby_sit', 'ride', 'seat', 'model', 'accept', 'be'}, {'computerized_tomography', 'panther', 'catamaran', 'guy', 'big_cat', 'cat_o_nine_tails', 'cat', 'feliform', 'saber_toothed_cat', 'pantherine_cat', 'catfish', 'kat', 'feline_cat', 'vomit', 'ct'}, {'flatness', 'master_of_arts_in_teaching', 'mat', 'matinee', 'felt', 'matt', 'raft_foundation', 'entangle'}, {'about', 'by_dint_of', 'base', 'along', 'ontario'}]\n",
      "{'cat': '', 'sit': 'do,not,,on,', 'mat': ''}\n",
      "{'cat': '', 'sit': 'on', 'mat': ''}\n",
      "-----------\n",
      "['not', 'do', 'sit', 'cat', 'mat', 'on']\n",
      "['sit', 'cat', 'mat', 'on']\n",
      "{'cat': '', 'sit': 'do,not,,on,', 'mat': ''}\n",
      "{'cat': '', 'sit': 'on', 'mat': ''}\n",
      "['not', 'do', 'sit', 'cat', 'mat', 'on']\n",
      "['sit', 'cat', 'mat', 'on']\n",
      "Revision de reemplazo cat cat {'computerized_tomography', 'panther', 'catamaran', 'guy', 'big_cat', 'cat_o_nine_tails', 'cat', 'feliform', 'saber_toothed_cat', 'pantherine_cat', 'catfish', 'kat', 'feline_cat', 'vomit', 'ct'}\n",
      "Revision de reemplazo mat mat {'flatness', 'master_of_arts_in_teaching', 'mat', 'matinee', 'felt', 'matt', 'raft_foundation', 'entangle'}\n",
      "[1, 1, 1, 1, 1, 1]\n",
      "[0, 0, 1, 1, 1, 1]\n",
      "BoW:  0.816496580927726\n",
      "sit sit sinonimos\n",
      "cat cat sinonimos\n",
      "mat mat sinonimos\n",
      "on on sinonimos\n",
      "[0, 0, 1, 1, 1, 1]\n",
      "[1, 1, 1, 1]\n",
      "4\n",
      "jaro con antonimos\n",
      "[0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0]\n",
      "0\n",
      "0.0 0.0\n",
      "jaro con relacionadas\n",
      "[0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0]\n",
      "0\n",
      "0.0 0.0\n",
      "sit sit sinonimos\n",
      "cat cat sinonimos\n",
      "mat mat sinonimos\n",
      "on on sinonimos\n",
      "[0, 0, 1, 1, 1, 1]\n",
      "[1, 1, 1, 1]\n",
      "4\n",
      "1.0 0.8888888888888888\n",
      "{'cat': '', 'sit': 'do,not,,on,', 'mat': ''} ['cat', 'sit', 'mat']\n",
      "{'cat': '', 'sit': 'on', 'mat': ''} ['cat', 'sit', 'mat']\n",
      "negT [('sit', 'not')]\n",
      "negH []\n",
      "1.0\n",
      "s1 ['not', 'do', 'sit', 'cat', 'mat', 'on']\n",
      "s2 ['sit', 'cat', 'mat', 'on']\n",
      "1.0\n",
      "t_lem ['not', 'do', 'sit', 'cat', 'mat', 'on']\n",
      "h_lem ['sit', 'cat', 'mat', 'on']\n",
      "      sit   cat   mat    on\n",
      "not  0.05  0.02  0.05  0.11\n",
      "do   0.24  0.02  0.00  0.12\n",
      "sit  1.00  0.03  0.13  0.22\n",
      "cat  0.03  1.00  0.14  0.01\n",
      "mat  0.13  0.14  1.00  0.06\n",
      "on   0.22  0.01  0.06  1.00\n",
      "rel nueva categoria [1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Valores para entropia {0, 1}\n",
      "Probabilidades [0.8333333333333334, 0.16666666666666666]\n",
      "si esta cat\n",
      "atributos de T []\n",
      "atributos de H []\n",
      "ENTAILMENT\n",
      "si esta sit\n",
      "atributos de T ['do', 'not', 'on']\n",
      "atributos de H ['on']\n",
      "CONTRADICTION\n",
      "si esta mat\n",
      "atributos de T []\n",
      "atributos de H []\n",
      "ENTAILMENT\n",
      "----------------------------------------------\n",
      "Contenidas ['cat', 'mat']\n",
      "Faltantes []\n",
      "Contradiccion ['no sit']\n",
      "Contradiccion2 []\n",
      "[('cat', 'same', 'cat'), ('mat', 'same', 'mat')]\n",
      "[('cat', 'same', 'cat'), ('mat', 'same', 'mat')]\n",
      "Matriz original\n",
      "      sit   cat   mat    on\n",
      "not  0.05  0.02  0.05  0.11\n",
      "do   0.24  0.02  0.00  0.12\n",
      "sit  1.00  0.03  0.13  0.22\n",
      "cat  0.03  1.00  0.14  0.01\n",
      "mat  0.13  0.14  1.00  0.06\n",
      "on   0.22  0.01  0.06  1.00\n",
      "Matriz quitando generalidad\n",
      "      sit    on\n",
      "not  0.05  0.11\n",
      "do   0.24  0.12\n",
      "sit  1.00  0.22\n",
      "cat  0.03  0.01\n",
      "mat  0.13  0.06\n",
      "on   0.22  1.00\n",
      "3.085\n",
      "3.252\n",
      "Matriz quitando contradiction\n",
      "      sit   cat   mat    on\n",
      "not  0.05  0.02  0.05  0.11\n",
      "do   0.24  0.02  0.00  0.12\n",
      "sit  1.00  0.03  0.13  0.22\n",
      "cat  0.03  1.00  0.14  0.01\n",
      "mat  0.13  0.14  1.00  0.06\n",
      "on   0.22  0.01  0.06  1.00\n",
      "0\n",
      "3.585\n",
      "Matriz con los que no hicieron match\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: [not, do, sit, cat, mat, on]\n",
      "proceso lexico\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: [not, do, sit, cat, mat, on] Index([], dtype='object')\n",
      "0.3333333333333333\n",
      "0.16666666666666666\n",
      "0.0\n",
      "Matriz con los que no hicieron match\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: [not, do, sit, cat, mat, on]\n",
      "proceso de obtención de no relaciones\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: [not, do, sit, cat, mat, on] Index([], dtype='object')\n",
      "rel [1 1 0]\n",
      "entropia final 0.918\n",
      "2\n",
      "When you've got no snow, it's really hard to learn a snow sport so we looked at all the different ways I could mimic being on snow without actually being on snow.\n",
      "{'when': '', 'get': 'when,,,have,,,', 'snow': 'no,,,,,,', 'to': '', 'learn': 'to,,', 'sport': '', 'so': '', 'look': 'so,,,at,', 'different': '', 'way': 'different', 'mimic': 'could,be,,without,'} ['when', 'get', 'snow', 'to', 'learn', 'sport', 'so', 'look', 'different', 'way', 'mimic']\n",
      "When you've got snow, it's really hard to learn a snow sport so we looked at all the different ways I could mimic being on snow without actually being on snow.\n",
      "{'when': '', 'get': 'when,,,have,,,', 'snow': '', 'to': '', 'learn': 'to,,', 'sport': '', 'so': '', 'look': 'so,,,at,', 'different': '', 'way': 'different', 'mimic': 'could,be,,without,'} ['when', 'get', 'snow', 'to', 'learn', 'sport', 'so', 'look', 'different', 'way', 'mimic']\n",
      "clase not_entailment\n",
      "valores de diccionario ['', 'when,,,have,,,', 'no,,,,,,', '', 'to,,', '', '', 'so,,,at,', '', 'different', 'could,be,,without,']\n",
      "valores de diccionario ['', 'when,,,have,,,', '', '', 'to,,', '', '', 'so,,,at,', '', 'different', 'could,be,,without,']\n",
      "[{'look', 'heed', 'behold', 'watch', 'study', 'seem', 'observe', 'search', 'expect', 'trust', 'examine', 'face', 'anticipate', 'attend', 'consider', 'await', 'front', 'spirit', 'investigate', 'inspect', 'contemplate', 'regard', 'notice', 'mind', 'expression', 'appear'}, {'out', 'lacking'}, {'arrive', 'come', 'perplex', 'acquire', 'suffer', 'assault', 'experience', 'be_able_to', 'scram', 'contract', 'cause', 'con', 'catch', 'begin', 'getter', 'become', 'pay_back', 'make', 'get_down', 'confuse', 'drive', 'have', 'obtain', 'dig', 'draw', 'grow', 'induce', 'beget', 'bring', 'receive'}, {'bear', 'give_birth', 'hold', 'suffer', 'possess', 'experience', 'know', 'get', 'rich_person', 'eat', 'consume', 'conquer', 'must', 'accept', 'partake', 'take', 'induce', 'own', 'sleep_with', 'have_got', 'receive'}, set(), set(), {'musical', 'sb', 'therefore', 'that', 'and_so', 'so_that', 'like', 'shootout', 'shutout', 'really', 'correct', 'sol', 'thusly'}, {'channel', 'far', 'way', 'behaviour', 'path', 'space', 'approach', 'usage', 'custom', 'street', 'fashion', 'road', 'habitude', 'method', 'trail', 'room', 'wont', 'wise', 'second_nature', 'highroad', 'means', 'alley', 'journey', 'practice', 'scheme', 'right_smart', 'interval', 'manner', 'pathway', 'course', 'advance', 'artery', 'direction', 'device', 'guise', 'so', 'distance', 'step'}, {'nobelium', 'ordinal_number', 'no_more', 'unsupported_titles_number_sign', 'ne', 'nope', 'negative', 'no', 'nay', 'a'}, {'self_shifting_transmission', 'auto', 'at_sign', 'commercial_at', 'n_speed_automatic', 'astatine'}, {'closed', 'for', 'in_order_to', 'for_to'}, {'study', 'memorize', 'determine', 'teach'}, {'several', 'dissimilar', 'other', 'apart', 'disparate', 'unlike', 'unusual', 'aberrant', 'unalike', 'sundry', 'distinct', 'various'}, set(), {'fun', 'sport', 'frolic', 'mutant'}, {'coke', 'bamboozle', 'snow', 'baron_snow_of_leicester', 'blow'}]\n",
      "[{'look', 'heed', 'behold', 'watch', 'study', 'seem', 'observe', 'search', 'expect', 'trust', 'examine', 'face', 'anticipate', 'attend', 'consider', 'await', 'front', 'spirit', 'investigate', 'inspect', 'contemplate', 'regard', 'notice', 'mind', 'expression', 'appear'}, {'out', 'lacking'}, {'arrive', 'come', 'perplex', 'acquire', 'suffer', 'assault', 'experience', 'be_able_to', 'scram', 'contract', 'cause', 'con', 'catch', 'begin', 'getter', 'become', 'pay_back', 'make', 'get_down', 'confuse', 'drive', 'have', 'obtain', 'dig', 'draw', 'grow', 'induce', 'beget', 'bring', 'receive'}, {'bear', 'give_birth', 'hold', 'suffer', 'possess', 'experience', 'know', 'get', 'rich_person', 'eat', 'consume', 'conquer', 'must', 'accept', 'partake', 'take', 'induce', 'own', 'sleep_with', 'have_got', 'receive'}, set(), set(), {'musical', 'sb', 'therefore', 'that', 'and_so', 'so_that', 'like', 'shootout', 'shutout', 'really', 'correct', 'sol', 'thusly'}, {'channel', 'far', 'way', 'behaviour', 'path', 'space', 'approach', 'usage', 'custom', 'street', 'fashion', 'road', 'habitude', 'method', 'trail', 'room', 'wont', 'wise', 'second_nature', 'highroad', 'means', 'alley', 'journey', 'practice', 'scheme', 'right_smart', 'interval', 'manner', 'pathway', 'course', 'advance', 'artery', 'direction', 'device', 'guise', 'so', 'distance', 'step'}, {'self_shifting_transmission', 'auto', 'at_sign', 'commercial_at', 'n_speed_automatic', 'astatine'}, {'closed', 'for', 'in_order_to', 'for_to'}, {'study', 'memorize', 'determine', 'teach'}, {'several', 'dissimilar', 'other', 'apart', 'disparate', 'unlike', 'unusual', 'aberrant', 'unalike', 'sundry', 'distinct', 'various'}, set(), {'fun', 'sport', 'frolic', 'mutant'}, {'coke', 'bamboozle', 'snow', 'baron_snow_of_leicester', 'blow'}]\n",
      "{'when': '', 'get': 'when,,,have,,,', 'snow': 'no,,,,,,', 'to': '', 'learn': 'to,,', 'sport': '', 'so': '', 'look': 'so,,,at,', 'different': '', 'way': 'different', 'mimic': 'could,be,,without,'}\n",
      "{'when': '', 'get': 'when,,,have,,,', 'snow': '', 'to': '', 'learn': 'to,,', 'sport': '', 'so': '', 'look': 'so,,,at,', 'different': '', 'way': 'different', 'mimic': 'could,be,,without,'}\n",
      "-----------\n",
      "['look', 'without', 'get', 'have', 'mimic', 'could', 'so', 'way', 'no', 'at', 'to', 'learn', 'different', 'when', 'sport', 'snow']\n",
      "['look', 'without', 'get', 'have', 'mimic', 'could', 'so', 'way', 'at', 'to', 'learn', 'different', 'when', 'sport', 'snow']\n",
      "{'when': '', 'get': 'when,,,have,,,', 'snow': 'no,,,,,,', 'to': '', 'learn': 'to,,', 'sport': '', 'so': '', 'look': 'so,,,at,', 'different': '', 'way': 'different', 'mimic': 'could,be,,without,'}\n",
      "{'when': '', 'get': 'when,,,have,,,', 'snow': '', 'to': '', 'learn': 'to,,', 'sport': '', 'so': '', 'look': 'so,,,at,', 'different': '', 'way': 'different', 'mimic': 'could,be,,without,'}\n",
      "['look', 'without', 'get', 'have', 'mimic', 'could', 'so', 'way', 'no', 'at', 'to', 'learn', 'different', 'when', 'sport', 'snow']\n",
      "['look', 'without', 'get', 'have', 'mimic', 'could', 'so', 'way', 'at', 'to', 'learn', 'different', 'when', 'sport', 'snow']\n",
      "Revision de reemplazo look look {'look', 'heed', 'behold', 'watch', 'study', 'seem', 'observe', 'search', 'expect', 'trust', 'examine', 'face', 'anticipate', 'attend', 'consider', 'await', 'front', 'spirit', 'investigate', 'inspect', 'contemplate', 'regard', 'notice', 'mind', 'expression', 'appear'}\n",
      "Revision de reemplazo get have {'bear', 'give_birth', 'hold', 'suffer', 'possess', 'experience', 'know', 'get', 'rich_person', 'eat', 'consume', 'conquer', 'must', 'accept', 'partake', 'take', 'induce', 'own', 'sleep_with', 'have_got', 'receive'}\n",
      "get have\n",
      "Revision de reemplazo have get {'arrive', 'come', 'perplex', 'acquire', 'suffer', 'assault', 'experience', 'be_able_to', 'scram', 'contract', 'cause', 'con', 'catch', 'begin', 'getter', 'become', 'pay_back', 'make', 'get_down', 'confuse', 'drive', 'have', 'obtain', 'dig', 'draw', 'grow', 'induce', 'beget', 'bring', 'receive'}\n",
      "have get\n",
      "Revision de reemplazo get have {'bear', 'give_birth', 'hold', 'suffer', 'possess', 'experience', 'know', 'get', 'rich_person', 'eat', 'consume', 'conquer', 'must', 'accept', 'partake', 'take', 'induce', 'own', 'sleep_with', 'have_got', 'receive'}\n",
      "get have\n",
      "Revision de reemplazo so way {'channel', 'far', 'way', 'behaviour', 'path', 'space', 'approach', 'usage', 'custom', 'street', 'fashion', 'road', 'habitude', 'method', 'trail', 'room', 'wont', 'wise', 'second_nature', 'highroad', 'means', 'alley', 'journey', 'practice', 'scheme', 'right_smart', 'interval', 'manner', 'pathway', 'course', 'advance', 'artery', 'direction', 'device', 'guise', 'so', 'distance', 'step'}\n",
      "so way\n",
      "Revision de reemplazo way way {'channel', 'far', 'way', 'behaviour', 'path', 'space', 'approach', 'usage', 'custom', 'street', 'fashion', 'road', 'habitude', 'method', 'trail', 'room', 'wont', 'wise', 'second_nature', 'highroad', 'means', 'alley', 'journey', 'practice', 'scheme', 'right_smart', 'interval', 'manner', 'pathway', 'course', 'advance', 'artery', 'direction', 'device', 'guise', 'so', 'distance', 'step'}\n",
      "Revision de reemplazo sport sport {'fun', 'sport', 'frolic', 'mutant'}\n",
      "Revision de reemplazo snow snow {'coke', 'bamboozle', 'snow', 'baron_snow_of_leicester', 'blow'}\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[1, 1, 2, 1, 1, 1, 1, 0, 1, 2, 0, 0, 1, 1, 1, 1]\n",
      "BoW:  0.8603090020146066\n",
      "look look sinonimos\n",
      "without without sinonimos\n",
      "get have sinonimos\n",
      "have have sinonimos\n",
      "mimic mimic sinonimos\n",
      "could could sinonimos\n",
      "hiperonimos sobre sinonimos way so\n",
      "way way sinonimos\n",
      "at at sinonimos\n",
      "to to sinonimos\n",
      "learn learn sinonimos\n",
      "different different sinonimos\n",
      "when when sinonimos\n",
      "sport sport sinonimos\n",
      "snow snow sinonimos\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "15\n",
      "jaro con antonimos\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "0\n",
      "0.0 0.0\n",
      "jaro con relacionadas\n",
      "get have antonyms\n",
      "have could antonyms\n",
      "could have antonyms\n",
      "no to antonyms\n",
      "to at antonyms\n",
      "[0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0]\n",
      "[0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0]\n",
      "5\n",
      "0.3333333333333333 0.48194444444444445\n",
      "look look sinonimos\n",
      "without without sinonimos\n",
      "get have sinonimos\n",
      "have have sinonimos\n",
      "mimic mimic sinonimos\n",
      "could could sinonimos\n",
      "hiperonimos sobre sinonimos way so\n",
      "way way sinonimos\n",
      "relacionales to no\n",
      "at at sinonimos\n",
      "learn learn sinonimos\n",
      "different different sinonimos\n",
      "when when sinonimos\n",
      "sport sport sinonimos\n",
      "snow snow sinonimos\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "14\n",
      "0.9333333333333333 0.4884920634920634\n",
      "{'when': '', 'get': 'when,,,have,,,', 'snow': 'no,,,,,,', 'to': '', 'learn': 'to,,', 'sport': '', 'so': '', 'look': 'so,,,at,', 'different': '', 'way': 'different', 'mimic': 'could,be,,without,'} ['when', 'get', 'snow', 'to', 'learn', 'sport', 'so', 'look', 'different', 'way', 'mimic']\n",
      "{'when': '', 'get': 'when,,,have,,,', 'snow': '', 'to': '', 'learn': 'to,,', 'sport': '', 'so': '', 'look': 'so,,,at,', 'different': '', 'way': 'different', 'mimic': 'could,be,,without,'} ['when', 'get', 'snow', 'to', 'learn', 'sport', 'so', 'look', 'different', 'way', 'mimic']\n",
      "negT [('snow', 'no'), ('mimic', 'without')]\n",
      "negH [('mimic', 'without')]\n",
      "1.0\n",
      "s1 ['look', 'without', 'get', 'have', 'mimic', 'could', 'so', 'way', 'no', 'at', 'to', 'learn', 'different', 'when', 'sport', 'snow']\n",
      "s2 ['look', 'without', 'have', 'have', 'mimic', 'could', 'way', 'way', 'at', 'to', 'learn', 'different', 'when', 'sport', 'snow']\n",
      "1.0\n",
      "t_lem ['look', 'without', 'get', 'have', 'mimic', 'could', 'so', 'way', 'no', 'at', 'to', 'learn', 'different', 'when', 'sport', 'snow']\n",
      "h_lem ['look', 'without', 'get', 'have', 'mimic', 'could', 'so', 'way', 'at', 'to', 'learn', 'different', 'when', 'sport', 'snow']\n",
      "           look  without   get  have  mimic  could    so   way    at    to  \\\n",
      "look       1.00     0.01  0.24  0.25   0.18   0.24  0.26  0.21  0.22  0.22   \n",
      "without    0.01     1.00  0.16  0.26   0.00   0.32  0.19  0.15  0.19  0.19   \n",
      "get        0.24     0.16  1.00  0.46   0.03   0.33  0.28  0.22  0.16  0.39   \n",
      "have       0.25     0.26  0.46  1.00   0.05   0.62  0.44  0.12  0.17  0.34   \n",
      "mimic      0.18     0.00  0.03  0.05   1.00   0.08  0.00  0.01  0.00  0.05   \n",
      "could      0.24     0.32  0.33  0.62   0.08   1.00  0.49  0.30  0.22  0.51   \n",
      "so         0.26     0.19  0.28  0.44   0.00   0.49  1.00  0.34  0.28  0.38   \n",
      "way        0.21     0.15  0.22  0.12   0.01   0.30  0.34  1.00  0.18  0.37   \n",
      "no         0.06     0.52  0.07  0.33   0.00   0.32  0.32  0.09  0.14  0.10   \n",
      "at         0.22     0.19  0.16  0.17   0.00   0.22  0.28  0.18  1.00  0.44   \n",
      "to         0.22     0.19  0.39  0.34   0.05   0.51  0.38  0.37  0.44  1.00   \n",
      "learn      0.26     0.00  0.37  0.24   0.21   0.19  0.11  0.14  0.11  0.30   \n",
      "different  0.11     0.05  0.05  0.17   0.18   0.12  0.09  0.19  0.08  0.06   \n",
      "when       0.09     0.22  0.18  0.23   0.01   0.37  0.53  0.25  0.41  0.34   \n",
      "sport      0.06     0.00  0.01  0.03   0.00   0.00  0.01  0.01  0.00  0.00   \n",
      "snow       0.00     0.00  0.01  0.00   0.00   0.00  0.01  0.04  0.00  0.01   \n",
      "\n",
      "           learn  different  when  sport  snow  \n",
      "look        0.26       0.11  0.09   0.06  0.00  \n",
      "without     0.00       0.05  0.22   0.00  0.00  \n",
      "get         0.37       0.05  0.18   0.01  0.01  \n",
      "have        0.24       0.17  0.23   0.03  0.00  \n",
      "mimic       0.21       0.18  0.01   0.00  0.00  \n",
      "could       0.19       0.12  0.37   0.00  0.00  \n",
      "so          0.11       0.09  0.53   0.01  0.01  \n",
      "way         0.14       0.19  0.25   0.01  0.04  \n",
      "no          0.00       0.13  0.17   0.00  0.01  \n",
      "at          0.11       0.08  0.41   0.00  0.00  \n",
      "to          0.30       0.06  0.34   0.00  0.01  \n",
      "learn       1.00       0.18  0.13   0.04  0.00  \n",
      "different   0.18       1.00  0.17   0.01  0.00  \n",
      "when        0.13       0.17  1.00   0.00  0.04  \n",
      "sport       0.04       0.01  0.00   1.00  0.12  \n",
      "snow        0.00       0.00  0.04   0.12  1.00  \n",
      "rel nueva categoria [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Valores para entropia {0, 1, 2}\n",
      "Probabilidades [0.9083333333333333, 0.0625, 0.029166666666666667]\n",
      "si esta when\n",
      "atributos de T []\n",
      "atributos de H []\n",
      "ENTAILMENT\n",
      "si esta get\n",
      "atributos de T ['when', 'have']\n",
      "atributos de H ['when', 'have']\n",
      "si esta when\n",
      "si esta have\n",
      "ENTAILMENT\n",
      "si esta snow\n",
      "atributos de T ['no']\n",
      "atributos de H []\n",
      "CONTRADICTION\n",
      "si esta to\n",
      "atributos de T []\n",
      "atributos de H []\n",
      "ENTAILMENT\n",
      "si esta learn\n",
      "atributos de T ['to']\n",
      "atributos de H ['to']\n",
      "si esta to\n",
      "ENTAILMENT\n",
      "si esta sport\n",
      "atributos de T []\n",
      "atributos de H []\n",
      "ENTAILMENT\n",
      "si esta so\n",
      "atributos de T []\n",
      "atributos de H []\n",
      "ENTAILMENT\n",
      "si esta look\n",
      "atributos de T ['so', 'at']\n",
      "atributos de H ['so', 'at']\n",
      "si esta so\n",
      "si esta at\n",
      "ENTAILMENT\n",
      "si esta different\n",
      "atributos de T []\n",
      "atributos de H []\n",
      "ENTAILMENT\n",
      "si esta way\n",
      "atributos de T ['different']\n",
      "atributos de H ['different']\n",
      "si esta different\n",
      "ENTAILMENT\n",
      "si esta mimic\n",
      "atributos de T ['could', 'without']\n",
      "atributos de H ['could', 'without']\n",
      "si esta could\n",
      "si esta without\n",
      "ENTAILMENT\n",
      "----------------------------------------------\n",
      "Contenidas ['when', 'get', 'to', 'learn', 'sport', 'so', 'look', 'different', 'way', 'mimic']\n",
      "Faltantes []\n",
      "Contradiccion ['no snow']\n",
      "Contradiccion2 []\n",
      "[('when', 'same', 'when'), ('get', 'same', 'get'), ('to', 'same', 'to'), ('learn', 'same', 'learn'), ('sport', 'same', 'sport'), ('so', 'same', 'so'), ('look', 'same', 'look'), ('different', 'same', 'different'), ('way', 'same', 'way'), ('mimic', 'same', 'mimic')]\n",
      "[('when', 'same', 'when'), ('get', 'same', 'get'), ('to', 'same', 'to'), ('learn', 'same', 'learn'), ('sport', 'same', 'sport'), ('so', 'same', 'so'), ('look', 'same', 'look'), ('different', 'same', 'different'), ('way', 'same', 'way'), ('mimic', 'same', 'mimic')]\n",
      "Matriz original\n",
      "           look  without   get  have  mimic  could    so   way    at    to  \\\n",
      "look       1.00     0.01  0.24  0.25   0.18   0.24  0.26  0.21  0.22  0.22   \n",
      "without    0.01     1.00  0.16  0.26   0.00   0.32  0.19  0.15  0.19  0.19   \n",
      "get        0.24     0.16  1.00  0.46   0.03   0.33  0.28  0.22  0.16  0.39   \n",
      "have       0.25     0.26  0.46  1.00   0.05   0.62  0.44  0.12  0.17  0.34   \n",
      "mimic      0.18     0.00  0.03  0.05   1.00   0.08  0.00  0.01  0.00  0.05   \n",
      "could      0.24     0.32  0.33  0.62   0.08   1.00  0.49  0.30  0.22  0.51   \n",
      "so         0.26     0.19  0.28  0.44   0.00   0.49  1.00  0.34  0.28  0.38   \n",
      "way        0.21     0.15  0.22  0.12   0.01   0.30  0.34  1.00  0.18  0.37   \n",
      "no         0.06     0.52  0.07  0.33   0.00   0.32  0.32  0.09  0.14  0.10   \n",
      "at         0.22     0.19  0.16  0.17   0.00   0.22  0.28  0.18  1.00  0.44   \n",
      "to         0.22     0.19  0.39  0.34   0.05   0.51  0.38  0.37  0.44  1.00   \n",
      "learn      0.26     0.00  0.37  0.24   0.21   0.19  0.11  0.14  0.11  0.30   \n",
      "different  0.11     0.05  0.05  0.17   0.18   0.12  0.09  0.19  0.08  0.06   \n",
      "when       0.09     0.22  0.18  0.23   0.01   0.37  0.53  0.25  0.41  0.34   \n",
      "sport      0.06     0.00  0.01  0.03   0.00   0.00  0.01  0.01  0.00  0.00   \n",
      "snow       0.00     0.00  0.01  0.00   0.00   0.00  0.01  0.04  0.00  0.01   \n",
      "\n",
      "           learn  different  when  sport  snow  \n",
      "look        0.26       0.11  0.09   0.06  0.00  \n",
      "without     0.00       0.05  0.22   0.00  0.00  \n",
      "get         0.37       0.05  0.18   0.01  0.01  \n",
      "have        0.24       0.17  0.23   0.03  0.00  \n",
      "mimic       0.21       0.18  0.01   0.00  0.00  \n",
      "could       0.19       0.12  0.37   0.00  0.00  \n",
      "so          0.11       0.09  0.53   0.01  0.01  \n",
      "way         0.14       0.19  0.25   0.01  0.04  \n",
      "no          0.00       0.13  0.17   0.00  0.01  \n",
      "at          0.11       0.08  0.41   0.00  0.00  \n",
      "to          0.30       0.06  0.34   0.00  0.01  \n",
      "learn       1.00       0.18  0.13   0.04  0.00  \n",
      "different   0.18       1.00  0.17   0.01  0.00  \n",
      "when        0.13       0.17  1.00   0.00  0.04  \n",
      "sport       0.04       0.01  0.00   1.00  0.12  \n",
      "snow        0.00       0.00  0.04   0.12  1.00  \n",
      "Matriz quitando generalidad\n",
      "           without  have  could    at  snow\n",
      "look          0.01  0.25   0.24  0.22  0.00\n",
      "without       1.00  0.26   0.32  0.19  0.00\n",
      "get           0.16  0.46   0.33  0.16  0.01\n",
      "have          0.26  1.00   0.62  0.17  0.00\n",
      "mimic         0.00  0.05   0.08  0.00  0.00\n",
      "could         0.32  0.62   1.00  0.22  0.00\n",
      "so            0.19  0.44   0.49  0.28  0.01\n",
      "way           0.15  0.12   0.30  0.18  0.04\n",
      "no            0.52  0.33   0.32  0.14  0.01\n",
      "at            0.19  0.17   0.22  1.00  0.00\n",
      "to            0.19  0.34   0.51  0.44  0.01\n",
      "learn         0.00  0.24   0.19  0.11  0.00\n",
      "different     0.05  0.17   0.12  0.08  0.00\n",
      "when          0.22  0.23   0.37  0.41  0.04\n",
      "sport         0.00  0.03   0.00  0.00  0.12\n",
      "snow          0.00  0.00   0.00  0.00  1.00\n",
      "4.814\n",
      "4.445\n",
      "Matriz quitando contradiction\n",
      "           look  without   get  have  mimic  could    so   way    at    to  \\\n",
      "look       1.00     0.01  0.24  0.25   0.18   0.24  0.26  0.21  0.22  0.22   \n",
      "without    0.01     1.00  0.16  0.26   0.00   0.32  0.19  0.15  0.19  0.19   \n",
      "get        0.24     0.16  1.00  0.46   0.03   0.33  0.28  0.22  0.16  0.39   \n",
      "have       0.25     0.26  0.46  1.00   0.05   0.62  0.44  0.12  0.17  0.34   \n",
      "mimic      0.18     0.00  0.03  0.05   1.00   0.08  0.00  0.01  0.00  0.05   \n",
      "could      0.24     0.32  0.33  0.62   0.08   1.00  0.49  0.30  0.22  0.51   \n",
      "so         0.26     0.19  0.28  0.44   0.00   0.49  1.00  0.34  0.28  0.38   \n",
      "way        0.21     0.15  0.22  0.12   0.01   0.30  0.34  1.00  0.18  0.37   \n",
      "no         0.06     0.52  0.07  0.33   0.00   0.32  0.32  0.09  0.14  0.10   \n",
      "at         0.22     0.19  0.16  0.17   0.00   0.22  0.28  0.18  1.00  0.44   \n",
      "to         0.22     0.19  0.39  0.34   0.05   0.51  0.38  0.37  0.44  1.00   \n",
      "learn      0.26     0.00  0.37  0.24   0.21   0.19  0.11  0.14  0.11  0.30   \n",
      "different  0.11     0.05  0.05  0.17   0.18   0.12  0.09  0.19  0.08  0.06   \n",
      "when       0.09     0.22  0.18  0.23   0.01   0.37  0.53  0.25  0.41  0.34   \n",
      "sport      0.06     0.00  0.01  0.03   0.00   0.00  0.01  0.01  0.00  0.00   \n",
      "snow       0.00     0.00  0.01  0.00   0.00   0.00  0.01  0.04  0.00  0.01   \n",
      "\n",
      "           learn  different  when  sport  snow  \n",
      "look        0.26       0.11  0.09   0.06  0.00  \n",
      "without     0.00       0.05  0.22   0.00  0.00  \n",
      "get         0.37       0.05  0.18   0.01  0.01  \n",
      "have        0.24       0.17  0.23   0.03  0.00  \n",
      "mimic       0.21       0.18  0.01   0.00  0.00  \n",
      "could       0.19       0.12  0.37   0.00  0.00  \n",
      "so          0.11       0.09  0.53   0.01  0.01  \n",
      "way         0.14       0.19  0.25   0.01  0.04  \n",
      "no          0.00       0.13  0.17   0.00  0.01  \n",
      "at          0.11       0.08  0.41   0.00  0.00  \n",
      "to          0.30       0.06  0.34   0.00  0.01  \n",
      "learn       1.00       0.18  0.13   0.04  0.00  \n",
      "different   0.18       1.00  0.17   0.01  0.00  \n",
      "when        0.13       0.17  1.00   0.00  0.04  \n",
      "sport       0.04       0.01  0.00   1.00  0.12  \n",
      "snow        0.00       0.00  0.04   0.12  1.00  \n",
      "0\n",
      "4.83\n",
      "Matriz con los que no hicieron match\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: [look, without, get, have, mimic, could, so, way, no, at, to, learn, different, when, sport, snow]\n",
      "proceso lexico\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: [look, without, get, have, mimic, could, so, way, no, at, to, learn, different, when, sport, snow] Index([], dtype='object')\n",
      "0.625\n",
      "0.0625\n",
      "0.0\n",
      "Matriz con los que no hicieron match\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: [look, without, get, have, mimic, could, so, way, no, at, to, learn, different, when, sport, snow]\n",
      "proceso de obtención de no relaciones\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: [look, without, get, have, mimic, could, so, way, no, at, to, learn, different, when, sport, snow] Index([], dtype='object')\n",
      "rel [1 1 1 1 1 1 1 1 1 1 0]\n",
      "entropia final 0.439\n",
      "3\n",
      "When you've got snow, it's really hard to learn a snow sport so we looked at all the different ways I could mimic being on snow without actually being on snow.\n",
      "{'when': '', 'get': 'when,,,have,,,', 'snow': '', 'to': '', 'learn': 'to,,', 'sport': '', 'so': '', 'look': 'so,,,at,', 'different': '', 'way': 'different', 'mimic': 'could,be,,without,'} ['when', 'get', 'snow', 'to', 'learn', 'sport', 'so', 'look', 'different', 'way', 'mimic']\n",
      "When you've got no snow, it's really hard to learn a snow sport so we looked at all the different ways I could mimic being on snow without actually being on snow.\n",
      "{'when': '', 'get': 'when,,,have,,,', 'snow': 'no,,,,,,', 'to': '', 'learn': 'to,,', 'sport': '', 'so': '', 'look': 'so,,,at,', 'different': '', 'way': 'different', 'mimic': 'could,be,,without,'} ['when', 'get', 'snow', 'to', 'learn', 'sport', 'so', 'look', 'different', 'way', 'mimic']\n",
      "clase not_entailment\n",
      "valores de diccionario ['', 'when,,,have,,,', '', '', 'to,,', '', '', 'so,,,at,', '', 'different', 'could,be,,without,']\n",
      "valores de diccionario ['', 'when,,,have,,,', 'no,,,,,,', '', 'to,,', '', '', 'so,,,at,', '', 'different', 'could,be,,without,']\n",
      "[{'look', 'heed', 'behold', 'watch', 'study', 'seem', 'observe', 'search', 'expect', 'trust', 'examine', 'face', 'anticipate', 'attend', 'consider', 'await', 'front', 'spirit', 'investigate', 'inspect', 'contemplate', 'regard', 'notice', 'mind', 'expression', 'appear'}, {'out', 'lacking'}, {'arrive', 'come', 'perplex', 'acquire', 'suffer', 'assault', 'experience', 'be_able_to', 'scram', 'contract', 'cause', 'con', 'catch', 'begin', 'getter', 'become', 'pay_back', 'make', 'get_down', 'confuse', 'drive', 'have', 'obtain', 'dig', 'draw', 'grow', 'induce', 'beget', 'bring', 'receive'}, {'bear', 'give_birth', 'hold', 'suffer', 'possess', 'experience', 'know', 'get', 'rich_person', 'eat', 'consume', 'conquer', 'must', 'accept', 'partake', 'take', 'induce', 'own', 'sleep_with', 'have_got', 'receive'}, set(), set(), {'musical', 'sb', 'therefore', 'that', 'and_so', 'so_that', 'like', 'shootout', 'shutout', 'really', 'correct', 'sol', 'thusly'}, {'channel', 'far', 'way', 'behaviour', 'path', 'space', 'approach', 'usage', 'custom', 'street', 'fashion', 'road', 'habitude', 'method', 'trail', 'room', 'wont', 'wise', 'second_nature', 'highroad', 'means', 'alley', 'journey', 'practice', 'scheme', 'right_smart', 'interval', 'manner', 'pathway', 'course', 'advance', 'artery', 'direction', 'device', 'guise', 'so', 'distance', 'step'}, {'self_shifting_transmission', 'auto', 'at_sign', 'commercial_at', 'n_speed_automatic', 'astatine'}, {'closed', 'for', 'in_order_to', 'for_to'}, {'study', 'memorize', 'determine', 'teach'}, {'several', 'dissimilar', 'other', 'apart', 'disparate', 'unlike', 'unusual', 'aberrant', 'unalike', 'sundry', 'distinct', 'various'}, set(), {'fun', 'sport', 'frolic', 'mutant'}, {'coke', 'bamboozle', 'snow', 'baron_snow_of_leicester', 'blow'}]\n",
      "[{'look', 'heed', 'behold', 'watch', 'study', 'seem', 'observe', 'search', 'expect', 'trust', 'examine', 'face', 'anticipate', 'attend', 'consider', 'await', 'front', 'spirit', 'investigate', 'inspect', 'contemplate', 'regard', 'notice', 'mind', 'expression', 'appear'}, {'out', 'lacking'}, {'arrive', 'come', 'perplex', 'acquire', 'suffer', 'assault', 'experience', 'be_able_to', 'scram', 'contract', 'cause', 'con', 'catch', 'begin', 'getter', 'become', 'pay_back', 'make', 'get_down', 'confuse', 'drive', 'have', 'obtain', 'dig', 'draw', 'grow', 'induce', 'beget', 'bring', 'receive'}, {'bear', 'give_birth', 'hold', 'suffer', 'possess', 'experience', 'know', 'get', 'rich_person', 'eat', 'consume', 'conquer', 'must', 'accept', 'partake', 'take', 'induce', 'own', 'sleep_with', 'have_got', 'receive'}, set(), set(), {'musical', 'sb', 'therefore', 'that', 'and_so', 'so_that', 'like', 'shootout', 'shutout', 'really', 'correct', 'sol', 'thusly'}, {'channel', 'far', 'way', 'behaviour', 'path', 'space', 'approach', 'usage', 'custom', 'street', 'fashion', 'road', 'habitude', 'method', 'trail', 'room', 'wont', 'wise', 'second_nature', 'highroad', 'means', 'alley', 'journey', 'practice', 'scheme', 'right_smart', 'interval', 'manner', 'pathway', 'course', 'advance', 'artery', 'direction', 'device', 'guise', 'so', 'distance', 'step'}, {'nobelium', 'ordinal_number', 'no_more', 'unsupported_titles_number_sign', 'ne', 'nope', 'negative', 'no', 'nay', 'a'}, {'self_shifting_transmission', 'auto', 'at_sign', 'commercial_at', 'n_speed_automatic', 'astatine'}, {'closed', 'for', 'in_order_to', 'for_to'}, {'study', 'memorize', 'determine', 'teach'}, {'several', 'dissimilar', 'other', 'apart', 'disparate', 'unlike', 'unusual', 'aberrant', 'unalike', 'sundry', 'distinct', 'various'}, set(), {'fun', 'sport', 'frolic', 'mutant'}, {'coke', 'bamboozle', 'snow', 'baron_snow_of_leicester', 'blow'}]\n",
      "{'when': '', 'get': 'when,,,have,,,', 'snow': '', 'to': '', 'learn': 'to,,', 'sport': '', 'so': '', 'look': 'so,,,at,', 'different': '', 'way': 'different', 'mimic': 'could,be,,without,'}\n",
      "{'when': '', 'get': 'when,,,have,,,', 'snow': 'no,,,,,,', 'to': '', 'learn': 'to,,', 'sport': '', 'so': '', 'look': 'so,,,at,', 'different': '', 'way': 'different', 'mimic': 'could,be,,without,'}\n",
      "-----------\n",
      "['look', 'without', 'get', 'have', 'mimic', 'could', 'so', 'way', 'at', 'to', 'learn', 'different', 'when', 'sport', 'snow']\n",
      "['look', 'without', 'get', 'have', 'mimic', 'could', 'so', 'way', 'no', 'at', 'to', 'learn', 'different', 'when', 'sport', 'snow']\n",
      "{'when': '', 'get': 'when,,,have,,,', 'snow': '', 'to': '', 'learn': 'to,,', 'sport': '', 'so': '', 'look': 'so,,,at,', 'different': '', 'way': 'different', 'mimic': 'could,be,,without,'}\n",
      "{'when': '', 'get': 'when,,,have,,,', 'snow': 'no,,,,,,', 'to': '', 'learn': 'to,,', 'sport': '', 'so': '', 'look': 'so,,,at,', 'different': '', 'way': 'different', 'mimic': 'could,be,,without,'}\n",
      "['look', 'without', 'get', 'have', 'mimic', 'could', 'so', 'way', 'at', 'to', 'learn', 'different', 'when', 'sport', 'snow']\n",
      "['look', 'without', 'get', 'have', 'mimic', 'could', 'so', 'way', 'no', 'at', 'to', 'learn', 'different', 'when', 'sport', 'snow']\n",
      "Revision de reemplazo look look {'look', 'heed', 'behold', 'watch', 'study', 'seem', 'observe', 'search', 'expect', 'trust', 'examine', 'face', 'anticipate', 'attend', 'consider', 'await', 'front', 'spirit', 'investigate', 'inspect', 'contemplate', 'regard', 'notice', 'mind', 'expression', 'appear'}\n",
      "Revision de reemplazo get have {'bear', 'give_birth', 'hold', 'suffer', 'possess', 'experience', 'know', 'get', 'rich_person', 'eat', 'consume', 'conquer', 'must', 'accept', 'partake', 'take', 'induce', 'own', 'sleep_with', 'have_got', 'receive'}\n",
      "get have\n",
      "Revision de reemplazo have get {'arrive', 'come', 'perplex', 'acquire', 'suffer', 'assault', 'experience', 'be_able_to', 'scram', 'contract', 'cause', 'con', 'catch', 'begin', 'getter', 'become', 'pay_back', 'make', 'get_down', 'confuse', 'drive', 'have', 'obtain', 'dig', 'draw', 'grow', 'induce', 'beget', 'bring', 'receive'}\n",
      "have get\n",
      "Revision de reemplazo get have {'bear', 'give_birth', 'hold', 'suffer', 'possess', 'experience', 'know', 'get', 'rich_person', 'eat', 'consume', 'conquer', 'must', 'accept', 'partake', 'take', 'induce', 'own', 'sleep_with', 'have_got', 'receive'}\n",
      "get have\n",
      "Revision de reemplazo so way {'channel', 'far', 'way', 'behaviour', 'path', 'space', 'approach', 'usage', 'custom', 'street', 'fashion', 'road', 'habitude', 'method', 'trail', 'room', 'wont', 'wise', 'second_nature', 'highroad', 'means', 'alley', 'journey', 'practice', 'scheme', 'right_smart', 'interval', 'manner', 'pathway', 'course', 'advance', 'artery', 'direction', 'device', 'guise', 'so', 'distance', 'step'}\n",
      "so way\n",
      "Revision de reemplazo way way {'channel', 'far', 'way', 'behaviour', 'path', 'space', 'approach', 'usage', 'custom', 'street', 'fashion', 'road', 'habitude', 'method', 'trail', 'room', 'wont', 'wise', 'second_nature', 'highroad', 'means', 'alley', 'journey', 'practice', 'scheme', 'right_smart', 'interval', 'manner', 'pathway', 'course', 'advance', 'artery', 'direction', 'device', 'guise', 'so', 'distance', 'step'}\n",
      "Revision de reemplazo sport sport {'fun', 'sport', 'frolic', 'mutant'}\n",
      "Revision de reemplazo snow snow {'coke', 'bamboozle', 'snow', 'baron_snow_of_leicester', 'blow'}\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1]\n",
      "[1, 1, 2, 1, 1, 1, 1, 0, 1, 2, 0, 1, 1, 1, 1, 1]\n",
      "BoW:  0.8660254037844386\n",
      "look look sinonimos\n",
      "without without sinonimos\n",
      "get have sinonimos\n",
      "have have sinonimos\n",
      "mimic mimic sinonimos\n",
      "could could sinonimos\n",
      "hiperonimos sobre sinonimos way so\n",
      "way way sinonimos\n",
      "at at sinonimos\n",
      "to to sinonimos\n",
      "learn learn sinonimos\n",
      "different different sinonimos\n",
      "when when sinonimos\n",
      "sport sport sinonimos\n",
      "snow snow sinonimos\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1, 1, 1]\n",
      "15\n",
      "jaro con antonimos\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "0\n",
      "0.0 0.0\n",
      "jaro con relacionadas\n",
      "get have antonyms\n",
      "have could antonyms\n",
      "could have antonyms\n",
      "at to antonyms\n",
      "to no antonyms\n",
      "[0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0]\n",
      "[0, 0, 1, 1, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0]\n",
      "5\n",
      "0.3125 0.48194444444444445\n",
      "look look sinonimos\n",
      "without without sinonimos\n",
      "get have sinonimos\n",
      "have have sinonimos\n",
      "mimic mimic sinonimos\n",
      "could could sinonimos\n",
      "hiperonimos sobre sinonimos way so\n",
      "way way sinonimos\n",
      "at at sinonimos\n",
      "relacionales no to\n",
      "learn learn sinonimos\n",
      "different different sinonimos\n",
      "when when sinonimos\n",
      "sport sport sinonimos\n",
      "snow snow sinonimos\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 1, 1, 1, 1, 1]\n",
      "14\n",
      "0.875 0.4884920634920634\n",
      "{'when': '', 'get': 'when,,,have,,,', 'snow': '', 'to': '', 'learn': 'to,,', 'sport': '', 'so': '', 'look': 'so,,,at,', 'different': '', 'way': 'different', 'mimic': 'could,be,,without,'} ['when', 'get', 'snow', 'to', 'learn', 'sport', 'so', 'look', 'different', 'way', 'mimic']\n",
      "{'when': '', 'get': 'when,,,have,,,', 'snow': 'no,,,,,,', 'to': '', 'learn': 'to,,', 'sport': '', 'so': '', 'look': 'so,,,at,', 'different': '', 'way': 'different', 'mimic': 'could,be,,without,'} ['when', 'get', 'snow', 'to', 'learn', 'sport', 'so', 'look', 'different', 'way', 'mimic']\n",
      "negT [('mimic', 'without')]\n",
      "negH [('snow', 'no'), ('mimic', 'without')]\n",
      "1.0\n",
      "s1 ['look', 'without', 'get', 'have', 'mimic', 'could', 'so', 'way', 'at', 'to', 'learn', 'different', 'when', 'sport', 'snow']\n",
      "s2 ['look', 'without', 'have', 'have', 'mimic', 'could', 'way', 'way', 'no', 'at', 'to', 'learn', 'different', 'when', 'sport', 'snow']\n",
      "0.9285714285714286\n",
      "t_lem ['look', 'without', 'get', 'have', 'mimic', 'could', 'so', 'way', 'at', 'to', 'learn', 'different', 'when', 'sport', 'snow']\n",
      "h_lem ['look', 'without', 'get', 'have', 'mimic', 'could', 'so', 'way', 'no', 'at', 'to', 'learn', 'different', 'when', 'sport', 'snow']\n",
      "           look  without   get  have  mimic  could    so   way    no    at  \\\n",
      "look       1.00     0.01  0.24  0.25   0.18   0.24  0.26  0.21  0.06  0.22   \n",
      "without    0.01     1.00  0.16  0.26   0.00   0.32  0.19  0.15  0.52  0.19   \n",
      "get        0.24     0.16  1.00  0.46   0.03   0.33  0.28  0.22  0.07  0.16   \n",
      "have       0.25     0.26  0.46  1.00   0.05   0.62  0.44  0.12  0.33  0.17   \n",
      "mimic      0.18     0.00  0.03  0.05   1.00   0.08  0.00  0.01  0.00  0.00   \n",
      "could      0.24     0.32  0.33  0.62   0.08   1.00  0.49  0.30  0.32  0.22   \n",
      "so         0.26     0.19  0.28  0.44   0.00   0.49  1.00  0.34  0.32  0.28   \n",
      "way        0.21     0.15  0.22  0.12   0.01   0.30  0.34  1.00  0.09  0.18   \n",
      "at         0.22     0.19  0.16  0.17   0.00   0.22  0.28  0.18  0.14  1.00   \n",
      "to         0.22     0.19  0.39  0.34   0.05   0.51  0.38  0.37  0.10  0.44   \n",
      "learn      0.26     0.00  0.37  0.24   0.21   0.19  0.11  0.14  0.00  0.11   \n",
      "different  0.11     0.05  0.05  0.17   0.18   0.12  0.09  0.19  0.13  0.08   \n",
      "when       0.09     0.22  0.18  0.23   0.01   0.37  0.53  0.25  0.17  0.41   \n",
      "sport      0.06     0.00  0.01  0.03   0.00   0.00  0.01  0.01  0.00  0.00   \n",
      "snow       0.00     0.00  0.01  0.00   0.00   0.00  0.01  0.04  0.01  0.00   \n",
      "\n",
      "             to  learn  different  when  sport  snow  \n",
      "look       0.22   0.26       0.11  0.09   0.06  0.00  \n",
      "without    0.19   0.00       0.05  0.22   0.00  0.00  \n",
      "get        0.39   0.37       0.05  0.18   0.01  0.01  \n",
      "have       0.34   0.24       0.17  0.23   0.03  0.00  \n",
      "mimic      0.05   0.21       0.18  0.01   0.00  0.00  \n",
      "could      0.51   0.19       0.12  0.37   0.00  0.00  \n",
      "so         0.38   0.11       0.09  0.53   0.01  0.01  \n",
      "way        0.37   0.14       0.19  0.25   0.01  0.04  \n",
      "at         0.44   0.11       0.08  0.41   0.00  0.00  \n",
      "to         1.00   0.30       0.06  0.34   0.00  0.01  \n",
      "learn      0.30   1.00       0.18  0.13   0.04  0.00  \n",
      "different  0.06   0.18       1.00  0.17   0.01  0.00  \n",
      "when       0.34   0.13       0.17  1.00   0.00  0.04  \n",
      "sport      0.00   0.04       0.01  0.00   1.00  0.12  \n",
      "snow       0.01   0.00       0.00  0.04   0.12  1.00  \n",
      "rel nueva categoria [1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      "Valores para entropia {0, 1, 2}\n",
      "Probabilidades [0.9083333333333333, 0.0625, 0.029166666666666667]\n",
      "si esta when\n",
      "atributos de T []\n",
      "atributos de H []\n",
      "ENTAILMENT\n",
      "si esta get\n",
      "atributos de T ['when', 'have']\n",
      "atributos de H ['when', 'have']\n",
      "si esta when\n",
      "si esta have\n",
      "ENTAILMENT\n",
      "si esta snow\n",
      "atributos de T []\n",
      "atributos de H ['no']\n",
      "CONTRADICTION\n",
      "si esta to\n",
      "atributos de T []\n",
      "atributos de H []\n",
      "ENTAILMENT\n",
      "si esta learn\n",
      "atributos de T ['to']\n",
      "atributos de H ['to']\n",
      "si esta to\n",
      "ENTAILMENT\n",
      "si esta sport\n",
      "atributos de T []\n",
      "atributos de H []\n",
      "ENTAILMENT\n",
      "si esta so\n",
      "atributos de T []\n",
      "atributos de H []\n",
      "ENTAILMENT\n",
      "si esta look\n",
      "atributos de T ['so', 'at']\n",
      "atributos de H ['so', 'at']\n",
      "si esta so\n",
      "si esta at\n",
      "ENTAILMENT\n",
      "si esta different\n",
      "atributos de T []\n",
      "atributos de H []\n",
      "ENTAILMENT\n",
      "si esta way\n",
      "atributos de T ['different']\n",
      "atributos de H ['different']\n",
      "si esta different\n",
      "ENTAILMENT\n",
      "si esta mimic\n",
      "atributos de T ['could', 'without']\n",
      "atributos de H ['could', 'without']\n",
      "si esta could\n",
      "si esta without\n",
      "ENTAILMENT\n",
      "----------------------------------------------\n",
      "Contenidas ['when', 'get', 'to', 'learn', 'sport', 'so', 'look', 'different', 'way', 'mimic']\n",
      "Faltantes []\n",
      "Contradiccion ['no snow']\n",
      "Contradiccion2 []\n",
      "[('when', 'same', 'when'), ('get', 'same', 'get'), ('to', 'same', 'to'), ('learn', 'same', 'learn'), ('sport', 'same', 'sport'), ('so', 'same', 'so'), ('look', 'same', 'look'), ('different', 'same', 'different'), ('way', 'same', 'way'), ('mimic', 'same', 'mimic')]\n",
      "[('when', 'same', 'when'), ('get', 'same', 'get'), ('to', 'same', 'to'), ('learn', 'same', 'learn'), ('sport', 'same', 'sport'), ('so', 'same', 'so'), ('look', 'same', 'look'), ('different', 'same', 'different'), ('way', 'same', 'way'), ('mimic', 'same', 'mimic')]\n",
      "Matriz original\n",
      "           look  without   get  have  mimic  could    so   way    no    at  \\\n",
      "look       1.00     0.01  0.24  0.25   0.18   0.24  0.26  0.21  0.06  0.22   \n",
      "without    0.01     1.00  0.16  0.26   0.00   0.32  0.19  0.15  0.52  0.19   \n",
      "get        0.24     0.16  1.00  0.46   0.03   0.33  0.28  0.22  0.07  0.16   \n",
      "have       0.25     0.26  0.46  1.00   0.05   0.62  0.44  0.12  0.33  0.17   \n",
      "mimic      0.18     0.00  0.03  0.05   1.00   0.08  0.00  0.01  0.00  0.00   \n",
      "could      0.24     0.32  0.33  0.62   0.08   1.00  0.49  0.30  0.32  0.22   \n",
      "so         0.26     0.19  0.28  0.44   0.00   0.49  1.00  0.34  0.32  0.28   \n",
      "way        0.21     0.15  0.22  0.12   0.01   0.30  0.34  1.00  0.09  0.18   \n",
      "at         0.22     0.19  0.16  0.17   0.00   0.22  0.28  0.18  0.14  1.00   \n",
      "to         0.22     0.19  0.39  0.34   0.05   0.51  0.38  0.37  0.10  0.44   \n",
      "learn      0.26     0.00  0.37  0.24   0.21   0.19  0.11  0.14  0.00  0.11   \n",
      "different  0.11     0.05  0.05  0.17   0.18   0.12  0.09  0.19  0.13  0.08   \n",
      "when       0.09     0.22  0.18  0.23   0.01   0.37  0.53  0.25  0.17  0.41   \n",
      "sport      0.06     0.00  0.01  0.03   0.00   0.00  0.01  0.01  0.00  0.00   \n",
      "snow       0.00     0.00  0.01  0.00   0.00   0.00  0.01  0.04  0.01  0.00   \n",
      "\n",
      "             to  learn  different  when  sport  snow  \n",
      "look       0.22   0.26       0.11  0.09   0.06  0.00  \n",
      "without    0.19   0.00       0.05  0.22   0.00  0.00  \n",
      "get        0.39   0.37       0.05  0.18   0.01  0.01  \n",
      "have       0.34   0.24       0.17  0.23   0.03  0.00  \n",
      "mimic      0.05   0.21       0.18  0.01   0.00  0.00  \n",
      "could      0.51   0.19       0.12  0.37   0.00  0.00  \n",
      "so         0.38   0.11       0.09  0.53   0.01  0.01  \n",
      "way        0.37   0.14       0.19  0.25   0.01  0.04  \n",
      "at         0.44   0.11       0.08  0.41   0.00  0.00  \n",
      "to         1.00   0.30       0.06  0.34   0.00  0.01  \n",
      "learn      0.30   1.00       0.18  0.13   0.04  0.00  \n",
      "different  0.06   0.18       1.00  0.17   0.01  0.00  \n",
      "when       0.34   0.13       0.17  1.00   0.00  0.04  \n",
      "sport      0.00   0.04       0.01  0.00   1.00  0.12  \n",
      "snow       0.01   0.00       0.00  0.04   0.12  1.00  \n",
      "Matriz quitando generalidad\n",
      "           without  have  could    no    at  snow\n",
      "look          0.01  0.25   0.24  0.06  0.22  0.00\n",
      "without       1.00  0.26   0.32  0.52  0.19  0.00\n",
      "get           0.16  0.46   0.33  0.07  0.16  0.01\n",
      "have          0.26  1.00   0.62  0.33  0.17  0.00\n",
      "mimic         0.00  0.05   0.08  0.00  0.00  0.00\n",
      "could         0.32  0.62   1.00  0.32  0.22  0.00\n",
      "so            0.19  0.44   0.49  0.32  0.28  0.01\n",
      "way           0.15  0.12   0.30  0.09  0.18  0.04\n",
      "at            0.19  0.17   0.22  0.14  1.00  0.00\n",
      "to            0.19  0.34   0.51  0.10  0.44  0.01\n",
      "learn         0.00  0.24   0.19  0.00  0.11  0.00\n",
      "different     0.05  0.17   0.12  0.13  0.08  0.00\n",
      "when          0.22  0.23   0.37  0.17  0.41  0.04\n",
      "sport         0.00  0.03   0.00  0.00  0.00  0.12\n",
      "snow          0.00  0.00   0.00  0.01  0.00  1.00\n",
      "4.757\n",
      "4.56\n",
      "Matriz quitando contradiction\n",
      "           look  without   get  have  mimic  could    so   way    no    at  \\\n",
      "look       1.00     0.01  0.24  0.25   0.18   0.24  0.26  0.21  0.06  0.22   \n",
      "without    0.01     1.00  0.16  0.26   0.00   0.32  0.19  0.15  0.52  0.19   \n",
      "get        0.24     0.16  1.00  0.46   0.03   0.33  0.28  0.22  0.07  0.16   \n",
      "have       0.25     0.26  0.46  1.00   0.05   0.62  0.44  0.12  0.33  0.17   \n",
      "mimic      0.18     0.00  0.03  0.05   1.00   0.08  0.00  0.01  0.00  0.00   \n",
      "could      0.24     0.32  0.33  0.62   0.08   1.00  0.49  0.30  0.32  0.22   \n",
      "so         0.26     0.19  0.28  0.44   0.00   0.49  1.00  0.34  0.32  0.28   \n",
      "way        0.21     0.15  0.22  0.12   0.01   0.30  0.34  1.00  0.09  0.18   \n",
      "at         0.22     0.19  0.16  0.17   0.00   0.22  0.28  0.18  0.14  1.00   \n",
      "to         0.22     0.19  0.39  0.34   0.05   0.51  0.38  0.37  0.10  0.44   \n",
      "learn      0.26     0.00  0.37  0.24   0.21   0.19  0.11  0.14  0.00  0.11   \n",
      "different  0.11     0.05  0.05  0.17   0.18   0.12  0.09  0.19  0.13  0.08   \n",
      "when       0.09     0.22  0.18  0.23   0.01   0.37  0.53  0.25  0.17  0.41   \n",
      "sport      0.06     0.00  0.01  0.03   0.00   0.00  0.01  0.01  0.00  0.00   \n",
      "snow       0.00     0.00  0.01  0.00   0.00   0.00  0.01  0.04  0.01  0.00   \n",
      "\n",
      "             to  learn  different  when  sport  snow  \n",
      "look       0.22   0.26       0.11  0.09   0.06  0.00  \n",
      "without    0.19   0.00       0.05  0.22   0.00  0.00  \n",
      "get        0.39   0.37       0.05  0.18   0.01  0.01  \n",
      "have       0.34   0.24       0.17  0.23   0.03  0.00  \n",
      "mimic      0.05   0.21       0.18  0.01   0.00  0.00  \n",
      "could      0.51   0.19       0.12  0.37   0.00  0.00  \n",
      "so         0.38   0.11       0.09  0.53   0.01  0.01  \n",
      "way        0.37   0.14       0.19  0.25   0.01  0.04  \n",
      "at         0.44   0.11       0.08  0.41   0.00  0.00  \n",
      "to         1.00   0.30       0.06  0.34   0.00  0.01  \n",
      "learn      0.30   1.00       0.18  0.13   0.04  0.00  \n",
      "different  0.06   0.18       1.00  0.17   0.01  0.00  \n",
      "when       0.34   0.13       0.17  1.00   0.00  0.04  \n",
      "sport      0.00   0.04       0.01  0.00   1.00  0.12  \n",
      "snow       0.01   0.00       0.00  0.04   0.12  1.00  \n",
      "0\n",
      "4.83\n",
      "Matriz con los que no hicieron match\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: [look, without, get, have, mimic, could, so, way, at, to, learn, different, when, sport, snow]\n",
      "proceso lexico\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: [look, without, get, have, mimic, could, so, way, at, to, learn, different, when, sport, snow] Index([], dtype='object')\n",
      "0.6666666666666666\n",
      "0.06666666666666667\n",
      "0.0\n",
      "Matriz con los que no hicieron match\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: [look, without, get, have, mimic, could, so, way, at, to, learn, different, when, sport, snow]\n",
      "proceso de obtención de no relaciones\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: [look, without, get, have, mimic, could, so, way, at, to, learn, different, when, sport, snow] Index([], dtype='object')\n",
      "rel [1 1 1 1 1 1 1 1 1 1 0]\n",
      "entropia final 0.439\n",
      "Tiempo que se llevo: 1.46  segundos\n"
     ]
    }
   ],
   "source": [
    "inicio = time.time()\n",
    "#for i in range(len(textos)):\n",
    "for i in range(4):\n",
    "    print(i)\n",
    "    \n",
    "    texto_i=str(textos[i])\n",
    "    hipotesis_i=str(hipotesis[i])\n",
    "    \n",
    "    print(texto_i)\n",
    "    r_t,t_clean_m=ptxt.representacion_entidadesDavid(nlp,texto_i)\n",
    "    print(r_t,t_clean_m)\n",
    "    \n",
    "    print(hipotesis_i)\n",
    "    r_h,h_clean_m = ptxt.representacion_entidadesDavid(nlp,hipotesis_i)\n",
    "    print(r_h,h_clean_m)\n",
    "    \n",
    "    new_data['dicEntT'].append(r_t)\n",
    "    new_data['dicEntH'].append(r_h)\n",
    "    \n",
    "    #Revisar si es numerico la hipótesis\n",
    "    \n",
    "    if(type(hipotesis[i])==type(1.0) or type(textos[i])==type(1.0)):\n",
    "        print(\"Falla\")\n",
    "        new_data['clases'].append(9)\n",
    "        process_=False\n",
    "    else:\n",
    "        print(\"clase\",clases[i])\n",
    "        new_data['clases'].append(clases[i])\n",
    "        process_=True\n",
    "        \n",
    "    # obtener los lemmas de las frases en listas\n",
    "    #s1=ut.get_lemmasALL_(texto_i,nlp)\n",
    "    #s2=ut.get_lemmasALL_(hipotesis_i,nlp)\n",
    "    if(process_):\n",
    "        s1 = t_clean_m[:]\n",
    "        data=list(r_t.values())\n",
    "        print(\"valores de diccionario\",data)\n",
    "        for ps in data:\n",
    "            s1.extend(ptxt.eliminacion_espacios(ps.split(\",\")))\n",
    "        #s1.extend()\n",
    "        s2 = h_clean_m[:]\n",
    "        data=list(r_h.values())\n",
    "        print(\"valores de diccionario\",data)\n",
    "        for ps in data:\n",
    "            s2.extend(ptxt.eliminacion_espacios(ps.split(\",\")))\n",
    "        \n",
    "        s1=list(set(s1))\n",
    "        s2=list(set(s2))\n",
    "        \n",
    "        tss1=s1[:]\n",
    "        tss2=s2[:]\n",
    "        \n",
    "        # obtener todos los sinonimos\n",
    "        sinT=[]\n",
    "        antT=[]\n",
    "        HipT=[]\n",
    "        sinH=[]\n",
    "        antH=[]\n",
    "\n",
    "        HipH=[]\n",
    "        hipH=[]\n",
    "        relT=[]\n",
    "        relH=[]\n",
    "        \n",
    "        # # encontrar bolsa de sinonimos de cada token\n",
    "        for t in s1:\n",
    "            if t in cn.df_diccionario_generales:\n",
    "                sinT.append((cn.df_diccionario_generales[t][\"synonym\"]).union(cn.df_diccionario_generales[t][\"derived_from\"]))\n",
    "                antT.append((cn.df_diccionario_generales[t][\"antonym\"]).union(cn.df_diccionario_generales[t][\"distinct_from\"]))\n",
    "                relT.append((cn.df_diccionario_generales[t][\"related_to\"]).union(cn.df_diccionario_generales[t][\"similar_to\"]))\n",
    "                temp_set=set()\n",
    "                for p_g in cn.relaciones_generales1:\n",
    "                    temp_set=temp_set.union(cn.df_diccionario_generales[t][p_g])\n",
    "                HipT.append(temp_set)\n",
    "            else:\n",
    "                sinT.append(set(t))\n",
    "                HipT.append(set(t))\n",
    "                antT.append(set(t))\n",
    "                relT.append(set(t))\n",
    "        for h in s2:\n",
    "            if h in cn.df_diccionario_generales:\n",
    "                sinH.append((cn.df_diccionario_generales[h][\"synonym\"]).union(cn.df_diccionario_generales[h][\"derived_from\"]))\n",
    "                antH.append((cn.df_diccionario_generales[h][\"antonym\"]).union(cn.df_diccionario_generales[h][\"distinct_from\"]))\n",
    "                relH.append((cn.df_diccionario_generales[h][\"related_to\"]).union(cn.df_diccionario_generales[h][\"similar_to\"]))\n",
    "                temp_set=set()\n",
    "                # se usan las mismas relacines pero en el otro sentido para la hiponimia\n",
    "                for p_g in cn.relaciones_generales1:\n",
    "                    temp_set=temp_set.union(cn.df_diccionario_especificas[h][p_g])\n",
    "                hipH.append(temp_set)\n",
    "            else:\n",
    "                sinH.append(set(h))\n",
    "                hipH.append(set(h))\n",
    "                antH.append(set(h))\n",
    "                relH.append(set(h))\n",
    "                \n",
    "        print(sinT)\n",
    "        print(sinH)\n",
    "                \n",
    "        print(r_t)\n",
    "        print(r_h)\n",
    "        # reemplazar sinonimos en las frases para la BoW \n",
    "        print(\"-----------\")\n",
    "        print(s1)\n",
    "        print(s2)\n",
    "        print(r_t)\n",
    "        print(r_h)\n",
    "        # for nt in range(len(s1)):\n",
    "        #     for nh in range(len(sinH)):\n",
    "        #         if(s1[nt] in sinH[nh]):\n",
    "        #             print(\"Revision de reemplazo\",s1[nt],s2[nh],sinH[nh])\n",
    "        #             if(s1[nt]!=s2[nh]):\n",
    "        #                 print(s1[nt],s2[nh])\n",
    "        #                 if s1[nt] in r_t and s2[nh] in r_t:\n",
    "        #                     r_t[s2[nh]]=r_t[s1[nt]]\n",
    "        #                     r_t.pop(s1[nt])\n",
    "        #                     s1[nt]=s2[nh]\n",
    "        print(s1)\n",
    "        print(s2)\n",
    "        for nh in range(len(s2)):\n",
    "            for nt in range(len(sinT)):\n",
    "                if(s2[nh] in sinT[nt]):\n",
    "                    print(\"Revision de reemplazo\",s2[nh],s1[nt],sinT[nt])\n",
    "                    if(s2[nh]!=s1[nt]):\n",
    "                        print(s2[nh],s1[nt])\n",
    "                        #if s1[nt] in r_h and s2[nh] in r_h:\n",
    "                            #r_h[s1[nt]]=r_h[s2[nh]]\n",
    "                            #r_h.pop(s2[nh])\n",
    "                        s2[nh]=s1[nt]\n",
    "        # for nh in range(len(s2)):\n",
    "        #     for nt in range(len(HipT)):\n",
    "        #         if(s2[nh] in HipT[nt]):\n",
    "        #             print(\"Revision de reemplazo\",s2[nh],s1[nt],HipT[nt])\n",
    "        #             if(s2[nh]!=s1[nt]):\n",
    "        #                 print(s2[nh],s1[nt])\n",
    "        #                 if s1[nt] in r_h and s2[nh] in r_h:\n",
    "        #                     r_h[s1[nt]]=r_h[s2[nh]]\n",
    "        #                     r_h.pop(s2[nh])\n",
    "        #                     s2[nh]=s1[nt]\n",
    "                    \n",
    "        v_vocabulario=set(s1).union(set(s2))\n",
    "        t_bow=[]\n",
    "        h_bow=[]\n",
    "        for e in v_vocabulario:\n",
    "            t_bow.append(s1.count(e))\n",
    "            h_bow.append(s2.count(e))\n",
    "        \n",
    "        print(t_bow)\n",
    "        print(h_bow)\n",
    "        \n",
    "        resultBoW = 1 - spatial.distance.cosine(t_bow, h_bow)\n",
    "        print(\"BoW: \",resultBoW)\n",
    "        new_data['simBoWrel'].append(resultBoW)\n",
    "       \n",
    "        # Medidas con Jaro-Winckler\n",
    "        tp1,tp2=cn.jaro_distance(s1, s2,sinT,sinH,HipT,hipH)\n",
    "        new_data['Jaro-Winkler_rit'].append(tp2)\n",
    "        new_data['Jaro-Winkler_rit1'].append(tp1)\n",
    "\n",
    "        print(\"jaro con antonimos\")\n",
    "        tp1_c,tp2_c=cn.jaro_distanceC(s1, s2,antT,antH)\n",
    "        print(tp1_c,tp2_c)\n",
    "        new_data['Jaro-Winkler_ritN'].append(tp2_c)\n",
    "        new_data['Jaro-Winkler_ritN1'].append(tp1_c)\n",
    "\n",
    "        print(\"jaro con relacionadas\")\n",
    "        tp1_c_,tp2_c_=cn.jaro_distanceC(s1, s2,relT,relH)\n",
    "        print(tp1_c_,tp2_c_)\n",
    "        new_data['Jaro-Winkler_ritN_'].append(tp2_c_)\n",
    "        new_data['Jaro-Winkler_ritN1_'].append(tp1_c_)\n",
    "\n",
    "        tp1_c__,tp2_c__=cn.jaro_distanceP(s1, s2,sinT,sinH,HipT,hipH,antT,antH,relT,relH)\n",
    "        print(tp1_c__,tp2_c__)\n",
    "        new_data['Jaro-Winkler_ritN__'].append(tp2_c__)\n",
    "        new_data['Jaro-Winkler_ritN1__'].append(tp1_c__)\n",
    "        print(r_t,t_clean_m)\n",
    "        print(r_h,h_clean_m)\n",
    "\n",
    "        neg_t=ptxt.obtener_negaciones(nlp,texto_i)\n",
    "        new_data['negT'].append(len(neg_t))\n",
    "        print(\"negT\",neg_t)\n",
    "        neg_h=ptxt.obtener_negaciones(nlp,hipotesis_i)\n",
    "        new_data['negH'].append(len(neg_h))\n",
    "        print(\"negH\",neg_h)\n",
    "        \n",
    "        if len(set(h_clean_m))!=0 and len(set(t_clean_m))!=0:\n",
    "            new_data['jaccard'].append(len(set(t_clean_m).intersection(set(h_clean_m)))/len(set(h_clean_m)))\n",
    "            print(len(set(t_clean_m).intersection(set(h_clean_m)))/len(set(h_clean_m)))\n",
    "        else:\n",
    "            new_data['jaccard'].append(0)\n",
    "\n",
    "        print(\"s1\",s1)\n",
    "        print(\"s2\",s2)\n",
    "        if len(set(s1))!=0 and len(set(s2))!=0:\n",
    "            new_data['jaccard_rel'].append(len(set(s1).intersection(set(s2)))/len(set(s2)))\n",
    "            print(len(set(s1).intersection(set(s2)))/len(set(s2)))\n",
    "        else:\n",
    "            new_data['jaccard_rel'].append(0)\n",
    "\n",
    "        ##########################################################  \n",
    "        # obtener los datos de los tokens en orden para la creación de la matriz inicial\n",
    "        \n",
    "        s1=tss1\n",
    "        s2=tss2\n",
    "        \n",
    "        t_lem=[]\n",
    "        h_lem=[]\n",
    "        for a in s1:\n",
    "            if a not in t_lem:\n",
    "                t_lem.append(a)\n",
    "        for a in s2:\n",
    "            if a not in h_lem:\n",
    "                h_lem.append(a)\n",
    "        \n",
    "        t_vectors=ut.get_matrix_rep2(t_lem, nlp, normed=False)\n",
    "        h_vectors=ut.get_matrix_rep2(h_lem, nlp, normed=False)\n",
    "        t_vectors_n=ut.get_matrix_rep2(t_lem, nlp, normed=True)\n",
    "        h_vectors_n=ut.get_matrix_rep2(h_lem, nlp, normed=True)\n",
    "        \n",
    "        print(\"t_lem\",t_lem)\n",
    "        print(\"h_lem\",h_lem)\n",
    "        \n",
    "        redondeo=2\n",
    "        ma_n=np.dot(t_vectors_n,h_vectors_n.T)\n",
    "        ma_n = np.clip(ma_n, 0, 1).round(redondeo)\n",
    "        \n",
    "        ma=pd.DataFrame(ma_n,index=t_lem,columns=h_lem)\n",
    "        print(ma)\n",
    "        \n",
    "        m_earth,m_mi=utils.wasserstein_mutual_infF(nlp,t_vectors_n,h_vectors_n,t_lem,h_lem)\n",
    "        \n",
    "        distY = ma.round(1).values.flatten()\n",
    "        new_data['entropia_total'].append(ut.entropia(distY))\n",
    "\n",
    "        new_data['max_info_t'].append(ma.max().sum()/(ma.shape[1]))#\n",
    "        new_data['sumas_t'].append(ma.sum().sum()/((ma.shape[1]*(ma.shape[0]))))#\n",
    "        new_data['mearts_t'].append(m_earth.min().sum()/(ma.shape[1]))# \n",
    "        new_data['mutinf_t'].append(m_mi.max().sum()/(ma.shape[1]))# \n",
    "        # vamos a crear una copia para aplicarles diferentes procesos de eliminacion dependiendo el grupo\n",
    "        # una para cada grupo\n",
    "        ma_original = ma.copy()\n",
    "        simC1 = len(np.where(ma_original.values >= 0.8)[0])\n",
    "        simC2 = len(np.where((ma_original.values >= 0.5) & (ma_original.values < 0.8))[0])\n",
    "        simC3 = len(np.where(ma_original.values < 0.5)[0])\n",
    "\n",
    "        distXSim=[]\n",
    "\n",
    "        distX = ma.round(1).values.flatten()\n",
    "        for m_sim in range(simC1):\n",
    "            distXSim.append(1)\n",
    "        for m_sim in range(simC2):\n",
    "            distXSim.append(2)\n",
    "        for m_sim in range(simC3):\n",
    "            distXSim.append(0)\n",
    "\n",
    "        print(\"rel nueva categoria\",np.array(distXSim).round(1))\n",
    "\n",
    "        new_data['entSimilitud'].append(utils.entropia(np.array(distXSim).round(1)))  \n",
    "\n",
    "        # proceso de eliminación de entidades y overlap las cosas que están compartiendo en caso de faltar por eliminar a la matriz \n",
    "        lista_entidades_no_match=[]\n",
    "        lista_entidades_distintas=[]\n",
    "        lista_entidades_contra=[]\n",
    "        lista_entidades_contenidas=[]\n",
    "        \n",
    "        lista_relaciones_grupos=[]\n",
    "        lista_rel_G1=[]\n",
    "        lista_rel_G2=[]\n",
    "        lista_rel_G3=[]\n",
    "        lista_rel_G4=[]\n",
    "        # conteo de relaciones de compatibilidad e incompatibilidad\n",
    "        c_compatibilidad=0\n",
    "        c_incompatibilidad=0\n",
    "        c_rel_concep=0\n",
    "            \n",
    "        for clave in r_h.keys():\n",
    "            #print(\"hipotesis\",clave)\n",
    "            if clave in r_t:\n",
    "                print(\"si esta\",clave)\n",
    "                t_atributos = ptxt.eliminacion_espacios(r_t[clave].split(\",\"))\n",
    "                h_atributos = ptxt.eliminacion_espacios(r_h[clave].split(\",\"))\n",
    "                print(\"atributos de T\",t_atributos)\n",
    "                print(\"atributos de H\",h_atributos)\n",
    "                if \"no\" in h_atributos or \"not\" in h_atributos or \"no\" in t_atributos or \"not\" in t_atributos:\n",
    "                    print(\"CONTRADICTION\")\n",
    "                    c_incompatibilidad+=1\n",
    "                    lista_rel_G2.append((clave,\"distinct_from\",\"no \"+clave))\n",
    "                    lista_entidades_distintas.append(\"no \"+clave)\n",
    "                elif len(h_atributos)>0:\n",
    "                    matches=0\n",
    "                    for h_a in h_atributos:\n",
    "                        if h_a in t_atributos:\n",
    "                            print(\"si esta\",h_a)\n",
    "                            matches+=1\n",
    "                        else:\n",
    "                            print(\"busqueda\",h_a)\n",
    "                            att_found=False\n",
    "                            for attT in t_atributos:\n",
    "                                ctat,rctat=cn.relacion_contra(h_a,attT)\n",
    "                                if(ctat):\n",
    "                                    print(\"CONTRADICTION_c\")\n",
    "                                    lista_entidades_distintas.append(h_a+\" \"+clave)\n",
    "                                    lista_entidades_contra.append(clave)\n",
    "                                    lista_entidades_contra.append(h_a)\n",
    "                                    lista_relaciones_grupos.append((attT+\" \"+clave,\"distinct_from\",h_a+\" \"+clave))\n",
    "                                    lista_rel_G2.append((attT+\" \"+clave,\"distinct_from\",h_a+\" \"+clave))\n",
    "                                    c_incompatibilidad+=1\n",
    "                                    bandera=False\n",
    "                                    break\n",
    "                                elif cn.relacion_entailmentF(h_a,attT):\n",
    "                                    att_found=True\n",
    "                                    print(\"se encontró en relacion entailment\",attT)\n",
    "                                    break\n",
    "                            if att_found:\n",
    "                                matches+=1\n",
    "                    if matches==len(h_atributos):\n",
    "                        print(\"ENTAILMENT\")\n",
    "                        c_compatibilidad+=1\n",
    "                        lista_entidades_contenidas.append(clave)\n",
    "                        lista_relaciones_grupos.append((clave,\"same\",clave))\n",
    "                        lista_rel_G1.append((clave,\"same\",clave))\n",
    "                    # else:\n",
    "                    #     print(\"Entidad tiene más atributos - NEUTRAL\")\n",
    "                    #     lista_entidades_no_match.append(clave)\n",
    "                else:\n",
    "                    print(\"ENTAILMENT\")\n",
    "                    c_compatibilidad+=1\n",
    "                    lista_relaciones_grupos.append((clave,\"same\",clave))\n",
    "                    lista_rel_G1.append((clave,\"same\",clave))\n",
    "                    lista_entidades_contenidas.append(clave)\n",
    "            else:\n",
    "                bandera=True\n",
    "                for entT in list(r_t.keys()):\n",
    "                    ct,rct=cn.relacion_contra(clave,entT)\n",
    "                    if(cn.relacion_noentailmentF(clave,entT)):\n",
    "                        print(\"CONTRADICTION\")\n",
    "                        lista_entidades_distintas.append(clave)\n",
    "                        lista_entidades_contra.append(clave)\n",
    "                        lista_relaciones_grupos.append((entT,\"distinct_from\",clave))\n",
    "                        lista_rel_G2.append((entT,\"distinct_from\",clave))\n",
    "                        c_incompatibilidad+=1\n",
    "                        bandera=False\n",
    "                        break\n",
    "                    elif(ct):\n",
    "                        print(\"CONTRADICTION_c\")\n",
    "                        lista_entidades_distintas.append(clave)\n",
    "                        lista_entidades_contra.append(clave)\n",
    "                        lista_relaciones_grupos.append((entT,\"distinct_from\",clave))\n",
    "                        lista_rel_G2.append((entT,\"distinct_from\",clave))\n",
    "                        bandera=False\n",
    "                        c_incompatibilidad+=1\n",
    "                        break\n",
    "                    elif cn.relacion_entailmentF(clave,entT):\n",
    "                        matches=0\n",
    "                        t_atributos = ptxt.eliminacion_espacios(r_t[entT].split(\",\"))\n",
    "                        h_atributos = ptxt.eliminacion_espacios(r_h[clave].split(\",\"))\n",
    "                        print(\"atributos de T\",t_atributos)\n",
    "                        print(\"atributos de H\",h_atributos)\n",
    "                        if \"no\" in h_atributos or \"no\" in t_atributos or \"not\" in t_atributos:\n",
    "                            print(\"CONTRADICTION\")\n",
    "                            lista_entidades_distintas.append(clave)\n",
    "                            lista_relaciones_grupos.append((entT,\"distinct_from\",clave))\n",
    "                            lista_rel_G2.append((entT,\"distinct_from\",clave))\n",
    "                            bandera=False\n",
    "                            c_incompatibilidad+=1\n",
    "                            break\n",
    "                        elif len(h_atributos)>0:\n",
    "                            for h_a in h_atributos:\n",
    "                                if h_a in t_atributos:\n",
    "                                    print(\"si esta\",h_a)\n",
    "                                    matches+=1\n",
    "                                else:\n",
    "                                    print(\"busqueda\",h_a)\n",
    "                                    att_found=False\n",
    "                                    for attT in t_atributos:\n",
    "                                        ctat,rctat=cn.relacion_contra(h_a,attT)\n",
    "                                        if(ctat):\n",
    "                                            print(\"CONTRADICTION_c\")\n",
    "                                            lista_entidades_distintas.append(clave)\n",
    "                                            lista_entidades_distintas.append(h_a)\n",
    "                                            lista_relaciones_grupos.append((entT,\"distinct_from\",clave))\n",
    "                                            lista_rel_G2.append((entT,\"distinct_from\",clave))\n",
    "                                            bandera=False\n",
    "                                            c_incompatibilidad+=1\n",
    "                                            break\n",
    "                                        elif cn.relacion_entailmentF(h_a,attT):\n",
    "                                            att_found=True\n",
    "                                            print(\"se encontro en relacion entailment\",attT)\n",
    "                                            break\n",
    "                                    if att_found:\n",
    "                                        matches+=1\n",
    "                            if matches==len(h_atributos):\n",
    "                                print(\"ENTAILMENT\")\n",
    "                                lista_entidades_contenidas.append(clave)\n",
    "                                bandera=False\n",
    "                                c_compatibilidad+=len(h_atributos)+1\n",
    "                                break\n",
    "                        else:\n",
    "                            print(\"ENTAILMENT\")\n",
    "                            lista_entidades_contenidas.append(clave)\n",
    "                            lista_rel_G1.append((clave,\"same\",entT))\n",
    "                            bandera=False\n",
    "                            c_compatibilidad+=1\n",
    "                            break\n",
    "                if(bandera):\n",
    "                    print(\"no esta\",clave)\n",
    "                    if clave not in lista_entidades_no_match:\n",
    "                        lista_entidades_no_match.append(clave)\n",
    "                    #break\n",
    "        print(\"----------------------------------------------\")\n",
    "        lista_entidades_no_match = list(set(lista_entidades_no_match).difference(lista_entidades_contra))\n",
    "        print(\"Contenidas\",lista_entidades_contenidas)\n",
    "        print(\"Faltantes\",lista_entidades_no_match)\n",
    "        print(\"Contradiccion\",lista_entidades_distintas)\n",
    "        print(\"Contradiccion2\",lista_entidades_contra)\n",
    "        \n",
    "        if len(lista_entidades_distintas)>0:\n",
    "            new_data[\"relation\"].append(-1)\n",
    "            #new_data[\"relation\"].append(\"CONTRADICTION\")\n",
    "        elif len(lista_entidades_no_match)>0:\n",
    "            new_data[\"relation\"].append(2)\n",
    "            #new_data[\"relation\"].append(\"NEUTRAL\")\n",
    "        else:\n",
    "            #new_data[\"relation\"].append(\"ENTAILMENT\")\n",
    "            new_data[\"relation\"].append(1)\n",
    "        \n",
    "        if len(h_clean_m)==0:\n",
    "            new_data[\"overlap_ent\"].append(0)\n",
    "            new_data[\"no_matcheadas\"].append(0)\n",
    "            new_data[\"contradiction\"].append(0)\n",
    "        else:\n",
    "            new_data[\"overlap_ent\"].append(len(lista_entidades_contenidas)/len(h_clean_m))\n",
    "            new_data[\"no_matcheadas\"].append(len(lista_entidades_no_match)/len(h_clean_m))\n",
    "            new_data[\"contradiction\"].append(len(lista_entidades_distintas)/len(h_clean_m))\n",
    "        \n",
    "        ###############################################################################\n",
    "            \n",
    "        print(lista_rel_G1)\n",
    "        print(lista_relaciones_grupos)\n",
    "        \n",
    "        print(\"Matriz original\")\n",
    "        print(ma)\n",
    "        \n",
    "        ma_generalidad = ma_original.drop(list(set(lista_entidades_contenidas)),errors='ignore',axis=1)\n",
    "        print(\"Matriz quitando generalidad\")\n",
    "        print(ma_generalidad)\n",
    "        print(ut.entropia(ma_original[list(set(lista_entidades_contenidas))].values.flatten()))\n",
    "        new_data[\"origE\"].append(ut.entropia(ma_original[list(set(lista_entidades_contenidas))].values.flatten()))\n",
    "        print(ut.entropia(ma_generalidad.values.flatten()))\n",
    "        new_data[\"genE\"].append(ut.entropia(ma_generalidad.values.flatten()))\n",
    "        \n",
    "        ma_contradiction = ma_original.drop(list(set(lista_entidades_contra)),errors='ignore',axis=1)\n",
    "        print(\"Matriz quitando contradiction\")\n",
    "        print(ma_contradiction)\n",
    "        print(ut.entropia(ma_original[list(set(lista_entidades_contra))].values.flatten()))\n",
    "        new_data[\"oricE\"].append(ut.entropia(ma_original[list(set(lista_entidades_contra))].values.flatten()))\n",
    "        print(ut.entropia(ma_contradiction.values.flatten()))\n",
    "        new_data[\"conE\"].append(ut.entropia(ma_contradiction.values.flatten()))\n",
    "        \n",
    "        \n",
    "        ma = ma_original[list(set(lista_entidades_no_match))]\n",
    "        print(\"Matriz con los que no hicieron match\")\n",
    "        print(ma)\n",
    "        \n",
    "        ############# Proceso léxico de las cosas restantes\n",
    "        \n",
    "        top_k=3\n",
    "        # # #PARA REVISAR SI EXISTEN RELACIONES DE SIMILITUD SEMÁNTICA A TRAVÉS DEL USO DE CONCEPNET\n",
    "        print(\"proceso lexico\")\n",
    "        print(ma,ma.columns)\n",
    "        borrar_g=[]\n",
    "        borrar_c=[]\n",
    "        borrar_e=[]\n",
    "        for c_c in ma.columns:\n",
    "            print(\"columna a checar\",c_c)\n",
    "            # filtrar el top 3 de los mejores similitud coseno para cada token de H vs tokens de T que sean mayores a 0\n",
    "            # una vez que encontremos quien se sale del ciclo\n",
    "            temp=ma[c_c].sort_values(ascending=False)\n",
    "            ranks=list(temp[:top_k].index)\n",
    "            valranks=list(temp[:top_k].values)\n",
    "            for r_i in range(len(ranks)):\n",
    "                print(\"acces\",c_c,ranks[r_i],valranks[r_i])\n",
    "                if valranks[r_i]>0:\n",
    "                    r_wt=str(ranks[r_i])\n",
    "                    r_wh=str(c_c)\n",
    "                    verdad1,tipoG1=cn.relacion_entail(r_wt,r_wh)\n",
    "                    verdad2,tipoG2=cn.relacion_contra(r_wt,r_wh)\n",
    "                    verdad3,tipoG3=cn.relacion_especifica(r_wt,r_wh)\n",
    "                    if(verdad2):\n",
    "                        c_incompatibilidad+=1\n",
    "                        lista_relaciones_grupos.append(tipoG2)\n",
    "                        lista_rel_G2.append(tipoG2)\n",
    "                        borrar_c.append(r_wh)\n",
    "                        print(tipoG2)\n",
    "                        #break\n",
    "                    elif(verdad1):\n",
    "                        c_compatibilidad+=1\n",
    "                        lista_relaciones_grupos.append(tipoG1)\n",
    "                        lista_rel_G1.append(tipoG1)\n",
    "                        borrar_g.append(r_wh)\n",
    "                        print(tipoG1)\n",
    "                        #break\n",
    "                    elif(verdad3):\n",
    "                        c_rel_concep+=1\n",
    "                        lista_relaciones_grupos.append(tipoG3)\n",
    "                        lista_rel_G3.append(tipoG3)\n",
    "                        borrar_e.append(r_wh)\n",
    "                        print(tipoG3)\n",
    "                        \n",
    "        print(c_compatibilidad/ma_original.shape[0])\n",
    "        print(c_incompatibilidad/ma_original.shape[0])\n",
    "        print(c_rel_concep/ma_original.shape[0])\n",
    "        \n",
    "        if(len(borrar_g)>0):\n",
    "            ma_generalidad = ma_generalidad.drop(borrar_g,errors='ignore',axis=1)\n",
    "            print(\"Matriz quitando generalidad\")\n",
    "            print(ma_generalidad)\n",
    "            pred=list(set(lista_entidades_contenidas))\n",
    "            pred.extend(borrar_g)\n",
    "            print(ut.entropia(ma_original[pred].values.flatten()))\n",
    "            new_data[\"origM\"].append(ut.entropia(ma_original[pred].values.flatten()))\n",
    "            print(ut.entropia(ma_generalidad.values.flatten()))\n",
    "            new_data[\"genM\"].append(ut.entropia(ma_generalidad.values.flatten()))\n",
    "        else:\n",
    "            new_data[\"origM\"].append(0)\n",
    "            new_data[\"genM\"].append(0)\n",
    "            \n",
    "        if(len(borrar_c)>0):\n",
    "            ma_contradiction = ma_contradiction.drop(borrar_c,errors='ignore',axis=1)\n",
    "            print(\"Matriz quitando contradiction\")\n",
    "            print(ma_contradiction)\n",
    "            pred=list(set(lista_entidades_contra))\n",
    "            pred.extend(borrar_c)\n",
    "            print(ut.entropia(ma_original[pred].values.flatten()))\n",
    "            new_data[\"oricM\"].append(ut.entropia(ma_original[pred].values.flatten()))\n",
    "            print(ut.entropia(ma_contradiction.values.flatten()))\n",
    "            new_data[\"conM\"].append(ut.entropia(ma_contradiction.values.flatten()))\n",
    "        else:\n",
    "            new_data[\"oricM\"].append(0)\n",
    "            new_data[\"conM\"].append(0)\n",
    "            \n",
    "        if(len(borrar_e)>0):\n",
    "            ma_especificidad = ma_original.drop(borrar_e,errors='ignore',axis=1)\n",
    "            print(\"Matriz quitando contradiction\")\n",
    "            print(ma_especificidad)\n",
    "            print(ut.entropia(ma_original[borrar_e].values.flatten()))\n",
    "            new_data[\"orieM\"].append(ut.entropia(ma_original[borrar_e].values.flatten()))\n",
    "            print(ut.entropia(ma_especificidad.values.flatten()))\n",
    "            new_data[\"espM\"].append(ut.entropia(ma_especificidad.values.flatten()))\n",
    "        else:\n",
    "            new_data[\"orieM\"].append(0)\n",
    "            new_data[\"espM\"].append(0)\n",
    "            \n",
    "        ma = ma_original[list(set(lista_entidades_no_match).difference(set(borrar_e)))]\n",
    "        print(\"Matriz con los que no hicieron match\")\n",
    "        print(ma)\n",
    "        \n",
    "        \n",
    "        # proceso para checar las no relaciones que existen\n",
    "        print(\"proceso de obtención de no relaciones\")\n",
    "        print(ma,ma.columns)\n",
    "        for c_c in ma.columns:\n",
    "            lista_rel_G4.append((\"\",\"unknown\",c_c))\n",
    "            \n",
    "        new_data[\"rest\"].append(len(ma.columns)/ma_original.shape[1])\n",
    "        \n",
    "        rel_entropia=[]\n",
    "        \n",
    "        for elem in lista_rel_G1:\n",
    "            rel_entropia.append(1)\n",
    "        for elem in lista_rel_G2:\n",
    "            rel_entropia.append(0)\n",
    "        for elem in lista_rel_G3:\n",
    "            rel_entropia.append(2)\n",
    "        for elem in lista_rel_G4:\n",
    "            rel_entropia.append(3)\n",
    "        \n",
    "        print(\"rel\",np.array(rel_entropia).round(1))\n",
    "        print(\"entropia final\",ut.entropia(np.array(rel_entropia).round(1)))\n",
    "        if len(rel_entropia)>0:\n",
    "            new_data['entail'].append(rel_entropia.count(1)/len(rel_entropia))\n",
    "            new_data['contra'].append(rel_entropia.count(0)/len(rel_entropia))\n",
    "            new_data['neutral'].append(rel_entropia.count(2)/len(rel_entropia))\n",
    "            new_data['no_match'].append(rel_entropia.count(3)/len(rel_entropia))\n",
    "        else:\n",
    "            new_data['entail'].append(0)\n",
    "            new_data['contra'].append(0)\n",
    "            new_data['neutral'].append(0)\n",
    "            new_data['no_match'].append(0)\n",
    "            \n",
    "        new_data['entropia_relaciones'].append(ut.entropia(np.array(rel_entropia).round(1)))\n",
    "        \n",
    "        # for a in lista_entidades_no_match:\n",
    "        #     if a not in h_lem:\n",
    "        #         h_lem.append(a)\n",
    "        #     for a_ in r_h[a].split(','):\n",
    "        #         if a_!=\" \" and a_!=\"\" and a_ not in h_lem:\n",
    "        #             h_lem.append(a_)\n",
    "        \n",
    "        b_col=[0]\n",
    "        b_index=[0]\n",
    "        b_col.extend(list(set(lista_entidades_no_match).difference(set(borrar_e))))\n",
    "        #   ALMACENAMIENTO DE TODA LA INFORMACIÓN PROCESADA DE CARACTERÍSTICAS\n",
    "        m_earth=m_earth[ma.columns]\n",
    "        m_mi=m_mi[ma.columns]\n",
    "        \n",
    "        m_distancia = utils.obtener_distancia(t_vectors,h_vectors,t_lem,h_lem,b_col,b_index)\n",
    "        m_earth=m_earth*m_distancia\n",
    "\n",
    "        if ma.shape[1]==0:\n",
    "            new_data['entropias'].append(0)\n",
    "            new_data['KL_divergence'].append(0)\n",
    "            new_data['max_info'].append(0)\n",
    "            new_data['sumas'].append(0)\n",
    "            new_data['mearts'].append(0)\n",
    "            new_data['mutinf'].append(0)\n",
    "            new_data['diferencias'].append(0)\n",
    "            new_data['distancias'].append(0)\n",
    "            new_data['semantics'].append(1)\n",
    "            if c_incompatibilidad>0:\n",
    "                new_data['nlp_semantics'].append(0)\n",
    "            elif c_rel_concep>0:\n",
    "                new_data['nlp_semantics'].append(0.5)\n",
    "            else:\n",
    "                new_data['nlp_semantics'].append(1)\n",
    "            #new_data['nlp_semantics'].append(1)\n",
    "            #new_data['semantics'].append(1)\n",
    "        else:\n",
    "            distY = ma.round(1).values.flatten()\n",
    "            new_data['entropias'].append(ut.entropia(distY))\n",
    "            new_data['KL_divergence'].append(utils.kullback_leibler(distX,distY))\n",
    "            new_data['max_info'].append(ma.max().sum()/(ma.shape[1]))#\n",
    "            new_data['sumas'].append(ma.sum().sum()/((ma.shape[1]*(ma.shape[0]))))#\n",
    "            new_data['mearts'].append(m_earth.min().sum()/(ma.shape[1]))# \n",
    "            new_data['mutinf'].append(m_mi.max().sum()/(ma.shape[1]))# \n",
    "            new_data['diferencias'].append(len(ma.columns)/len(ma.index))\n",
    "            new_data['distancias'].append(m_distancia.min().sum()/(ma.shape[1]))\n",
    "            doc1 = nlp(\" \".join(ma.idxmax().values))\n",
    "            doc2 = nlp(\" \".join(ma.columns.values))\n",
    "            new_data['semantics'].append(float(doc1.similarity(doc2)))\n",
    "            if c_incompatibilidad>0:\n",
    "                new_data['nlp_semantics'].append(1-float(doc1.similarity(doc2)))\n",
    "            else:\n",
    "                new_data['nlp_semantics'].append(float(doc1.similarity(doc2)))\n",
    "\n",
    "        new_data['list_comp'].append(c_compatibilidad/ma_original.shape[0])\n",
    "        new_data['list_incomp'].append(c_incompatibilidad/ma_original.shape[0])\n",
    "        new_data['rel_conceptuales'].append(c_rel_concep/ma_original.shape[0])\n",
    "        new_data['ConteosR'].append(lista_relaciones_grupos)\n",
    "        new_data['ConteosG1'].append(lista_rel_G1[:])\n",
    "        new_data['ConteosG2'].append(lista_rel_G2[:])\n",
    "        new_data['ConteosG3'].append(lista_rel_G3[:])\n",
    "        new_data['ConteosG4'].append(lista_rel_G4[:])\n",
    "        new_data['Texto'].append(texto_i)\n",
    "        new_data['Hipotesis'].append(hipotesis_i)\n",
    "        new_data['TextoL'].append(t_lem)\n",
    "        new_data['HipotesisL'].append(h_lem)\n",
    "    else:\n",
    "        new_data['Jaro-Winkler_rit'].append(0)\n",
    "        new_data['Jaro-Winkler_rit1'].append(0)\n",
    "        new_data['Jaro-Winkler_ritN'].append(0)\n",
    "        new_data['Jaro-Winkler_ritN1'].append(0)\n",
    "        new_data['Jaro-Winkler_ritN_'].append(0)\n",
    "        new_data['Jaro-Winkler_ritN1_'].append(0)\n",
    "        new_data['Jaro-Winkler_ritN__'].append(0)\n",
    "        new_data['Jaro-Winkler_ritN1__'].append(0)\n",
    "\n",
    "        new_data['origE'].append(0)\n",
    "        new_data['genE'].append(0)\n",
    "        new_data['oricE'].append(0)\n",
    "        new_data['conE'].append(0)\n",
    "        new_data['origM'].append(0)\n",
    "        new_data['genM'].append(0)\n",
    "        new_data['oricM'].append(0)\n",
    "        new_data['conM'].append(0)\n",
    "        new_data['orieM'].append(0)\n",
    "        new_data['espM'].append(0)\n",
    "        new_data['rest'].append(0)\n",
    "        \n",
    "        \n",
    "        new_data['simBoWrel'].append(0)\n",
    "        new_data['negT'].append(0)\n",
    "        new_data['negH'].append(0)\n",
    "        new_data['jaccard'].append(0)\n",
    "        new_data['jaccard_rel'].append(0)\n",
    "        new_data['entropia_total'].append(0)\n",
    "        new_data['max_info_t'].append(0)#\n",
    "        new_data['sumas_t'].append(0)#\n",
    "        new_data['mearts_t'].append(0)# \n",
    "        new_data['mutinf_t'].append(0)# \n",
    "        new_data['entSimilitud'].append(0)\n",
    "        new_data[\"relation\"].append(0)\n",
    "        new_data[\"overlap_ent\"].append(0)\n",
    "        new_data[\"no_matcheadas\"].append(0)\n",
    "        new_data[\"contradiction\"].append(0)\n",
    "        new_data['entail'].append(0)\n",
    "        new_data['contra'].append(0)\n",
    "        new_data['neutral'].append(0)\n",
    "        new_data['no_match'].append(0)\n",
    "        new_data['entropia_relaciones'].append(0)\n",
    "        new_data['entropias'].append(0)\n",
    "        new_data['KL_divergence'].append(0)\n",
    "        new_data['max_info'].append(0)#\n",
    "        new_data['sumas'].append(0)#\n",
    "        new_data['mearts'].append(0)# \n",
    "        new_data['mutinf'].append(0)# \n",
    "        new_data['diferencias'].append(0)\n",
    "        new_data['distancias'].append(0)\n",
    "        new_data['semantics'].append(0)\n",
    "        new_data['nlp_semantics'].append(0)\n",
    "        new_data['list_comp'].append(0)\n",
    "        new_data['list_incomp'].append(0)\n",
    "        new_data['rel_conceptuales'].append(0)\n",
    "        new_data['ConteosR'].append([])\n",
    "        new_data['ConteosG1'].append([])\n",
    "        new_data['ConteosG2'].append([])\n",
    "        new_data['ConteosG3'].append([])\n",
    "        new_data['ConteosG4'].append([])\n",
    "        new_data['Texto'].append(texto_i)\n",
    "        new_data['Hipotesis'].append(hipotesis_i)\n",
    "        new_data['TextoL'].append([])\n",
    "        new_data['HipotesisL'].append([])\n",
    "\n",
    "#df_resultados = pd.DataFrame(new_data)\n",
    "#df_resultados.to_pickle(\"salida/nuevo10/\"+sys.argv[1]+\"_.pickle\")\n",
    "fin = time.time()\n",
    "print(\"Tiempo que se llevo:\",round(fin-inicio,2),\" segundos\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
